{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install libraries \n",
    "    sudo -H pip install biopython\n",
    "    sudo -H pip install numpy\n",
    "    sudo -H pip install tensorflow\n",
    "    \n",
    "Optional (it's nice for you to have these in general! , haha) :\n",
    "\n",
    "    sudo -H pip install scipy\n",
    "    sudo -H pip install scikit-learn\n",
    "    sudo -H pip install matplotlib\n",
    "    sudo -H pip install pandas\n",
    "    \n",
    "\n",
    "from Bio import SeqIO\n",
    "from Bio.Data import IUPACData \n",
    "import numpy as np\n",
    "from numpy import argmax\n",
    "import random\n",
    "import pickle\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing orthology pairs file \n",
    "Paires file is provided in this format:\n",
    "\n",
    "   <span style=\"color:orange\">geneID [tab] geneID [tab] type_of_correlation [tab] OMA-orthology-group-number</span>\n",
    "\n",
    "Sample:\n",
    "\n",
    "    ORYNI25992\tARATH00066\t1:1\t488382\n",
    "    ORYNI25992\tTHECC03451\t1:1\t488382\n",
    "    ORYPU22376\tORYRU26544\t1:1\t488382\n",
    "\n",
    "<span style=\"color:blue\">**read_pairs_file**</span> function parses this file, and returns:\n",
    "1. Orthology dictionary made of gene IDs (keys) and their new orthology cluster number (values)\n",
    "2. Number of existing orthology clusters\n",
    "\n",
    "It also saves pairs of the <span style=\"color:orange\">OMA cluster IDs ~ new ID numbers</span> in <span style=\"color:brown\">OMAgroupID-newClusterNumbers.pickle</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pairs_file(file_in):\n",
    "    orthoID = {}\n",
    "    my_index = 0\n",
    "    my_cluster = {}\n",
    "    current_index = 0\n",
    "    \n",
    "    for line in open(file_in, mode='r'):\n",
    "        words = line.split()\n",
    "        \n",
    "        # we first convert OMA cluster IDs (4th column) to our own ID numbers (my_index starting from 0)\n",
    "        if words[3].rstrip() in my_cluster:\n",
    "            current_index = my_cluster[words[3].rstrip()]\n",
    "        else:\n",
    "            my_cluster[words[3].rstrip()] = my_index\n",
    "            current_index = my_index\n",
    "            my_index += 1\n",
    "            \n",
    "        # then we relate each gene ID to this new cluster number\n",
    "        orthoID[words[0]] = current_index\n",
    "        orthoID[words[1]] = current_index        \n",
    "    \n",
    "    # we save a pickle file storing : OMA cluster IDs ~ new ID numbers (we need it later)\n",
    "    with open('OMAgroupID-newClusterNumbers.pickle','wb+') as f:\n",
    "        pickle.dump({v: k for k, v in my_cluster.items()}, f, protocol=pickle.HIGHEST_PROTOCOL)        \n",
    "    \n",
    "    return(orthoID,len(my_cluster.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Longest protein sequence\n",
    "I checked the longest plant protein sequence size in 2 ways:\n",
    "1. [link to NCBI query](https://www.ncbi.nlm.nih.gov/protein/?term=20000%3A30000%5BSequence+Length%5D)\n",
    "\n",
    "   Plants -> Accession: PNW77743.1 has 23859 aa protein\n",
    "   \n",
    "   \n",
    "2. python3 fasta_seq_size.py oma-seqs-viridiplantae.fa > tmp\n",
    "\n",
    "   CHLRE14650 has the longest sequence with size 23776\n",
    "\n",
    "so I decided to give the default longest size \"23776\". \n",
    "But to prevent unnecessary calculations I also added a function which calculates only the longest sequence size in the input fasta file \"longest_seq_size()\". And I try to use this number instead of the longest possible sequence size in general.\n",
    "\n",
    "<span style=\"color:blue\">**longest_seq_size**</span> function parses fasta input file, and returns:\n",
    "1. length of the longest protein sequence in the file\n",
    "2. name of the longest protein sequence in the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def longest_seq_size(file_in):\n",
    "    max_size = 0\n",
    "    max_name = \"\"\n",
    "    for seq_record in SeqIO.parse(open(file_in, mode='r'), 'fasta'):\n",
    "        name, sequence = seq_record.id, str(seq_record.seq)\n",
    "        if (max_size < len(sequence)) :\n",
    "            max_size = len(sequence)\n",
    "            max_name = name\n",
    "    return(max_size,max_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convering cluster number to one_hot matrix\n",
    "\n",
    "The locations represented by indices in indices take value *on_value* (default = 1), while all other locations take value *off_value* (default = 0).\n",
    "\n",
    "for instance: \n",
    "\n",
    "    indices = [0, 1, 2]\n",
    "    depth = 3\n",
    "    tf.one_hot(indices, depth)  # output: [3 x 3]\n",
    "    [[1., 0., 0.],\n",
    "     [0., 1., 0.],\n",
    "     [0., 0., 1.]]\n",
    "    \n",
    "<span style=\"color:blue\">**one_hot_matrix**</span> function converts any number (label, here we input cluster number) to a one_hot tensor of given depth (C, which is total number of clusters).\n",
    "\n",
    "See the example below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_matrix(labels,C):\n",
    "    \n",
    "    C = tf.constant(C,name=\"C\")\n",
    "    one_hot_matrix = tf.one_hot(labels,C,axis=1)\n",
    "    sess = tf.Session()\n",
    "    one_hot = sess.run(one_hot_matrix)\n",
    "    sess.close()\n",
    "    \n",
    "    return one_hot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting a protein sequence to binary\n",
    "\n",
    "<span style=\"color:blue\">**protein_seq_to_binary**</span> function converts any protein sequence to arrays of binary values. \n",
    "\n",
    "How it works?\n",
    "\n",
    "We have 26 IUPAC protein letters. For each letter in the protein sequence we consider an array of 26 values, which the i-th value is 1 and the rest are 0. For instance, whenever we see a 'K', the 9th value in the array is set to 1 and the rest are set to 0. \n",
    "\n",
    "Sequences can have different length. But for this algorithm we decided to work with fixed size. Therefore, the program gets a fixed value as input (l_seq_size) which is always >= sequence length. \n",
    "\n",
    "Like that for each sequence we get an array (l_seq_size) of arrays (26 values) - 2 dimentional array. If the fixed value of l_seq_size is larger than sequence length, the extras will have only 0 in their array. \n",
    "\n",
    "See the example below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def protein_seq_to_binary(in_seq,l_seq_size=23776):\n",
    "    all_protein_letters = list(IUPACData.extended_protein_letters) # 26 letters\n",
    "      \n",
    "    features = np.zeros((l_seq_size,len(all_protein_letters)))\n",
    "       \n",
    "    for i in range(0,len(in_seq)):\n",
    "        aminoacid = list(in_seq)[i]\n",
    "        if aminoacid.upper() in all_protein_letters:\n",
    "            index_value = all_protein_letters.index(aminoacid.upper())\n",
    "            features[i] = one_hot_matrix([index_value],len(all_protein_letters))\n",
    "            \n",
    "    return(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(protein_seq_to_binary(\"AKO\",4)) # test\n",
    "#all_protein_letters = list(IUPACData.extended_protein_letters)\n",
    "#print(all_protein_letters)\n",
    "#index_value = all_protein_letters.index(('A'))\n",
    "#print(index_value)\n",
    "#print(one_hot_matrix([index_value,8,25],26))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting a protein fasta to a binary table\n",
    "\n",
    "<span style=\"color:blue\">**fasta_to_features**</span> function converts a protein fasta file to a binary table.\n",
    "\n",
    "How it works?\n",
    "\n",
    "Each protein sequence is converted to an arrays of binary arrays. For that we use \"protein_seq_to_binary\" function.\n",
    "but then we flatten this 2 dimentional array. so what is left, is a long binary sequence - (0,1)s . All protein sequences in the fasta file will have the same binary length = l_seq_size x 26.\n",
    "\n",
    "The function also keeps the gene ID and the cluster number of that gene ID.\n",
    "\n",
    "So for each fasta file, fasta_to_features returns a 2 dimentional array (all_seq_features): \n",
    "\n",
    "    Number of rows  = number of sequences in the fasta file\n",
    "    Number of columns = 3 (first column is gene ID, second column is a binary list representing the protein sequence, third item is the cluster number)\n",
    "\n",
    "See the example below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from numba import jit\n",
    "#@jit(nopython=True) # Set \"nopython\" mode for best performance\n",
    "\n",
    "def fasta_to_features(input_file,ortho_cluster_IDs,clustertotalnum,l_seq_size=23776):\n",
    "    outfile = open(\"features.csv\",\"w+\")\n",
    "    \n",
    "    with open(input_file, buffering=100000, mode='r') as handle:\n",
    "        for fasta in SeqIO.parse(handle,'fasta'):\n",
    "            name, sequence = fasta.id, str(fasta.seq)\n",
    "            #print(name)\n",
    "            if name in ortho_cluster_IDs:\n",
    "                cluster_num = ortho_cluster_IDs[name]\n",
    "                cluster_num_one_hot = one_hot_matrix([cluster_num],clustertotalnum)\n",
    "                \n",
    "                #outfile.write(name)\n",
    "                #outfile.write(sequence)\n",
    "                #outfile.write(str(cluster_num))\n",
    "                outfile.write(','.join([name,sequence,str(cluster_num)]))\n",
    "                outfile.write(\"\\n\")\n",
    "            \n",
    "    outfile.close()\n",
    "    return()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#seq_f = fasta_to_features(\"CHLRE00002.fa\",{\"CHLRE00002\":5},390) # e.g. cluster 5 = 737531 OMA group\n",
    "#print(np.array(seq_f).shape)\n",
    "#print(seq_f)\n",
    "print(one_hot_matrix([5],5))\n",
    "'''\n",
    "gene_f,seq_f,cluster_f = fasta_to_features(\"CHLRE00002.fa\",{\"CHLRE00002\":5},6,390)\n",
    "print(gene_f)\n",
    "np.set_printoptions(threshold='nan')\n",
    "print(seq_f)\n",
    "print(len(seq_f)/390)\n",
    "print(seq_f)\n",
    "print(cluster_f)\n",
    "'''\n",
    "fasta_to_features(\"CHLRE00002.fa\",{\"CHLRE00002\":5},390)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def protein2integer(in_seq):\n",
    "    \n",
    "    ## define universe of possible input values\n",
    "    all_protein_letters = list(IUPACData.extended_protein_letters)\n",
    "    #print(all_protein_letters)\n",
    "    ## define a mapping of chars to integers\n",
    "    char_to_int = dict((c, i) for i, c in enumerate(all_protein_letters))\n",
    "    int_to_char = dict((i, c) for i, c in enumerate(all_protein_letters))\n",
    "    ## integer encode input data\n",
    "    integer_encoded = [char_to_int[char] for char in in_seq.upper()]\n",
    "    \n",
    "    return(integer_encoded,len(all_protein_letters))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def protein2onehot(in_seq):\n",
    "    \n",
    "    ## integer encode input data\n",
    "    integer_encoded, all_protein_letters_size = protein2integer(in_seq)\n",
    "    #print(integer_encoded)\n",
    "    ## one hot encode\n",
    "    onehot_encoded = list()\n",
    "    onehot_encoded = one_hot_matrix(integer_encoded, all_protein_letters_size)\n",
    "    #for value in integer_encoded:\n",
    "    #    letter = [0 for _ in range(len(all_protein_letters))]\n",
    "    #    letter[value] = 1\n",
    "    #    onehot_encoded.append(letter)\n",
    "    #print(onehot_encoded)\n",
    "    ## invert encoding\n",
    "    ##inverted = int_to_char[argmax(onehot_encoded[0])]\n",
    "    ##print(inverted)\n",
    "    \n",
    "    return(onehot_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]]\n",
      "40\n",
      "26\n"
     ]
    }
   ],
   "source": [
    "myseq = 'MAAMTMTRPNDPFTSDIARFHEVKGAKDEPFAELTRYHEV'\n",
    "myseq_onehot = protein2onehot(myseq)\n",
    "print(myseq_onehot[1:10])\n",
    "print(len(myseq_onehot))\n",
    "print(len(myseq_onehot[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiprotein2onehot(in_seq_l):\n",
    "    \n",
    "    ## integer encode input data\n",
    "    integer_encoded, all_protein_letters_size = protein2integer(in_seq_l)\n",
    "    #print(integer_encoded)\n",
    "    ## one hot encode\n",
    "    onehot_encoded = list()\n",
    "    onehot_encoded = one_hot_matrix(integer_encoded,all_protein_letters_size)\n",
    "    #print(onehot_encoded)\n",
    "    ## invert encoding\n",
    "    #inverted = int_to_char[argmax(onehot_encoded[0])]\n",
    "    #print(inverted)\n",
    "    \n",
    "    return(onehot_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]]\n",
      "40\n",
      "26\n"
     ]
    }
   ],
   "source": [
    "myseq = 'MAAMTMTRPNDPFTSDIARFHEVKGAKDEPFAELTRYHEV'\n",
    "myseq_onehot = multiprotein2onehot(myseq)\n",
    "print(myseq_onehot[0:10])\n",
    "print(len(myseq_onehot))\n",
    "print(len(myseq_onehot[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'read_pairs_file' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-0ab97888c039>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmyOrthoIDs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmyclusternum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_pairs_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'CENH3_DMR6_LUCA-CHLRE00002_orthologues_pairs.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0ml_seq_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ml_seq_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlongest_seq_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'CENH3_DMR6_LUCA-CHLRE00002_orthologues.fa'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mfasta_to_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'CENH3_DMR6_LUCA-CHLRE00002_orthologues.fa'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmyOrthoIDs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmyclusternum\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ml_seq_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'read_pairs_file' is not defined"
     ]
    }
   ],
   "source": [
    "# Importing Keras Sequential Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "#\n",
    "myOrthoIDs,myclusternum = read_pairs_file('CENH3_DMR6_LUCA-CHLRE00002_orthologues_pairs.txt')  \n",
    "l_seq_size,l_seq_name = longest_seq_size('CENH3_DMR6_LUCA-CHLRE00002_orthologues.fa')\n",
    "fasta_to_features('CENH3_DMR6_LUCA-CHLRE00002_orthologues.fa',myOrthoIDs,myclusternum,l_seq_size)\n",
    "\n",
    "# Initializing the seed value to a integer.\n",
    "seed = 7\n",
    "\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Loading the data set (PIMA Diabetes Dataset)\n",
    "dataset = np.loadtxt('features.csv', delimiter=\",\")\n",
    "\n",
    "# Loading the input values to X and Label values Y using slicing.\n",
    "G = dataset[:, 0]\n",
    "X = dataset[:, 1]\n",
    "Y = dataset[:, 2]\n",
    "\n",
    "# Initializing the Sequential model from KERAS.\n",
    "model = Sequential()\n",
    "\n",
    "# Creating a 16 neuron hidden layer with Linear Rectified activation function.\n",
    "model.add(Dense(16, input_dim=1, init='uniform', activation='relu'))\n",
    "\n",
    "# Creating a 8 neuron hidden layer.\n",
    "model.add(Dense(8, init='uniform', activation='relu'))\n",
    "\n",
    "# Adding a output layer.\n",
    "model.add(Dense(1, init='uniform', activation='sigmoid'))\n",
    "\n",
    "\n",
    "\n",
    "# Compiling the model\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam', metrics=['accuracy'])\n",
    "# Fitting the model\n",
    "model.fit(X, Y, nb_epoch=150, batch_size=10)\n",
    "\n",
    "scores = model.evaluate(X, Y)\n",
    "\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1] * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#features_seq_f = np.array(seq_f)\n",
    "#print(list(features_seq_f[:,2][:]))\n",
    "#print(one_hot_matrix(list(features_seq_f[:,2][:]),6))\n",
    "\n",
    "\n",
    "print(one_hot_matrix([0,1,4,5],6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating TRAIN set and TEST set\n",
    "\n",
    "<span style=\"color:blue\">**creat_feature_sets_and_labels**</span> function converts the features into proper formatted Train set and Test set.\n",
    "\n",
    "The size of the Test set is by default 10% of the data, but you can change it if you wish.\n",
    "\n",
    "I made a function called <span style=\"color:blue\">**test_me**</span> to demonstrate the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feature_sets_and_labels_old(pairs_file,fasta_file,test_size = 0.1):\n",
    "       \n",
    "    myOrthoIDs,myclusternum = read_pairs_file(pairs_file)  \n",
    "    \n",
    "    l_seq_size,l_seq_name = longest_seq_size(fasta_file)\n",
    "        \n",
    "    features = []\n",
    "    features = fasta_to_features(fasta_file,myOrthoIDs,l_seq_size)\n",
    "    random.shuffle(features)\n",
    "    features = np.array(features)\n",
    "\n",
    "    testing_size = int(test_size*len(features)) # Calculating the test size based on input \n",
    "\n",
    "    train_x = list(features[:,1][:-testing_size])  # Protein sequences are already converted to one_hot like\n",
    "    train_y = list(one_hot_matrix(list(features[:,2][:-testing_size]),myclusternum)) # convert cluster number to one_hot format\n",
    "    test_x = list(features[:,1][-testing_size:])\n",
    "    test_y = list(one_hot_matrix(list(features[:,2][-testing_size:]),myclusternum))\n",
    "\n",
    "    return train_x,train_y,test_x,test_y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feature_sets_and_labels(pairs_file,fasta_file,test_size = 0.1):\n",
    "       \n",
    "    myOrthoIDs,myclusternum = read_pairs_file(pairs_file)  \n",
    "    \n",
    "    l_seq_size,l_seq_name = longest_seq_size(fasta_file)\n",
    "    \n",
    "    gene_f,seq_f,cluster_f = fasta_to_features(fasta_file,myOrthoIDs,myclusternum,l_seq_size)\n",
    "    \n",
    "    c = zip(gene_f,seq_f,cluster_f)\n",
    "    random.shuffle(c)\n",
    "    gene_f = [e[0] for e in c]\n",
    "    seq_f = [e[1] for e in c]\n",
    "    cluster_f = [e[2] for e in c]\n",
    "    print(len(gene_f))\n",
    "    testing_size = int(test_size*len(gene_f)) # Calculating the test size based on input \n",
    "    print(testing_size)\n",
    "\n",
    "    train_x = list(seq_f[:-testing_size])  # Protein sequences are already converted to one_hot like\n",
    "    train_y = list(cluster_f[:-testing_size]) # convert cluster number to one_hot format\n",
    "    test_x = list(seq_f[-testing_size:])\n",
    "    test_y = list(cluster_f[-testing_size:])\n",
    "\n",
    "    return train_x,train_y,test_x,test_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    train_x,train_y,test_x,test_y = create_feature_sets_and_labels('CENH3_DMR6_orthologues_pairs.txt','CENH3_DMR6_orthologues.fa',0.1)\n",
    "    # if you want to pickle this data:\n",
    "    with open('./sentiment_set.pickle','wb') as f:\n",
    "        pickle.dump([train_x,train_y,test_x,test_y],f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_me():\n",
    "    #train_x,train_y,test_x,test_y = create_feature_sets_and_labels('CENH3_DMR6_orthologues_pairs.txt','CENH3_DMR6_orthologues.fa',0.1)\n",
    "    train_x,train_y,test_x,test_y = create_feature_sets_and_labels('CENH3_DMR6_LUCA-CHLRE00002_orthologues_pairs.txt','CENH3_DMR6_LUCA-CHLRE00002_orthologues.fa',0.1)\n",
    "    print(\"Size of Train X matrix:\",np.array(train_x).shape)\n",
    "    print(\"Size of Train Y matrix:\",np.array(train_y).shape)\n",
    "    print(\"Size of Test X matrix:\",np.array(test_x).shape)\n",
    "    print(\"Size of Test Y matrix:\",np.array(test_y).shape)\n",
    "    print(\"==============================================\")\n",
    "    print(\"First element of Train X matrix:\",train_x[0])\n",
    "    print(\"First element of Train Y matrix:\",train_y[0])\n",
    "    print(\"==============================================\")\n",
    "    print(\"Last (69th) element of Train X matrix:\",train_x[68])\n",
    "    print(\"Last (69th) element of Train Y matrix:\",train_y[68])\n",
    "    return()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_me()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
