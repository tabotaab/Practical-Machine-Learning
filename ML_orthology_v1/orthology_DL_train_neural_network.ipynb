{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install libraries \n",
    "    sudo -H pip install biopython\n",
    "    sudo -H pip install numpy\n",
    "    sudo -H pip install tensorflow\n",
    "    \n",
    "You also need to save <span style=\"color:brown\">orthology_DL_create_sentiment_featuresets.ipynb</span> program in <span style=\"color:brown\">orthology_DL_create_sentiment_featuresets.py</span> format in the same directory of this file. We load it like a library to use the functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from orthology_DL_create_sentiment_featuresets import create_feature_sets_and_labels,protein_seq_to_binary\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import numpy as np\n",
    "from Bio import SeqIO\n",
    "from Bio.Data import IUPACData "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating TRAIN set and TEST set\n",
    "\n",
    "<span style=\"color:blue\">**creat_feature_sets_and_labels**</span> function converts the features into proper formatted Train set and Test set.\n",
    "\n",
    "**inputs:**\n",
    "\n",
    "1.Paires file is provided in this format:\n",
    "\n",
    "   <span style=\"color:orange\">geneID [tab] geneID [tab] type_of_correlation [tab] OMA-orthology-group-number</span>\n",
    "\n",
    "Sample:\n",
    "\n",
    "    ORYNI25992\tARATH00066\t1:1\t488382\n",
    "    ORYNI25992\tTHECC03451\t1:1\t488382\n",
    "    ORYPU22376\tORYRU26544\t1:1\t488382\n",
    "\n",
    "2.Proteins fasta file , which includes multiple protein sequences.\n",
    "\n",
    "Sample:\n",
    "\n",
    "    >MARPO07937\t\n",
    "    MAAMTMTRPNDPFTSDIARFHEVKGAKDEPFAELTRYHEVRGVREMVESFRVREVPSYYV\n",
    "    KPVNERRFTPPSAAVLSMEQQIPCIDLEALSGQELLSAIANACRDWGFFQVLNHGLPSQL\n",
    "    VQNMAKQSSEFFAQPLEEKMKCSTPARVSGPVHFGGGGNRDWRDVLKLNCAPASIVAKEY\n",
    "    WPQRPAGFRDTMEEYSSQQQALAIRLLKLISESLGLESNYLVAACGEPKVVMAINHYPPC\n",
    "    PDPSLTMGIKAHSDPNTITMLLQDDVGGLQVFKEDRWIDVRPLPNALVINVGDQLQILSN\n",
    "    GKYSSCLHRVVNNNRQARTSIATFFSPAHACIIGPAPGLVDEVNPAIYPNIVYADYIKAF\n",
    "    YTQALGPNNKNGGYLAGIELHRRYNCYTSSSSISS\n",
    "\n",
    "3.The size of the Test set is by default 10% (0.1) of the data, you can change it if you wish.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_x,train_y,test_x,test_y,l_seq_size = create_feature_sets_and_labels('CENH3_DMR6_orthologues_pairs.txt','CENH3_DMR6_orthologues.fa',0.1)\n",
    "#train_x,train_y,test_x,test_y = create_feature_sets_and_labels('CENH3_DMR6_LUCA-CHLRE00002_orthologues_pairs.txt','CENH3_DMR6_LUCA-CHLRE00002_orthologues.fa',0.1)\n",
    "train_x,train_y,test_x,test_y = create_feature_sets_and_labels('oma-pairs-viridiplantae.txt','oma-seqs-viridiplantae.fa',0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of nodes in each NN hidden layer\n",
    "n_nodes_hl1 = 1500\n",
    "n_nodes_hl2 = 1500\n",
    "n_nodes_hl3 = 1500\n",
    "\n",
    "# Defining the fix size of protein sequence (used in Test set and Train set) to use for input protein queries\n",
    "l_seq_size = int(len(np.array(test_x)[0])/len(IUPACData.extended_protein_letters))\n",
    "\n",
    "# Number of orthology clusters\n",
    "n_classes = len(np.array(test_y)[0])     #2 or 3 or ...\n",
    "\n",
    "# Batch size and Epoch size for training the NN\n",
    "batch_size = 32   #100\n",
    "hm_epochs = 10    #1000\n",
    "\n",
    "# Initializing X and Y\n",
    "x = tf.placeholder('float')\n",
    "y = tf.placeholder('float')\n",
    "\n",
    "# Initializing NN layers\n",
    "hidden_1_layer = {'f_fum':n_nodes_hl1,\n",
    "                  'weight':tf.Variable(tf.random_normal([len(train_x[0]), n_nodes_hl1])),\n",
    "                  'bias':tf.Variable(tf.random_normal([n_nodes_hl1]))}\n",
    "\n",
    "hidden_2_layer = {'f_fum':n_nodes_hl2,\n",
    "                  'weight':tf.Variable(tf.random_normal([n_nodes_hl1, n_nodes_hl2])),\n",
    "                  'bias':tf.Variable(tf.random_normal([n_nodes_hl2]))}\n",
    "\n",
    "hidden_3_layer = {'f_fum':n_nodes_hl3,\n",
    "                  'weight':tf.Variable(tf.random_normal([n_nodes_hl2, n_nodes_hl3])),\n",
    "                  'bias':tf.Variable(tf.random_normal([n_nodes_hl3]))}\n",
    "\n",
    "output_layer = {'f_fum':None,\n",
    "                'weight':tf.Variable(tf.random_normal([n_nodes_hl3, n_classes])),\n",
    "                'bias':tf.Variable(tf.random_normal([n_classes])),}\n",
    "\n",
    "# The Saver class adds ops to save and restore variables to and from checkpoints. \n",
    "saver = tf.train.Saver()\n",
    "tf_log = 'tf.log'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Model\n",
    "\n",
    "\n",
    "![alt text](./NN-3hl.jpg \"NN model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_network_model(data):\n",
    "\n",
    "    l1 = tf.add(tf.matmul(data,hidden_1_layer['weight']), hidden_1_layer['bias'])\n",
    "    l1 = tf.nn.relu(l1)\n",
    "\n",
    "    l2 = tf.add(tf.matmul(l1,hidden_2_layer['weight']), hidden_2_layer['bias'])\n",
    "    l2 = tf.nn.relu(l2)\n",
    "\n",
    "    l3 = tf.add(tf.matmul(l2,hidden_3_layer['weight']), hidden_3_layer['bias'])\n",
    "    l3 = tf.nn.relu(l3)\n",
    "\n",
    "    output = tf.matmul(l3,output_layer['weight']) + output_layer['bias']\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Neural Network\n",
    "\n",
    "The trained model is saved to <span style=\"color:brown\">./model.ckpt*</span>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_neural_network(x):\n",
    "    my_model_save_path = \"./model.ckpt\"\n",
    "    prediction = neural_network_model(x)\n",
    "    #cost = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(logits=prediction,labels=y) )\n",
    "    cost = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits_v2(logits=prediction,labels=y) )\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=0.001).minimize(cost)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        #sess.run(tf.initialize_all_variables())\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        try:\n",
    "            epoch = int(open(tf_log,'r').read().split('\\n')[-2])+1\n",
    "            print('STARTING:',epoch)\n",
    "        except:\n",
    "            epoch = 1\n",
    "\n",
    "        while epoch <= hm_epochs:\n",
    "            if epoch != 1:\n",
    "                saver.restore(sess,\"./model.ckpt\")\n",
    "            epoch_loss = 1\n",
    "            \n",
    "            i=0\n",
    "            while i < len(train_x):\n",
    "                start = i\n",
    "                end = i+batch_size\n",
    "                batch_x = np.array(train_x[start:end])\n",
    "                batch_y = np.array(train_y[start:end])\n",
    "\n",
    "                _, c = sess.run([optimizer, cost], feed_dict={x: batch_x,y: batch_y})\n",
    "                epoch_loss += c\n",
    "                i+=batch_size\n",
    "                \n",
    "            \n",
    "            my_model_save_path = saver.save(sess, \"./model.ckpt\")\n",
    "            \n",
    "            print('Epoch', epoch, 'completed out of',hm_epochs,'loss:',epoch_loss)\n",
    "            \n",
    "            with open(tf_log,'a') as f:\n",
    "                f.write(str(epoch)+'\\n') \n",
    "            epoch +=1\n",
    "            \n",
    "        correct = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct, 'float'))\n",
    "\n",
    "        print(\"\\nModel saved in path: %s \" % my_model_save_path)\n",
    "        print('\\nAccuracy:',accuracy.eval({x:test_x, y:test_y}))\n",
    "\n",
    "    \n",
    "    return()\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_neural_network(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting query protein fasta file to a binary table\n",
    "\n",
    "<span style=\"color:blue\">**fasta_input_to_features**</span> function converts a protein fasta file to a binary table.\n",
    "\n",
    "How it works?\n",
    "\n",
    "Each protein sequence is converted to an arrays of binary arrays. For that we use \"protein_seq_to_binary\" function.\n",
    "but then we flatten this 2 dimentional array. so what is left, is a long binary sequence - (0,1)s . All protein sequences in the fasta file will have the same binary length = l_seq_size x 26.\n",
    "\n",
    "So for each fasta file, fasta_input_to_features returns a 2 dimentional array (all_seq_features): \n",
    "\n",
    "    Number of rows  = number of sequences in the fasta file\n",
    "    Number of columns = 1 (a binary list representing the protein sequence)\n",
    "\n",
    "and an array of gene IDs (all_names).These IDs will be used for proper result report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fasta_input_to_features(input_fasta_file,lseqsize=23776):\n",
    "    fasta_sequences = SeqIO.parse(open(input_fasta_file),'fasta')\n",
    "    \n",
    "    all_seq_features = []\n",
    "    all_names = []\n",
    "    \n",
    "    for fasta in fasta_sequences:\n",
    "        name, sequence = fasta.id, str(fasta.seq)\n",
    "        sequence_features = protein_seq_to_binary(sequence,lseqsize)\n",
    "        sequence_features_flat = sequence_features.flatten()\n",
    "        all_seq_features.append(list(sequence_features_flat))\n",
    "        all_names.append(name)\n",
    "        \n",
    "        \n",
    "    return(all_seq_features,all_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the trained-tested model\n",
    "\n",
    "Now it is time to use our model. <span style=\"color:blue\">**use_neural_network**</span> function gets a query protein fasta file and returns predicted OMA group for each proteins sequence based on our model.\n",
    "\n",
    "The program reads the model. Predicts the orthology cluster for each protein sequence. converts the cluster IDs to OMA groups and returns results in a dictionary format.\n",
    "\n",
    "See the example below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def use_neural_network(in_fasta_file,lseqsize=l_seq_size): # there can be multiple sequences in one fasta file\n",
    "    prediction = neural_network_model(x)\n",
    "    \n",
    "    input_features, input_names = fasta_input_to_features(in_fasta_file,lseqsize)\n",
    "    my_cluster_inv = pickle.load( open( \"OMAgroupID-newClusterNumbers.pickle\", \"rb\" ) )    \n",
    "    #print(my_cluster_inv)    \n",
    "    \n",
    "    output={}\n",
    "    i=0\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        #sess.run(tf.initialize_all_variables())\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        saver.restore(sess,\"model.ckpt\")\n",
    "        for item in input_features:\n",
    "            features = item\n",
    "            features = np.array(list(features))\n",
    "            result = (sess.run(tf.argmax(prediction.eval(feed_dict={x:[features]}),1)))\n",
    "            omacluster = my_cluster_inv[result[0]]\n",
    "            #print(\"\\n* \",input_names[i],\"is in cluster\",result,\"= oma group\",omacluster) \n",
    "            output[input_names[i]]=omacluster\n",
    "            i += 1\n",
    "    \n",
    "    return(output)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use_neural_network(\"./CHLRE00002.fa\",l_seq_size)\n",
    "print(use_neural_network(\"./CHLRE00002.fa\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the inspect_checkpoint library\n",
    "from tensorflow.python.tools import inspect_checkpoint as chkp\n",
    "\n",
    "# print all tensors in checkpoint file\n",
    "chkp.print_tensors_in_checkpoint_file(\"./model.ckpt\", tensor_name='', all_tensors=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
