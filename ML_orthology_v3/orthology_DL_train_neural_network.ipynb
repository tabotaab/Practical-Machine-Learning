{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import GRU\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from Bio import SeqIO\n",
    "from Bio.Data import IUPACData \n",
    "import csv\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "#data_path = 'features_CENH3_DMR6_LUCA-CHLRE00002_orthologues.csv'\n",
    "#data_path = 'features_oma-seqs-viridiplantae_test-7-8.csv'\n",
    "data_path = 'features_oma-seqs-viridiplantae_test-5-6-7-8.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def protein2integer(in_seq):\n",
    "    \n",
    "    ## define universe of possible input values\n",
    "    all_protein_letters = list(IUPACData.extended_protein_letters)\n",
    "    #print(all_protein_letters)\n",
    "    ## define a mapping of chars to integers \n",
    "    ## i+1 beacuse we want to start from integer 1 instead of 0. 0 will be used for padding\n",
    "    char_to_int = dict((c, i+1) for i, c in enumerate(all_protein_letters))\n",
    "    int_to_char = dict((i+1, c) for i, c in enumerate(all_protein_letters))\n",
    "    ## integer encode input data\n",
    "    integer_encoded = [char_to_int[char] for char in in_seq.upper()]\n",
    "    \n",
    "    \n",
    "    #return(integer_encoded,len(all_protein_letters))\n",
    "    return(integer_encoded)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(in_file):\n",
    "    with open(in_file, 'r') as f:\n",
    "        reader = csv.reader(f, delimiter=\"\\t\")\n",
    "        # get all the rows as a list\n",
    "        d_set = list(reader)\n",
    "        # transform data into numpy array\n",
    "        d_set = np.array(d_set).astype(str)\n",
    "        \n",
    "    integer_encoded_proteins = np.array([protein2integer(seq) for seq in d_set[:,1]])\n",
    "    \n",
    "    G = d_set[:, 0]\n",
    "    X = integer_encoded_proteins\n",
    "    Y = d_set[:, 2].astype(int)\n",
    "                         \n",
    "    return(d_set,G,X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_test_set(G,X,Y):\n",
    "    indices = np.random.permutation(X.shape[0])\n",
    "    train_size = int(indices.size*0.80)\n",
    "    train_idx, test_idx = indices[:train_size], indices[train_size:]\n",
    "    #print(len(train_idx),len(test_idx))\n",
    "    \n",
    "    X_train, X_test = X[train_idx,], X[test_idx,]\n",
    "    #print(X_train.shape,X_test.shape)\n",
    "    \n",
    "    y_train, y_test = Y[train_idx,], Y[test_idx,]\n",
    "    \n",
    "    #print(X[train_idx[0],])\n",
    "    #print(Y[train_idx[0],])\n",
    "\n",
    "    #print(X_train[0,])\n",
    "    #print(y_train[0,])\n",
    "    \n",
    "    return(X_train,y_train,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model1(X_train_new, y_train,X_test_new, y_test,in_batch_size=100,in_epochs=10): # RNN: Recurrent Neural Networks\n",
    "    # https://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/\n",
    "    # create the model\n",
    "    embedding_vecor_length = 4\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(num_letters, embedding_vecor_length, input_length=fixed_seq_length))\n",
    "    model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(LSTM(75))\n",
    "    model.add(Dense(n_classes, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy',optimizer='rmsprop', metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "    \n",
    "#''' \n",
    "#    model.fit(X_train_new, y_train, epochs=in_epochs, batch_size=in_batch_size) #epochs=3, batch_size=64)\n",
    "#    ## Final evaluation of the model\n",
    "#    scores = model.evaluate(X_test_new, y_test, verbose=0)\n",
    "#    print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "#'''\n",
    "\n",
    "\n",
    "    # Convert labels to categorical one-hot encoding & fit the model\n",
    "    y_train_one_hot_labels = to_categorical(y_train, num_classes=n_classes)\n",
    "    model.fit(X_train_new, y_train_one_hot_labels, epochs=in_epochs, batch_size=in_batch_size)\n",
    "\n",
    "#    y_train_one_hot_labels = to_categorical(y_train, num_classes=n_classes)\n",
    "#    model.fit(X_train_new, y_train_one_hot_labels, epochs=in_epochs, verbose=1)\n",
    "\n",
    "    # evaluate the model\n",
    "    y_test_one_hot_labels = to_categorical(y_test, num_classes=n_classes)\n",
    "    loss, accuracy = model.evaluate(X_test_new, y_test_one_hot_labels, verbose=0)\n",
    "    print('Accuracy: %f' % (accuracy*100))\n",
    "    \n",
    "    return()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model2(X_train_new, y_train,X_test_new, y_test,in_batch_size=100,in_epochs=10): # RNN: Recurrent Neural Networks\n",
    "    # Initializing the Sequential model from KERAS.\n",
    "    model = Sequential()\n",
    "\n",
    "    # Creating a 16 neuron hidden layer with Linear Rectified activation function.\n",
    "    #model.add(Dense(16, input_dim=1, init='uniform', activation='relu'))\n",
    "    model.add(Dense(16, input_dim=fixed_seq_length, kernel_initializer='uniform', activation='relu'))\n",
    "\n",
    "    # Creating a 8 neuron hidden layer.\n",
    "    model.add(Dense(8, kernel_initializer='uniform', activation='relu'))\n",
    "\n",
    "    # Adding a output layer.\n",
    "    model.add(Dense(n_classes, kernel_initializer='uniform', activation='softmax'))\n",
    "    \n",
    "    # Compiling the model\n",
    "    model.compile(loss='categorical_crossentropy',optimizer='rmsprop', metrics=['accuracy'])\n",
    "\n",
    "    print(model.summary())\n",
    "   \n",
    "    # Convert labels to categorical one-hot encoding & fit the model\n",
    "    y_train_one_hot_labels = to_categorical(y_train, num_classes=n_classes)\n",
    "    model.fit(X_train_new, y_train_one_hot_labels, epochs=in_epochs, batch_size=in_batch_size)\n",
    "\n",
    "    # fit the model\n",
    "#    y_train_one_hot_labels = to_categorical(y_train, num_classes=n_classes)\n",
    "#    model.fit(X_train_new, y_train_one_hot_labels, epochs=in_epochs, verbose=1)\n",
    "\n",
    "    # evaluate the model\n",
    "    y_test_one_hot_labels = to_categorical(y_test, num_classes=n_classes)\n",
    "    loss, accuracy = model.evaluate(X_test_new, y_test_one_hot_labels, verbose=0)\n",
    "    print('Accuracy: %f' % (accuracy*100))\n",
    "    \n",
    "    return()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_matrix(labels,C):\n",
    "    \n",
    "    C = tf.constant(C,name=\"C\")\n",
    "    one_hot_matrix = tf.one_hot(labels,C,axis=1)\n",
    "    sess = tf.Session()\n",
    "    one_hot = sess.run(one_hot_matrix)\n",
    "    sess.close()\n",
    "    \n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model3(X_train_new, y_train,X_test_new, y_test, batch_size =100, hm_epochs =100): # CNN: Convolutional Neural Networks\n",
    "    # Number of nodes in each NN hidden layer\n",
    "    n_nodes_hl1 = 1500\n",
    "    n_nodes_hl2 = 1500\n",
    "    n_nodes_hl3 = 1500\n",
    "\n",
    "    # Number of orthology clusters\n",
    "    #n_classes = len(np.unique(np.concatenate((y_train,y_test),axis=0)))     #2 or 3 or ...\n",
    "    \n",
    "    #y_all = np.concatenate((y_train,y_test),axis=0)\n",
    "    #y_min = np.amin(y_all)\n",
    "    #n_classes = np.amax(y_all-y_min)+1\n",
    "    \n",
    "    \n",
    "    train_y = one_hot_matrix(y_train,n_classes)\n",
    "    test_y = one_hot_matrix(y_test,n_classes)\n",
    "\n",
    "    # Batch size and Epoch size for training the NN\n",
    "    #batch_size = 100   #100\n",
    "    #hm_epochs = 100    #1000\n",
    "\n",
    "    # Initializing X and Y\n",
    "    x = tf.placeholder('float')\n",
    "    y = tf.placeholder('float')\n",
    "\n",
    "    # Initializing NN layers\n",
    "    hidden_1_layer = {'f_fum':n_nodes_hl1,\n",
    "                  'weight':tf.Variable(tf.random_normal([len(X_train_new[0]), n_nodes_hl1])),\n",
    "                  'bias':tf.Variable(tf.random_normal([n_nodes_hl1]))}\n",
    "\n",
    "    hidden_2_layer = {'f_fum':n_nodes_hl2,\n",
    "                  'weight':tf.Variable(tf.random_normal([n_nodes_hl1, n_nodes_hl2])),\n",
    "                  'bias':tf.Variable(tf.random_normal([n_nodes_hl2]))}\n",
    "\n",
    "    hidden_3_layer = {'f_fum':n_nodes_hl3,\n",
    "                  'weight':tf.Variable(tf.random_normal([n_nodes_hl2, n_nodes_hl3])),\n",
    "                  'bias':tf.Variable(tf.random_normal([n_nodes_hl3]))}\n",
    "\n",
    "    output_layer = {'f_fum':None,\n",
    "                'weight':tf.Variable(tf.random_normal([n_nodes_hl3, n_classes])),\n",
    "                'bias':tf.Variable(tf.random_normal([n_classes])),}\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    l1 = tf.add(tf.matmul(x,hidden_1_layer['weight']), hidden_1_layer['bias'])\n",
    "    l1 = tf.nn.relu(l1)\n",
    "\n",
    "    l2 = tf.add(tf.matmul(l1,hidden_2_layer['weight']), hidden_2_layer['bias'])\n",
    "    l2 = tf.nn.relu(l2)\n",
    "\n",
    "    l3 = tf.add(tf.matmul(l2,hidden_3_layer['weight']), hidden_3_layer['bias'])\n",
    "    l3 = tf.nn.relu(l3)\n",
    "\n",
    "    prediction = tf.matmul(l3,output_layer['weight']) + output_layer['bias']\n",
    "\n",
    "        \n",
    "    cost = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits_v2(logits=prediction,labels=y) )\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=0.001).minimize(cost)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        #try:\n",
    "        #    epoch = int(open(tf_log,'r').read().split('\\n')[-2])+1\n",
    "        #    print('STARTING:',epoch)\n",
    "        #except:\n",
    "        #    epoch = 1\n",
    "        epoch = 1\n",
    "\n",
    "        while epoch <= hm_epochs:\n",
    "            epoch_loss = 1\n",
    "            \n",
    "            i=0\n",
    "            while i < len(X_train_new):\n",
    "                start = i\n",
    "                end = i+batch_size\n",
    "                batch_x = np.array(X_train_new[start:end])\n",
    "                batch_y = np.array(train_y[start:end])\n",
    "\n",
    "                _, c = sess.run([optimizer, cost], feed_dict={x: batch_x,y: batch_y})\n",
    "                epoch_loss += c\n",
    "                i+=batch_size\n",
    "                \n",
    "            \n",
    "            print('Epoch ',epoch,' out of ',hm_epochs,'- loss:',epoch_loss)\n",
    " \n",
    "            \n",
    "            #with open(tf_log,'a') as f:\n",
    "            #    f.write(str(epoch)+'\\n') \n",
    "            epoch +=1\n",
    "            \n",
    "        correct = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct, 'float'))\n",
    "\n",
    "        #print(\"\\nModel saved in path: %s \" % my_model_save_path)\n",
    "        print('\\nAccuracy:',accuracy.eval({x:X_test_new, y:test_y}) * 100)\n",
    "    return()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model4(X_train_new, y_train,X_test_new, y_test,in_batch_size=100,in_epochs=10): # RNN: Recurrent Neural Networks\n",
    "    \n",
    "    # Convert labels to categorical one-hot encoding  \n",
    "    y_train_one_hot_labels = to_categorical(y_train, num_classes=n_classes)\n",
    "    y_test_one_hot_labels = to_categorical(y_test, num_classes=n_classes)\n",
    "    X_train_new_one_hot_labels = np.array([to_categorical(x, num_classes=num_letters) for x in X_train_new])\n",
    "    X_test_new_one_hot_labels = np.array([to_categorical(x, num_classes=num_letters) for x in X_test_new])\n",
    "\n",
    "    # create the model    \n",
    "    model = Sequential()\n",
    "    model.add(LSTM(75,input_shape=X_train_new_one_hot_labels[0].shape,return_sequences=True))\n",
    "    model.add(LSTM(75))\n",
    "    model.add(Dense(n_classes, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy',optimizer='rmsprop', metrics=['accuracy'])\n",
    "    print(model.summary())  \n",
    "\n",
    "    # fit the model\n",
    "    model.fit(X_train_new_one_hot_labels, y_train_one_hot_labels, epochs=in_epochs, batch_size=in_batch_size, verbose=1)\n",
    "\n",
    "    # evaluate the model\n",
    "    loss, accuracy = model.evaluate(X_test_new_one_hot_labels, y_test_one_hot_labels, verbose=0)\n",
    "    print('Accuracy: %f' % (accuracy*100))\n",
    "    \n",
    "    return()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model5(X_train_new, y_train,X_test_new, y_test,in_batch_size=100,in_epochs=10): # RNN: Recurrent Neural Networks\n",
    "    # LSTM\n",
    "    \n",
    "    # Convert labels to categorical one-hot encoding  \n",
    "    y_train_one_hot_labels = to_categorical(y_train, num_classes=n_classes)\n",
    "    y_test_one_hot_labels = to_categorical(y_test, num_classes=n_classes)\n",
    "\n",
    "    # create the model    \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(num_letters, output_dim=fixed_seq_length))\n",
    "    model.add(LSTM(75,return_sequences=True))\n",
    "    model.add(LSTM(75))\n",
    "    model.add(Dense(n_classes, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy',optimizer='rmsprop', metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "\n",
    "    # fit the model\n",
    "    model.fit(X_train_new, y_train_one_hot_labels, epochs=in_epochs, batch_size=in_batch_size)\n",
    "\n",
    "    # evaluate the model\n",
    "    loss, accuracy = model.evaluate(X_test_new, y_test_one_hot_labels, verbose=0)\n",
    "    print('Accuracy: %f' % (accuracy*100))\n",
    "    \n",
    "    return()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model6(X_train_new, y_train,X_test_new, y_test,in_batch_size=100,in_epochs=10): # RNN: Recurrent Neural Networks\n",
    "    # GRU\n",
    "    \n",
    "    # Convert labels to categorical one-hot encoding  \n",
    "    y_train_one_hot_labels = to_categorical(y_train, num_classes=n_classes)\n",
    "    y_test_one_hot_labels = to_categorical(y_test, num_classes=n_classes)\n",
    "    \n",
    "    # create the model    \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(num_letters, output_dim=fixed_seq_length))\n",
    "    model.add(GRU(128, return_sequences=False))\n",
    "    model.add(Dense(n_classes, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "\n",
    "    # fit the model\n",
    "    model.fit(X_train_new, y_train_one_hot_labels, epochs=in_epochs, batch_size=in_batch_size)\n",
    "\n",
    "    # evaluate the model\n",
    "    loss, accuracy = model.evaluate(X_test_new, y_test_one_hot_labels, verbose=0)\n",
    "    print('Accuracy: %f' % (accuracy*100))\n",
    "    \n",
    "    return()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_1(d_set):\n",
    "    print(d_set[:,0])\n",
    "    print(d_set[0:2,1])\n",
    "    print(d_set[:,2].astype(int))\n",
    "    print(d_set.shape)\n",
    "    print(d_set[:,1].shape)\n",
    "    return()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_2(d_set):\n",
    "    integer_encoded_proteins = np.array([protein2integer(seq) for seq in d_set[:,1]])\n",
    "    print(len(integer_encoded_proteins))\n",
    "    print(integer_encoded_proteins[0])\n",
    "    #np.array(integer_encoded_proteins).shape\n",
    "    print(integer_encoded_proteins.shape)\n",
    "    #protein2integer(dataset[:,1])\n",
    "    return()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_3(G,X,Y):\n",
    "    print(G.shape)\n",
    "    print(X.shape)\n",
    "    print(Y.shape)\n",
    "\n",
    "    print(G[0:3,])\n",
    "    print(X[0:3,])\n",
    "    print(Y[0:3,])\n",
    "    \n",
    "    print('Maximum review length: {}'.format(len(max(X, key=len))))\n",
    "    print('Minimum review length: {}'.format(len(min(X, key=len))))\n",
    "    \n",
    "    return()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_4(X_train_new,X_train):\n",
    "    print(X_train_new.shape)\n",
    "    print(X_train_new[0,:])\n",
    "    print(X_train.shape)\n",
    "    print(X_train[0,])\n",
    "    return()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, G, X, Y = make_dataset(data_path)\n",
    "X_train,y_train,X_test,y_test = make_train_test_set(G,X,Y)\n",
    "\n",
    "#print(\"============ Test 1 =======================\")\n",
    "#test_1(dataset)\n",
    "#print(\"============ Test 2 =======================\")\n",
    "#test_2(dataset)\n",
    "#print(\"============ Test 3 =======================\")\n",
    "#test_3(G,X,Y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_letters = len(list(IUPACData.extended_protein_letters)) # = 26\n",
    "#fixed_seq_length = len(max(X, key=len)) # maximum\n",
    "#fixed_seq_length = (sum(len(X[i,]) for i in range(X.shape[0]))/X.shape[0])  # average\n",
    "fixed_seq_length = 1000\n",
    "n_classes = int(np.amax(np.concatenate((y_train,y_test),axis=0))+1)\n",
    "# truncate and pad input sequences\n",
    "X_train_new = sequence.pad_sequences(X_train, maxlen=fixed_seq_length, padding='post', truncating='post')\n",
    "X_test_new = sequence.pad_sequences(X_test, maxlen=fixed_seq_length, padding='post', truncating='post')\n",
    "  \n",
    "#print(\"============ Test 4 =======================\")\n",
    "#test_4(X_train_new,X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_8 (Embedding)      (None, 1000, 4)           104       \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 1000, 32)          416       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 500, 32)           0         \n",
      "_________________________________________________________________\n",
      "lstm_10 (LSTM)               (None, 75)                32400     \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 10219)             776644    \n",
      "=================================================================\n",
      "Total params: 809,564\n",
      "Trainable params: 809,564\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/1000\n",
      "3505/3505 [==============================] - 14s 4ms/step - loss: 8.5610 - acc: 2.8531e-04\n",
      "Epoch 2/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 6.9472 - acc: 0.0011\n",
      "Epoch 3/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 6.6308 - acc: 8.5592e-04\n",
      "Epoch 4/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 6.5298 - acc: 8.5592e-04\n",
      "Epoch 5/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 6.4849 - acc: 5.7061e-04\n",
      "Epoch 6/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 6.4643 - acc: 8.5592e-04\n",
      "Epoch 7/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 6.4547 - acc: 0.0011\n",
      "Epoch 8/1000\n",
      "3505/3505 [==============================] - 12s 4ms/step - loss: 6.4482 - acc: 2.8531e-04\n",
      "Epoch 9/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 6.4473 - acc: 2.8531e-04\n",
      "Epoch 10/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 6.4461 - acc: 8.5592e-04\n",
      "Epoch 11/1000\n",
      "3505/3505 [==============================] - 12s 4ms/step - loss: 6.4372 - acc: 2.8531e-04\n",
      "Epoch 12/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 6.4334 - acc: 2.8531e-04\n",
      "Epoch 13/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 6.4307 - acc: 0.0011\n",
      "Epoch 14/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 6.4292 - acc: 8.5592e-04\n",
      "Epoch 15/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 6.4272 - acc: 5.7061e-04\n",
      "Epoch 16/1000\n",
      "3505/3505 [==============================] - 12s 4ms/step - loss: 6.4241 - acc: 2.8531e-04\n",
      "Epoch 17/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 6.4118 - acc: 0.0017\n",
      "Epoch 18/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 6.3752 - acc: 0.0026\n",
      "Epoch 19/1000\n",
      "3505/3505 [==============================] - 12s 4ms/step - loss: 6.3118 - acc: 0.0034\n",
      "Epoch 20/1000\n",
      "3505/3505 [==============================] - 12s 4ms/step - loss: 6.2512 - acc: 0.0026\n",
      "Epoch 21/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 6.1833 - acc: 0.0031\n",
      "Epoch 22/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 6.1081 - acc: 0.0031\n",
      "Epoch 23/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 6.0667 - acc: 0.0014\n",
      "Epoch 24/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 6.0040 - acc: 0.0029\n",
      "Epoch 25/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 5.9315 - acc: 0.0049\n",
      "Epoch 26/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 5.8716 - acc: 0.0057\n",
      "Epoch 27/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 5.8381 - acc: 0.0054\n",
      "Epoch 28/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 5.7805 - acc: 0.0063\n",
      "Epoch 29/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 5.7357 - acc: 0.0063\n",
      "Epoch 30/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 5.6849 - acc: 0.0051\n",
      "Epoch 31/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 5.6375 - acc: 0.0066\n",
      "Epoch 32/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 5.6072 - acc: 0.0083\n",
      "Epoch 33/1000\n",
      "3505/3505 [==============================] - 12s 4ms/step - loss: 5.6205 - acc: 0.0083\n",
      "Epoch 34/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 5.4941 - acc: 0.0088\n",
      "Epoch 35/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 5.4713 - acc: 0.0103\n",
      "Epoch 36/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 5.4136 - acc: 0.0088\n",
      "Epoch 37/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 5.3974 - acc: 0.0100\n",
      "Epoch 38/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 5.3446 - acc: 0.0097\n",
      "Epoch 39/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 5.3225 - acc: 0.0111\n",
      "Epoch 40/1000\n",
      "3505/3505 [==============================] - 12s 4ms/step - loss: 5.2793 - acc: 0.0100\n",
      "Epoch 41/1000\n",
      "3505/3505 [==============================] - 12s 4ms/step - loss: 5.2473 - acc: 0.0114\n",
      "Epoch 42/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 5.2219 - acc: 0.0140\n",
      "Epoch 43/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 5.1910 - acc: 0.0111\n",
      "Epoch 44/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 5.1701 - acc: 0.0137\n",
      "Epoch 45/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 5.1470 - acc: 0.0131\n",
      "Epoch 46/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 5.1208 - acc: 0.0128\n",
      "Epoch 47/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 5.1038 - acc: 0.0168\n",
      "Epoch 48/1000\n",
      "3505/3505 [==============================] - 12s 4ms/step - loss: 5.0676 - acc: 0.0146\n",
      "Epoch 49/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 5.0634 - acc: 0.0117\n",
      "Epoch 50/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 5.0269 - acc: 0.0188\n",
      "Epoch 51/1000\n",
      "3505/3505 [==============================] - 12s 4ms/step - loss: 5.0169 - acc: 0.0163\n",
      "Epoch 52/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 5.0037 - acc: 0.0137\n",
      "Epoch 53/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 4.9710 - acc: 0.0188\n",
      "Epoch 54/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 4.9451 - acc: 0.0203\n",
      "Epoch 55/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 4.9484 - acc: 0.0143\n",
      "Epoch 56/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 4.9307 - acc: 0.0183\n",
      "Epoch 57/1000\n",
      "3505/3505 [==============================] - 12s 4ms/step - loss: 4.8904 - acc: 0.0214\n",
      "Epoch 58/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 4.8967 - acc: 0.0163\n",
      "Epoch 59/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 4.8661 - acc: 0.0237\n",
      "Epoch 60/1000\n",
      "3505/3505 [==============================] - 12s 4ms/step - loss: 4.8540 - acc: 0.0194\n",
      "Epoch 61/1000\n",
      "3505/3505 [==============================] - 12s 4ms/step - loss: 4.8220 - acc: 0.0200\n",
      "Epoch 62/1000\n",
      "3505/3505 [==============================] - 12s 4ms/step - loss: 4.8291 - acc: 0.0174\n",
      "Epoch 63/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 4.8117 - acc: 0.0214\n",
      "Epoch 64/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 4.7817 - acc: 0.0200\n",
      "Epoch 65/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 4.7810 - acc: 0.0237\n",
      "Epoch 66/1000\n",
      "3505/3505 [==============================] - 12s 4ms/step - loss: 4.7604 - acc: 0.0214\n",
      "Epoch 67/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 4.7550 - acc: 0.0231\n",
      "Epoch 68/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 4.7445 - acc: 0.0205\n",
      "Epoch 69/1000\n",
      "3505/3505 [==============================] - 12s 4ms/step - loss: 4.7291 - acc: 0.0214\n",
      "Epoch 70/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 4.7214 - acc: 0.0274\n",
      "Epoch 71/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 4.7044 - acc: 0.0237\n",
      "Epoch 72/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 4.6975 - acc: 0.0237\n",
      "Epoch 73/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 4.6916 - acc: 0.0240\n",
      "Epoch 74/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 4.6815 - acc: 0.0228\n",
      "Epoch 75/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 4.6546 - acc: 0.0308\n",
      "Epoch 76/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 4.6499 - acc: 0.0271\n",
      "Epoch 77/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 4.6470 - acc: 0.0300\n",
      "Epoch 78/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 4.6493 - acc: 0.0243\n",
      "Epoch 79/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 4.6220 - acc: 0.0282\n",
      "Epoch 80/1000\n",
      "3505/3505 [==============================] - 12s 4ms/step - loss: 4.6135 - acc: 0.0317\n",
      "Epoch 81/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 4.6109 - acc: 0.0280\n",
      "Epoch 82/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 4.5990 - acc: 0.0248\n",
      "Epoch 83/1000\n",
      "3505/3505 [==============================] - 12s 4ms/step - loss: 4.5809 - acc: 0.0274\n",
      "Epoch 84/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 4.5811 - acc: 0.0245\n",
      "Epoch 85/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 4.5929 - acc: 0.0254\n",
      "Epoch 86/1000\n",
      "3505/3505 [==============================] - 12s 4ms/step - loss: 4.5916 - acc: 0.0317\n",
      "Epoch 87/1000\n",
      "3505/3505 [==============================] - 12s 4ms/step - loss: 4.5651 - acc: 0.0314\n",
      "Epoch 88/1000\n",
      "3505/3505 [==============================] - 12s 4ms/step - loss: 4.5462 - acc: 0.0317\n",
      "Epoch 89/1000\n",
      "3505/3505 [==============================] - 12s 4ms/step - loss: 4.5444 - acc: 0.0311\n",
      "Epoch 90/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 4.5463 - acc: 0.0282\n",
      "Epoch 91/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 4.5252 - acc: 0.0302\n",
      "Epoch 92/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 4.5188 - acc: 0.0354\n",
      "Epoch 93/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 4.5063 - acc: 0.0340\n",
      "Epoch 94/1000\n",
      "3505/3505 [==============================] - 12s 4ms/step - loss: 4.5116 - acc: 0.0308\n",
      "Epoch 95/1000\n",
      "3505/3505 [==============================] - 12s 4ms/step - loss: 4.4993 - acc: 0.0394\n",
      "Epoch 96/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 4.4849 - acc: 0.0297\n",
      "Epoch 97/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 4.4823 - acc: 0.0320\n",
      "Epoch 98/1000\n",
      "3505/3505 [==============================] - 12s 4ms/step - loss: 4.4820 - acc: 0.0320\n",
      "Epoch 99/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 4.4641 - acc: 0.0322\n",
      "Epoch 100/1000\n",
      "3505/3505 [==============================] - 12s 4ms/step - loss: 4.4727 - acc: 0.0368\n",
      "Epoch 101/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 4.4578 - acc: 0.0377\n",
      "Epoch 102/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 4.4525 - acc: 0.0340\n",
      "Epoch 103/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 4.4571 - acc: 0.0342\n",
      "Epoch 104/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 4.4355 - acc: 0.0428\n",
      "Epoch 105/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 4.4266 - acc: 0.0340\n",
      "Epoch 106/1000\n",
      "3505/3505 [==============================] - 12s 4ms/step - loss: 4.4296 - acc: 0.0414\n",
      "Epoch 107/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 4.4297 - acc: 0.0379\n",
      "Epoch 108/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 4.4292 - acc: 0.0328\n",
      "Epoch 109/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 4.4236 - acc: 0.0322\n",
      "Epoch 110/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 4.4247 - acc: 0.0379\n",
      "Epoch 111/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 4.3987 - acc: 0.0388\n",
      "Epoch 112/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 4.3944 - acc: 0.0397\n",
      "Epoch 113/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 4.3915 - acc: 0.0408\n",
      "Epoch 114/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 4.3827 - acc: 0.0374\n",
      "Epoch 115/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 4.3867 - acc: 0.0377\n",
      "Epoch 116/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 4.3799 - acc: 0.0428\n",
      "Epoch 117/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 4.3779 - acc: 0.0417\n",
      "Epoch 118/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 4.3720 - acc: 0.0417\n",
      "Epoch 119/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 4.3639 - acc: 0.0399\n",
      "Epoch 120/1000\n",
      "3505/3505 [==============================] - 12s 4ms/step - loss: 4.3543 - acc: 0.0417\n",
      "Epoch 121/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 4.3569 - acc: 0.0385\n",
      "Epoch 122/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 4.3483 - acc: 0.0377\n",
      "Epoch 123/1000\n",
      "3505/3505 [==============================] - 12s 4ms/step - loss: 4.3406 - acc: 0.0397\n",
      "Epoch 124/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 4.3442 - acc: 0.0414\n",
      "Epoch 125/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 4.3426 - acc: 0.0428\n",
      "Epoch 126/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 4.3251 - acc: 0.0428\n",
      "Epoch 127/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 4.3306 - acc: 0.0351\n",
      "Epoch 128/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 4.3238 - acc: 0.0442\n",
      "Epoch 129/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 4.3285 - acc: 0.0431\n",
      "Epoch 130/1000\n",
      "3505/3505 [==============================] - 12s 4ms/step - loss: 4.3326 - acc: 0.0408\n",
      "Epoch 131/1000\n",
      "3505/3505 [==============================] - 12s 4ms/step - loss: 4.2988 - acc: 0.0471\n",
      "Epoch 132/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 4.3242 - acc: 0.0371\n",
      "Epoch 133/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 4.2959 - acc: 0.0448\n",
      "Epoch 134/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 4.3056 - acc: 0.0465\n",
      "Epoch 135/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 4.2974 - acc: 0.0419\n",
      "Epoch 136/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 4.2965 - acc: 0.0454\n",
      "Epoch 137/1000\n",
      "3505/3505 [==============================] - 12s 4ms/step - loss: 4.2906 - acc: 0.0408\n",
      "Epoch 138/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 4.2832 - acc: 0.0431\n",
      "Epoch 139/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 4.2865 - acc: 0.0479\n",
      "Epoch 140/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 4.2700 - acc: 0.0491\n",
      "Epoch 141/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 4.2808 - acc: 0.0437\n",
      "Epoch 142/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 4.2629 - acc: 0.0425\n",
      "Epoch 143/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 4.2616 - acc: 0.0488\n",
      "Epoch 144/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 4.2680 - acc: 0.0434\n",
      "Epoch 145/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 4.2598 - acc: 0.0437\n",
      "Epoch 146/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 4.3547 - acc: 0.0408\n",
      "Epoch 147/1000\n",
      "3505/3505 [==============================] - 12s 4ms/step - loss: 4.2340 - acc: 0.0476\n",
      "Epoch 148/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 4.2462 - acc: 0.0474\n",
      "Epoch 149/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 4.2621 - acc: 0.0456\n",
      "Epoch 150/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 4.2351 - acc: 0.0491\n",
      "Epoch 151/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3505/3505 [==============================] - 13s 4ms/step - loss: 4.2522 - acc: 0.0437\n",
      "Epoch 152/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 4.2290 - acc: 0.0511\n",
      "Epoch 153/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 4.2383 - acc: 0.0462\n",
      "Epoch 154/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 4.2267 - acc: 0.0491\n",
      "Epoch 155/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 4.2284 - acc: 0.0462\n",
      "Epoch 156/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 4.2180 - acc: 0.0485\n",
      "Epoch 157/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 4.2317 - acc: 0.0462\n",
      "Epoch 158/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 4.2203 - acc: 0.0536\n",
      "Epoch 159/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 4.2068 - acc: 0.0462\n",
      "Epoch 160/1000\n",
      "3505/3505 [==============================] - 12s 4ms/step - loss: 4.2128 - acc: 0.0519\n",
      "Epoch 161/1000\n",
      "3505/3505 [==============================] - 12s 4ms/step - loss: 4.2231 - acc: 0.0434\n",
      "Epoch 162/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 4.2444 - acc: 0.0511\n",
      "Epoch 163/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 4.1819 - acc: 0.0522\n",
      "Epoch 164/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 4.2038 - acc: 0.0496\n",
      "Epoch 165/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 4.2100 - acc: 0.0462\n",
      "Epoch 166/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 4.2028 - acc: 0.0505\n",
      "Epoch 167/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 4.1864 - acc: 0.0522\n",
      "Epoch 168/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 4.1855 - acc: 0.0528\n",
      "Epoch 169/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 4.1953 - acc: 0.0496\n",
      "Epoch 170/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 4.1895 - acc: 0.0531\n",
      "Epoch 171/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 4.1871 - acc: 0.0551\n",
      "Epoch 172/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 4.1921 - acc: 0.0471\n",
      "Epoch 173/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 4.1690 - acc: 0.0496\n",
      "Epoch 174/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 4.1941 - acc: 0.0454\n",
      "Epoch 175/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 4.1684 - acc: 0.0522\n",
      "Epoch 176/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 4.1661 - acc: 0.0499\n",
      "Epoch 177/1000\n",
      "3505/3505 [==============================] - 12s 4ms/step - loss: 4.1667 - acc: 0.0511\n",
      "Epoch 178/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 4.1691 - acc: 0.0502\n",
      "Epoch 179/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 4.1734 - acc: 0.0519\n",
      "Epoch 180/1000\n",
      "3505/3505 [==============================] - 12s 4ms/step - loss: 4.1678 - acc: 0.0459\n",
      "Epoch 181/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 4.1474 - acc: 0.0514\n",
      "Epoch 182/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 4.1789 - acc: 0.0485\n",
      "Epoch 183/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 4.1659 - acc: 0.0491\n",
      "Epoch 184/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 4.1563 - acc: 0.0531\n",
      "Epoch 185/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 4.1446 - acc: 0.0573\n",
      "Epoch 186/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 4.1527 - acc: 0.0465\n",
      "Epoch 187/1000\n",
      "3505/3505 [==============================] - 12s 4ms/step - loss: 4.1352 - acc: 0.0585\n",
      "Epoch 188/1000\n",
      "3505/3505 [==============================] - 12s 4ms/step - loss: 4.1448 - acc: 0.0556\n",
      "Epoch 189/1000\n",
      "3505/3505 [==============================] - 12s 4ms/step - loss: 4.1451 - acc: 0.0536\n",
      "Epoch 190/1000\n",
      "3505/3505 [==============================] - 12s 4ms/step - loss: 4.1451 - acc: 0.0534\n",
      "Epoch 191/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 4.1324 - acc: 0.0622\n",
      "Epoch 192/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 4.1465 - acc: 0.0528\n",
      "Epoch 193/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 4.1312 - acc: 0.0588\n",
      "Epoch 194/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 4.1314 - acc: 0.0534\n",
      "Epoch 195/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 4.1331 - acc: 0.0536\n",
      "Epoch 196/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 4.1283 - acc: 0.0525\n",
      "Epoch 197/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 4.1263 - acc: 0.0553\n",
      "Epoch 198/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 4.1262 - acc: 0.0559\n",
      "Epoch 199/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 4.1285 - acc: 0.0562\n",
      "Epoch 200/1000\n",
      "3505/3505 [==============================] - 12s 4ms/step - loss: 4.1246 - acc: 0.0591\n",
      "Epoch 201/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 4.1186 - acc: 0.0585\n",
      "Epoch 202/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 4.1060 - acc: 0.0593\n",
      "Epoch 203/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 4.1223 - acc: 0.0568\n",
      "Epoch 204/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 4.1062 - acc: 0.0571\n",
      "Epoch 205/1000\n",
      "3505/3505 [==============================] - 12s 4ms/step - loss: 4.1176 - acc: 0.0525\n",
      "Epoch 206/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 4.1049 - acc: 0.0553\n",
      "Epoch 207/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 4.0944 - acc: 0.0585\n",
      "Epoch 208/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 4.1013 - acc: 0.0585\n",
      "Epoch 209/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 4.0976 - acc: 0.0582\n",
      "Epoch 210/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 4.1088 - acc: 0.0596\n",
      "Epoch 211/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 4.0875 - acc: 0.0596\n",
      "Epoch 212/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 4.1016 - acc: 0.0588\n",
      "Epoch 213/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 4.0870 - acc: 0.0582\n",
      "Epoch 214/1000\n",
      "3505/3505 [==============================] - 12s 4ms/step - loss: 4.1040 - acc: 0.0579\n",
      "Epoch 215/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 4.0813 - acc: 0.0642\n",
      "Epoch 216/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 4.0879 - acc: 0.0571\n",
      "Epoch 217/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 4.0802 - acc: 0.0636\n",
      "Epoch 218/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 4.0921 - acc: 0.0599\n",
      "Epoch 219/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 4.0624 - acc: 0.0619\n",
      "Epoch 220/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 4.0877 - acc: 0.0562\n",
      "Epoch 221/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 4.0728 - acc: 0.0699\n",
      "Epoch 222/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 4.0906 - acc: 0.0605\n",
      "Epoch 223/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 4.0788 - acc: 0.0599\n",
      "Epoch 224/1000\n",
      "3505/3505 [==============================] - 12s 4ms/step - loss: 4.0592 - acc: 0.0636\n",
      "Epoch 225/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 4.0710 - acc: 0.0559\n",
      "Epoch 226/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 4.0761 - acc: 0.0608\n",
      "Epoch 227/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 4.0612 - acc: 0.0593\n",
      "Epoch 228/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 4.0684 - acc: 0.0653\n",
      "Epoch 229/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 4.0529 - acc: 0.0631\n",
      "Epoch 230/1000\n",
      "3505/3505 [==============================] - 12s 4ms/step - loss: 4.0638 - acc: 0.0622\n",
      "Epoch 231/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 4.0768 - acc: 0.0602\n",
      "Epoch 232/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 4.0702 - acc: 0.0599\n",
      "Epoch 233/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 4.0422 - acc: 0.0648\n",
      "Epoch 234/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 4.0647 - acc: 0.0611\n",
      "Epoch 235/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 4.0392 - acc: 0.0668\n",
      "Epoch 236/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 4.0627 - acc: 0.0562\n",
      "Epoch 237/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 4.0408 - acc: 0.0633\n",
      "Epoch 238/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 4.0590 - acc: 0.0628\n",
      "Epoch 239/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 4.0316 - acc: 0.0639\n",
      "Epoch 240/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 4.0313 - acc: 0.0659\n",
      "Epoch 241/1000\n",
      "3505/3505 [==============================] - 12s 4ms/step - loss: 4.0561 - acc: 0.0585\n",
      "Epoch 242/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 4.0364 - acc: 0.0602\n",
      "Epoch 243/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 4.0700 - acc: 0.0596\n",
      "Epoch 244/1000\n",
      "3505/3505 [==============================] - 12s 4ms/step - loss: 4.0424 - acc: 0.0611\n",
      "Epoch 245/1000\n",
      "3505/3505 [==============================] - 12s 4ms/step - loss: 4.0599 - acc: 0.0602\n",
      "Epoch 246/1000\n",
      "3505/3505 [==============================] - 12s 4ms/step - loss: 4.0417 - acc: 0.0619\n",
      "Epoch 247/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 4.0375 - acc: 0.0690\n",
      "Epoch 248/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 4.0300 - acc: 0.0688\n",
      "Epoch 249/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 4.0301 - acc: 0.0702\n",
      "Epoch 250/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 4.0576 - acc: 0.0593\n",
      "Epoch 251/1000\n",
      "3505/3505 [==============================] - 12s 4ms/step - loss: 4.0468 - acc: 0.0676\n",
      "Epoch 252/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 4.0310 - acc: 0.0659\n",
      "Epoch 253/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 4.0224 - acc: 0.0639\n",
      "Epoch 254/1000\n",
      "3505/3505 [==============================] - 12s 4ms/step - loss: 4.0408 - acc: 0.0596\n",
      "Epoch 255/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 4.0186 - acc: 0.0699\n",
      "Epoch 256/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 4.0175 - acc: 0.0645\n",
      "Epoch 257/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 4.0217 - acc: 0.0685\n",
      "Epoch 258/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 4.0056 - acc: 0.0650\n",
      "Epoch 259/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 4.0094 - acc: 0.0668\n",
      "Epoch 260/1000\n",
      "3505/3505 [==============================] - 12s 4ms/step - loss: 4.0153 - acc: 0.0628\n",
      "Epoch 261/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 4.0135 - acc: 0.0736\n",
      "Epoch 262/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 4.0055 - acc: 0.0685\n",
      "Epoch 263/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 4.0175 - acc: 0.0653\n",
      "Epoch 264/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 4.0100 - acc: 0.0668\n",
      "Epoch 265/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.9935 - acc: 0.0676\n",
      "Epoch 266/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.9990 - acc: 0.0685\n",
      "Epoch 267/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 4.0004 - acc: 0.0676\n",
      "Epoch 268/1000\n",
      "3505/3505 [==============================] - 12s 4ms/step - loss: 3.9989 - acc: 0.0636\n",
      "Epoch 269/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.9817 - acc: 0.0725\n",
      "Epoch 270/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.9976 - acc: 0.0645\n",
      "Epoch 271/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.9946 - acc: 0.0648\n",
      "Epoch 272/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.9877 - acc: 0.0665\n",
      "Epoch 273/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.9729 - acc: 0.0705\n",
      "Epoch 274/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 4.0041 - acc: 0.0642\n",
      "Epoch 275/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.9764 - acc: 0.0685\n",
      "Epoch 276/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.9814 - acc: 0.0688\n",
      "Epoch 277/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.9640 - acc: 0.0730\n",
      "Epoch 278/1000\n",
      "3505/3505 [==============================] - 12s 4ms/step - loss: 3.9717 - acc: 0.0699\n",
      "Epoch 279/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.9755 - acc: 0.0653\n",
      "Epoch 280/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.9740 - acc: 0.0745\n",
      "Epoch 281/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.9676 - acc: 0.0710\n",
      "Epoch 282/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.9638 - acc: 0.0753\n",
      "Epoch 283/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.9724 - acc: 0.0685\n",
      "Epoch 284/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.9695 - acc: 0.0628\n",
      "Epoch 285/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.9599 - acc: 0.0696\n",
      "Epoch 286/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.9745 - acc: 0.0708\n",
      "Epoch 287/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.9518 - acc: 0.0733\n",
      "Epoch 288/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.9659 - acc: 0.0682\n",
      "Epoch 289/1000\n",
      "3505/3505 [==============================] - 12s 4ms/step - loss: 3.9483 - acc: 0.0776\n",
      "Epoch 290/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.9527 - acc: 0.0653\n",
      "Epoch 291/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.9590 - acc: 0.0685\n",
      "Epoch 292/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.9572 - acc: 0.0702\n",
      "Epoch 293/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.9526 - acc: 0.0736\n",
      "Epoch 294/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.9518 - acc: 0.0699\n",
      "Epoch 295/1000\n",
      "3505/3505 [==============================] - 12s 4ms/step - loss: 3.9452 - acc: 0.0762\n",
      "Epoch 296/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.9457 - acc: 0.0742\n",
      "Epoch 297/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.9411 - acc: 0.0693\n",
      "Epoch 298/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.9555 - acc: 0.0693\n",
      "Epoch 299/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.9360 - acc: 0.0739\n",
      "Epoch 300/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.9406 - acc: 0.0773\n",
      "Epoch 301/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.9238 - acc: 0.0742\n",
      "Epoch 302/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.9346 - acc: 0.0710\n",
      "Epoch 303/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.9616 - acc: 0.0662\n",
      "Epoch 304/1000\n",
      "3505/3505 [==============================] - 12s 4ms/step - loss: 3.9101 - acc: 0.0796\n",
      "Epoch 305/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.9307 - acc: 0.0733\n",
      "Epoch 306/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.9471 - acc: 0.0676\n",
      "Epoch 307/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.9231 - acc: 0.0762\n",
      "Epoch 308/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.9142 - acc: 0.0696\n",
      "Epoch 309/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.9409 - acc: 0.0682\n",
      "Epoch 310/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.9140 - acc: 0.0748\n",
      "Epoch 311/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.9155 - acc: 0.0759\n",
      "Epoch 312/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.9181 - acc: 0.0762\n",
      "Epoch 313/1000\n",
      "3505/3505 [==============================] - 12s 4ms/step - loss: 3.9213 - acc: 0.0753\n",
      "Epoch 314/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.9148 - acc: 0.0719\n",
      "Epoch 315/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.9156 - acc: 0.0685\n",
      "Epoch 316/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.9016 - acc: 0.0799\n",
      "Epoch 317/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.9085 - acc: 0.0816\n",
      "Epoch 318/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.9100 - acc: 0.0716\n",
      "Epoch 319/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.9214 - acc: 0.0759\n",
      "Epoch 320/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.9009 - acc: 0.0810\n",
      "Epoch 321/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.9048 - acc: 0.0748\n",
      "Epoch 322/1000\n",
      "3505/3505 [==============================] - 12s 4ms/step - loss: 3.9140 - acc: 0.0810\n",
      "Epoch 323/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.9030 - acc: 0.0765\n",
      "Epoch 324/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.8908 - acc: 0.0748\n",
      "Epoch 325/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.9015 - acc: 0.0762\n",
      "Epoch 326/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.8910 - acc: 0.0782\n",
      "Epoch 327/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.9032 - acc: 0.0736\n",
      "Epoch 328/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.8909 - acc: 0.0807\n",
      "Epoch 329/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.9094 - acc: 0.0699\n",
      "Epoch 330/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.8858 - acc: 0.0756\n",
      "Epoch 331/1000\n",
      "3505/3505 [==============================] - 12s 4ms/step - loss: 3.8895 - acc: 0.0750\n",
      "Epoch 332/1000\n",
      "3505/3505 [==============================] - 12s 4ms/step - loss: 3.8825 - acc: 0.0807\n",
      "Epoch 333/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.8921 - acc: 0.0770\n",
      "Epoch 334/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.8854 - acc: 0.0782\n",
      "Epoch 335/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.8846 - acc: 0.0767\n",
      "Epoch 336/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.8692 - acc: 0.0773\n",
      "Epoch 337/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.8916 - acc: 0.0807\n",
      "Epoch 338/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.8709 - acc: 0.0833\n",
      "Epoch 339/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.8590 - acc: 0.0879\n",
      "Epoch 340/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.8693 - acc: 0.0810\n",
      "Epoch 341/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.8673 - acc: 0.0796\n",
      "Epoch 342/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.8810 - acc: 0.0776\n",
      "Epoch 343/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.8540 - acc: 0.0845\n",
      "Epoch 344/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.8551 - acc: 0.0787\n",
      "Epoch 345/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.8965 - acc: 0.0742\n",
      "Epoch 346/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.8463 - acc: 0.0819\n",
      "Epoch 347/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.8681 - acc: 0.0776\n",
      "Epoch 348/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.8681 - acc: 0.0853\n",
      "Epoch 349/1000\n",
      "3505/3505 [==============================] - 12s 4ms/step - loss: 3.8637 - acc: 0.0773\n",
      "Epoch 350/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.8589 - acc: 0.0836\n",
      "Epoch 351/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.8511 - acc: 0.0853\n",
      "Epoch 352/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.8660 - acc: 0.0767\n",
      "Epoch 353/1000\n",
      "3505/3505 [==============================] - 12s 4ms/step - loss: 3.8404 - acc: 0.0873\n",
      "Epoch 354/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.8448 - acc: 0.0802\n",
      "Epoch 355/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.8524 - acc: 0.0787\n",
      "Epoch 356/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.8463 - acc: 0.0807\n",
      "Epoch 357/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.8357 - acc: 0.0819\n",
      "Epoch 358/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.8407 - acc: 0.0845\n",
      "Epoch 359/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.8525 - acc: 0.0807\n",
      "Epoch 360/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.8460 - acc: 0.0816\n",
      "Epoch 361/1000\n",
      "3505/3505 [==============================] - 12s 4ms/step - loss: 3.8321 - acc: 0.0830\n",
      "Epoch 362/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.8337 - acc: 0.0822\n",
      "Epoch 363/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.8487 - acc: 0.0825\n",
      "Epoch 364/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.8432 - acc: 0.0779\n",
      "Epoch 365/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.8124 - acc: 0.0882\n",
      "Epoch 366/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.8276 - acc: 0.0836\n",
      "Epoch 367/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.8453 - acc: 0.0750\n",
      "Epoch 368/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.8332 - acc: 0.0836\n",
      "Epoch 369/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.8412 - acc: 0.0839\n",
      "Epoch 370/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.8210 - acc: 0.0816\n",
      "Epoch 371/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.8362 - acc: 0.0827\n",
      "Epoch 372/1000\n",
      "3505/3505 [==============================] - 12s 4ms/step - loss: 3.8276 - acc: 0.0864\n",
      "Epoch 373/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.8265 - acc: 0.0830\n",
      "Epoch 374/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.8339 - acc: 0.0802\n",
      "Epoch 375/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.8114 - acc: 0.0867\n",
      "Epoch 376/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.8210 - acc: 0.0859\n",
      "Epoch 377/1000\n",
      "3505/3505 [==============================] - 12s 4ms/step - loss: 3.8176 - acc: 0.0842\n",
      "Epoch 378/1000\n",
      "3505/3505 [==============================] - 12s 4ms/step - loss: 3.8165 - acc: 0.0793\n",
      "Epoch 379/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.8148 - acc: 0.0825\n",
      "Epoch 380/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.8030 - acc: 0.0867\n",
      "Epoch 381/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.8350 - acc: 0.0853\n",
      "Epoch 382/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.7975 - acc: 0.0942\n",
      "Epoch 383/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.7865 - acc: 0.0867\n",
      "Epoch 384/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.8192 - acc: 0.0807\n",
      "Epoch 385/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.8049 - acc: 0.0859\n",
      "Epoch 386/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.8019 - acc: 0.0810\n",
      "Epoch 387/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.8015 - acc: 0.0864\n",
      "Epoch 388/1000\n",
      "3505/3505 [==============================] - 12s 4ms/step - loss: 3.8114 - acc: 0.0813\n",
      "Epoch 389/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.7963 - acc: 0.0859\n",
      "Epoch 390/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.7919 - acc: 0.0842\n",
      "Epoch 391/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.8005 - acc: 0.0862\n",
      "Epoch 392/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.8025 - acc: 0.0887\n",
      "Epoch 393/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.7891 - acc: 0.0833\n",
      "Epoch 394/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.7954 - acc: 0.0890\n",
      "Epoch 395/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.7960 - acc: 0.0810\n",
      "Epoch 396/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.7844 - acc: 0.0859\n",
      "Epoch 397/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.7913 - acc: 0.0862\n",
      "Epoch 398/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.7875 - acc: 0.0864\n",
      "Epoch 399/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.7765 - acc: 0.0873\n",
      "Epoch 400/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.7695 - acc: 0.0913\n",
      "Epoch 401/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.7964 - acc: 0.0856\n",
      "Epoch 402/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.7747 - acc: 0.0904\n",
      "Epoch 403/1000\n",
      "3505/3505 [==============================] - 12s 4ms/step - loss: 3.7676 - acc: 0.0916\n",
      "Epoch 404/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.7799 - acc: 0.0870\n",
      "Epoch 405/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.7919 - acc: 0.0887\n",
      "Epoch 406/1000\n",
      "3505/3505 [==============================] - 12s 4ms/step - loss: 3.7531 - acc: 0.0950\n",
      "Epoch 407/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.7774 - acc: 0.0859\n",
      "Epoch 408/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.7820 - acc: 0.0842\n",
      "Epoch 409/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.7618 - acc: 0.0879\n",
      "Epoch 410/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.7729 - acc: 0.0902\n",
      "Epoch 411/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.7588 - acc: 0.0902\n",
      "Epoch 412/1000\n",
      "3505/3505 [==============================] - 12s 4ms/step - loss: 3.7596 - acc: 0.0947\n",
      "Epoch 413/1000\n",
      "3505/3505 [==============================] - 12s 4ms/step - loss: 3.7726 - acc: 0.0876\n",
      "Epoch 414/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.7650 - acc: 0.0887\n",
      "Epoch 415/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.7512 - acc: 0.0933\n",
      "Epoch 416/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.7597 - acc: 0.0879\n",
      "Epoch 417/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.7588 - acc: 0.0913\n",
      "Epoch 418/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.7492 - acc: 0.0904\n",
      "Epoch 419/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.7772 - acc: 0.0896\n",
      "Epoch 420/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.7309 - acc: 0.0924\n",
      "Epoch 421/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.8200 - acc: 0.0847\n",
      "Epoch 422/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.7780 - acc: 0.0902\n",
      "Epoch 423/1000\n",
      "3505/3505 [==============================] - 12s 4ms/step - loss: 3.7575 - acc: 0.0953\n",
      "Epoch 424/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.7937 - acc: 0.0856\n",
      "Epoch 425/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.7536 - acc: 0.0987\n",
      "Epoch 426/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.7590 - acc: 0.0882\n",
      "Epoch 427/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.7540 - acc: 0.0879\n",
      "Epoch 428/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.7366 - acc: 0.0953\n",
      "Epoch 429/1000\n",
      "3505/3505 [==============================] - 12s 4ms/step - loss: 3.7773 - acc: 0.0884\n",
      "Epoch 430/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.7260 - acc: 0.0981\n",
      "Epoch 431/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.7536 - acc: 0.0913\n",
      "Epoch 432/1000\n",
      "3505/3505 [==============================] - 12s 4ms/step - loss: 3.7652 - acc: 0.0916\n",
      "Epoch 433/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.7334 - acc: 0.0887\n",
      "Epoch 434/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.7501 - acc: 0.0959\n",
      "Epoch 435/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.7478 - acc: 0.0896\n",
      "Epoch 436/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.7391 - acc: 0.0884\n",
      "Epoch 437/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.7616 - acc: 0.0899\n",
      "Epoch 438/1000\n",
      "3505/3505 [==============================] - 12s 4ms/step - loss: 3.7336 - acc: 0.0930\n",
      "Epoch 439/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.7316 - acc: 0.0916\n",
      "Epoch 440/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.7471 - acc: 0.0896\n",
      "Epoch 441/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.7374 - acc: 0.0964\n",
      "Epoch 442/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.7262 - acc: 0.0922\n",
      "Epoch 443/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.7407 - acc: 0.0933\n",
      "Epoch 444/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.7491 - acc: 0.0933\n",
      "Epoch 445/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.7189 - acc: 0.0987\n",
      "Epoch 446/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.7307 - acc: 0.0910\n",
      "Epoch 447/1000\n",
      "3505/3505 [==============================] - 12s 4ms/step - loss: 3.7262 - acc: 0.0879\n",
      "Epoch 448/1000\n",
      "3505/3505 [==============================] - 12s 4ms/step - loss: 3.7156 - acc: 0.0922\n",
      "Epoch 449/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.7201 - acc: 0.0930\n",
      "Epoch 450/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.7436 - acc: 0.0864\n",
      "Epoch 451/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.7181 - acc: 0.0979\n",
      "Epoch 452/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.7281 - acc: 0.0913\n",
      "Epoch 453/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.7348 - acc: 0.0862\n",
      "Epoch 454/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.7134 - acc: 0.0899\n",
      "Epoch 455/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.7064 - acc: 0.0970\n",
      "Epoch 456/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.7180 - acc: 0.0973\n",
      "Epoch 457/1000\n",
      "3505/3505 [==============================] - 12s 4ms/step - loss: 3.6978 - acc: 0.0936\n",
      "Epoch 458/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.7153 - acc: 0.0964\n",
      "Epoch 459/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.7273 - acc: 0.0950\n",
      "Epoch 460/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.6975 - acc: 0.0950\n",
      "Epoch 461/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.7357 - acc: 0.0919\n",
      "Epoch 462/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.7175 - acc: 0.0961\n",
      "Epoch 463/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.6907 - acc: 0.1010\n",
      "Epoch 464/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.7044 - acc: 0.0933\n",
      "Epoch 465/1000\n",
      "3505/3505 [==============================] - 12s 4ms/step - loss: 3.7057 - acc: 0.0967\n",
      "Epoch 466/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.6989 - acc: 0.1019\n",
      "Epoch 467/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.6910 - acc: 0.0967\n",
      "Epoch 468/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.7110 - acc: 0.0950\n",
      "Epoch 469/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.6850 - acc: 0.1027\n",
      "Epoch 470/1000\n",
      "3505/3505 [==============================] - 12s 4ms/step - loss: 3.6943 - acc: 0.0993\n",
      "Epoch 471/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.6776 - acc: 0.0944\n",
      "Epoch 472/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.6999 - acc: 0.0999\n",
      "Epoch 473/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.7000 - acc: 0.0950\n",
      "Epoch 474/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.6730 - acc: 0.1027\n",
      "Epoch 475/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.6984 - acc: 0.0916\n",
      "Epoch 476/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.6906 - acc: 0.0942\n",
      "Epoch 477/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.6951 - acc: 0.0961\n",
      "Epoch 478/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.6623 - acc: 0.1058\n",
      "Epoch 479/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.6740 - acc: 0.1016\n",
      "Epoch 480/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.6879 - acc: 0.0984\n",
      "Epoch 481/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.6551 - acc: 0.0973\n",
      "Epoch 482/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.6804 - acc: 0.0959\n",
      "Epoch 483/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.6590 - acc: 0.0993\n",
      "Epoch 484/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.7310 - acc: 0.0936\n",
      "Epoch 485/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.6640 - acc: 0.1036\n",
      "Epoch 486/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.6523 - acc: 0.1041\n",
      "Epoch 487/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.6899 - acc: 0.1004\n",
      "Epoch 488/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.6616 - acc: 0.1033\n",
      "Epoch 489/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.6633 - acc: 0.0984\n",
      "Epoch 490/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.6645 - acc: 0.1024\n",
      "Epoch 491/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.6606 - acc: 0.1016\n",
      "Epoch 492/1000\n",
      "3505/3505 [==============================] - 12s 4ms/step - loss: 3.6587 - acc: 0.1061\n",
      "Epoch 493/1000\n",
      "3505/3505 [==============================] - 12s 4ms/step - loss: 3.6564 - acc: 0.1058\n",
      "Epoch 494/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.6651 - acc: 0.1016\n",
      "Epoch 495/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.6536 - acc: 0.1064\n",
      "Epoch 496/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.6584 - acc: 0.1064\n",
      "Epoch 497/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.6285 - acc: 0.1076\n",
      "Epoch 498/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.6535 - acc: 0.1016\n",
      "Epoch 499/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.6569 - acc: 0.1053\n",
      "Epoch 500/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.6472 - acc: 0.1039\n",
      "Epoch 501/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.6574 - acc: 0.0990\n",
      "Epoch 502/1000\n",
      "3505/3505 [==============================] - 12s 4ms/step - loss: 3.6406 - acc: 0.1058\n",
      "Epoch 503/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.6543 - acc: 0.1007\n",
      "Epoch 504/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.6562 - acc: 0.1036\n",
      "Epoch 505/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.6680 - acc: 0.1007\n",
      "Epoch 506/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.6736 - acc: 0.1027\n",
      "Epoch 507/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.6389 - acc: 0.1036\n",
      "Epoch 508/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.6297 - acc: 0.1073\n",
      "Epoch 509/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.6485 - acc: 0.1016\n",
      "Epoch 510/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.6102 - acc: 0.1076\n",
      "Epoch 511/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.6037 - acc: 0.1124\n",
      "Epoch 512/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.6236 - acc: 0.1081\n",
      "Epoch 513/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.6241 - acc: 0.1030\n",
      "Epoch 514/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.6122 - acc: 0.1090\n",
      "Epoch 515/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.6320 - acc: 0.1001\n",
      "Epoch 516/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.6226 - acc: 0.1064\n",
      "Epoch 517/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.6071 - acc: 0.1050\n",
      "Epoch 518/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.6252 - acc: 0.1041\n",
      "Epoch 519/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.6420 - acc: 0.1070\n",
      "Epoch 520/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.6196 - acc: 0.1073\n",
      "Epoch 521/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.6195 - acc: 0.1053\n",
      "Epoch 522/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.6275 - acc: 0.1024\n",
      "Epoch 523/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.6101 - acc: 0.1073\n",
      "Epoch 524/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.6343 - acc: 0.1067\n",
      "Epoch 525/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.6297 - acc: 0.1067\n",
      "Epoch 526/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.6237 - acc: 0.1016\n",
      "Epoch 527/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.6298 - acc: 0.1027\n",
      "Epoch 528/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.6126 - acc: 0.1087\n",
      "Epoch 529/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.5962 - acc: 0.1087\n",
      "Epoch 530/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.6075 - acc: 0.1107\n",
      "Epoch 531/1000\n",
      "3505/3505 [==============================] - 12s 4ms/step - loss: 3.6109 - acc: 0.1104\n",
      "Epoch 532/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.6073 - acc: 0.1041\n",
      "Epoch 533/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.6027 - acc: 0.1104\n",
      "Epoch 534/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.5926 - acc: 0.1116\n",
      "Epoch 535/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.6058 - acc: 0.1078\n",
      "Epoch 536/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.6198 - acc: 0.1047\n",
      "Epoch 537/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.6034 - acc: 0.1070\n",
      "Epoch 538/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.5980 - acc: 0.1133\n",
      "Epoch 539/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.6076 - acc: 0.1010\n",
      "Epoch 540/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.5898 - acc: 0.1087\n",
      "Epoch 541/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.6023 - acc: 0.1058\n",
      "Epoch 542/1000\n",
      "3505/3505 [==============================] - 12s 4ms/step - loss: 3.5872 - acc: 0.1130\n",
      "Epoch 543/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.6150 - acc: 0.1104\n",
      "Epoch 544/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.5673 - acc: 0.1107\n",
      "Epoch 545/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.5773 - acc: 0.1096\n",
      "Epoch 546/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.5927 - acc: 0.1121\n",
      "Epoch 547/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.5787 - acc: 0.1144\n",
      "Epoch 548/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.5740 - acc: 0.1076\n",
      "Epoch 549/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.5685 - acc: 0.1107\n",
      "Epoch 550/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.5805 - acc: 0.1113\n",
      "Epoch 551/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.5703 - acc: 0.1184\n",
      "Epoch 552/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.6098 - acc: 0.1067\n",
      "Epoch 553/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.5871 - acc: 0.1098\n",
      "Epoch 554/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.5793 - acc: 0.1113\n",
      "Epoch 555/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.5717 - acc: 0.1138\n",
      "Epoch 556/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.5819 - acc: 0.1167\n",
      "Epoch 557/1000\n",
      "3505/3505 [==============================] - 12s 4ms/step - loss: 3.5460 - acc: 0.1167\n",
      "Epoch 558/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.5531 - acc: 0.1198\n",
      "Epoch 559/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.6091 - acc: 0.1136\n",
      "Epoch 560/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.5927 - acc: 0.1090\n",
      "Epoch 561/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.5490 - acc: 0.1116\n",
      "Epoch 562/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.5606 - acc: 0.1127\n",
      "Epoch 563/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.5920 - acc: 0.1041\n",
      "Epoch 564/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.5464 - acc: 0.1210\n",
      "Epoch 565/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.5757 - acc: 0.1150\n",
      "Epoch 566/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.5511 - acc: 0.1093\n",
      "Epoch 567/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.5500 - acc: 0.1213\n",
      "Epoch 568/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.5694 - acc: 0.1124\n",
      "Epoch 569/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.5446 - acc: 0.1121\n",
      "Epoch 570/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.5288 - acc: 0.1155\n",
      "Epoch 571/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.5448 - acc: 0.1218\n",
      "Epoch 572/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.5581 - acc: 0.1190\n",
      "Epoch 573/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.5228 - acc: 0.1167\n",
      "Epoch 574/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.5333 - acc: 0.1267\n",
      "Epoch 575/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.5415 - acc: 0.1153\n",
      "Epoch 576/1000\n",
      "3505/3505 [==============================] - 12s 4ms/step - loss: 3.5607 - acc: 0.1144\n",
      "Epoch 577/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.5419 - acc: 0.1193\n",
      "Epoch 578/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.5075 - acc: 0.1247\n",
      "Epoch 579/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.5513 - acc: 0.1150\n",
      "Epoch 580/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.5278 - acc: 0.1087\n",
      "Epoch 581/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.5666 - acc: 0.1107\n",
      "Epoch 582/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.5202 - acc: 0.1155\n",
      "Epoch 583/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.5469 - acc: 0.1147\n",
      "Epoch 584/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.5261 - acc: 0.1218\n",
      "Epoch 585/1000\n",
      "3505/3505 [==============================] - 12s 4ms/step - loss: 3.5063 - acc: 0.1213\n",
      "Epoch 586/1000\n",
      "3505/3505 [==============================] - 12s 4ms/step - loss: 3.5317 - acc: 0.1213\n",
      "Epoch 587/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.5687 - acc: 0.1070\n",
      "Epoch 588/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.4827 - acc: 0.1267\n",
      "Epoch 589/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.5155 - acc: 0.1184\n",
      "Epoch 590/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.5289 - acc: 0.1181\n",
      "Epoch 591/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.5346 - acc: 0.1147\n",
      "Epoch 592/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.4902 - acc: 0.1170\n",
      "Epoch 593/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.5053 - acc: 0.1244\n",
      "Epoch 594/1000\n",
      "3505/3505 [==============================] - 12s 4ms/step - loss: 3.5093 - acc: 0.1224\n",
      "Epoch 595/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.5226 - acc: 0.1158\n",
      "Epoch 596/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.5041 - acc: 0.1190\n",
      "Epoch 597/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.5238 - acc: 0.1221\n",
      "Epoch 598/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.5887 - acc: 0.1076\n",
      "Epoch 599/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.5115 - acc: 0.1221\n",
      "Epoch 600/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.4984 - acc: 0.1215\n",
      "Epoch 601/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.5034 - acc: 0.1178\n",
      "Epoch 602/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.4881 - acc: 0.1210\n",
      "Epoch 603/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.5063 - acc: 0.1207\n",
      "Epoch 604/1000\n",
      "3505/3505 [==============================] - 12s 4ms/step - loss: 3.4877 - acc: 0.1255\n",
      "Epoch 605/1000\n",
      "3505/3505 [==============================] - 12s 4ms/step - loss: 3.4734 - acc: 0.1250\n",
      "Epoch 606/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.5127 - acc: 0.1198\n",
      "Epoch 607/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.4757 - acc: 0.1272\n",
      "Epoch 608/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.4950 - acc: 0.1261\n",
      "Epoch 609/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.4950 - acc: 0.1221\n",
      "Epoch 610/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.5073 - acc: 0.1198\n",
      "Epoch 611/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.4892 - acc: 0.1204\n",
      "Epoch 612/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.4867 - acc: 0.1218\n",
      "Epoch 613/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.5178 - acc: 0.1116\n",
      "Epoch 614/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.4694 - acc: 0.1227\n",
      "Epoch 615/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.4916 - acc: 0.1252\n",
      "Epoch 616/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.4778 - acc: 0.1272\n",
      "Epoch 617/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.4637 - acc: 0.1261\n",
      "Epoch 618/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.4785 - acc: 0.1267\n",
      "Epoch 619/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.4590 - acc: 0.1301\n",
      "Epoch 620/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.4694 - acc: 0.1244\n",
      "Epoch 621/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.4970 - acc: 0.1213\n",
      "Epoch 622/1000\n",
      "3505/3505 [==============================] - 12s 4ms/step - loss: 3.4769 - acc: 0.1264\n",
      "Epoch 623/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.4699 - acc: 0.1261\n",
      "Epoch 624/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.5066 - acc: 0.1201\n",
      "Epoch 625/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.4620 - acc: 0.1241\n",
      "Epoch 626/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.4453 - acc: 0.1330\n",
      "Epoch 627/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.4732 - acc: 0.1218\n",
      "Epoch 628/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.4547 - acc: 0.1258\n",
      "Epoch 629/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.5229 - acc: 0.1124\n",
      "Epoch 630/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.4584 - acc: 0.1275\n",
      "Epoch 631/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.4440 - acc: 0.1275\n",
      "Epoch 632/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.4744 - acc: 0.1238\n",
      "Epoch 633/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.4243 - acc: 0.1304\n",
      "Epoch 634/1000\n",
      "3505/3505 [==============================] - 12s 4ms/step - loss: 3.4646 - acc: 0.1272\n",
      "Epoch 635/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.4659 - acc: 0.1244\n",
      "Epoch 636/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.4232 - acc: 0.1355\n",
      "Epoch 637/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.4334 - acc: 0.1304\n",
      "Epoch 638/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.4333 - acc: 0.1312\n",
      "Epoch 639/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.4224 - acc: 0.1330\n",
      "Epoch 640/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.4335 - acc: 0.1258\n",
      "Epoch 641/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.4052 - acc: 0.1255\n",
      "Epoch 642/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.4414 - acc: 0.1261\n",
      "Epoch 643/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.4355 - acc: 0.1264\n",
      "Epoch 644/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.4371 - acc: 0.1278\n",
      "Epoch 645/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.4071 - acc: 0.1358\n",
      "Epoch 646/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.4179 - acc: 0.1295\n",
      "Epoch 647/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.4016 - acc: 0.1367\n",
      "Epoch 648/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.3923 - acc: 0.1384\n",
      "Epoch 649/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.5327 - acc: 0.1147\n",
      "Epoch 650/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.3764 - acc: 0.1355\n",
      "Epoch 651/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.4266 - acc: 0.1298\n",
      "Epoch 652/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.4031 - acc: 0.1372\n",
      "Epoch 653/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.3624 - acc: 0.1441\n",
      "Epoch 654/1000\n",
      "3505/3505 [==============================] - 12s 4ms/step - loss: 3.3974 - acc: 0.1364\n",
      "Epoch 655/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.4024 - acc: 0.1338\n",
      "Epoch 656/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.5524 - acc: 0.1204\n",
      "Epoch 657/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.3783 - acc: 0.1352\n",
      "Epoch 658/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.3530 - acc: 0.1407\n",
      "Epoch 659/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.4203 - acc: 0.1278\n",
      "Epoch 660/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.4197 - acc: 0.1312\n",
      "Epoch 661/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.4068 - acc: 0.1304\n",
      "Epoch 662/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.3863 - acc: 0.1401\n",
      "Epoch 663/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.3683 - acc: 0.1364\n",
      "Epoch 664/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.4050 - acc: 0.1330\n",
      "Epoch 665/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.3867 - acc: 0.1361\n",
      "Epoch 666/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.3955 - acc: 0.1315\n",
      "Epoch 667/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.4187 - acc: 0.1307\n",
      "Epoch 668/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.3746 - acc: 0.1355\n",
      "Epoch 669/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.3828 - acc: 0.1398\n",
      "Epoch 670/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.4018 - acc: 0.1381\n",
      "Epoch 671/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.3793 - acc: 0.1389\n",
      "Epoch 672/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.3697 - acc: 0.1372\n",
      "Epoch 673/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.3707 - acc: 0.1364\n",
      "Epoch 674/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.3558 - acc: 0.1352\n",
      "Epoch 675/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.3771 - acc: 0.1347\n",
      "Epoch 676/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.3616 - acc: 0.1421\n",
      "Epoch 677/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.3460 - acc: 0.1449\n",
      "Epoch 678/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.4935 - acc: 0.1335\n",
      "Epoch 679/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.3499 - acc: 0.1424\n",
      "Epoch 680/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.3924 - acc: 0.1341\n",
      "Epoch 681/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.3934 - acc: 0.1358\n",
      "Epoch 682/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.3593 - acc: 0.1389\n",
      "Epoch 683/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.3632 - acc: 0.1358\n",
      "Epoch 684/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.4056 - acc: 0.1355\n",
      "Epoch 685/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.3631 - acc: 0.1389\n",
      "Epoch 686/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.3733 - acc: 0.1355\n",
      "Epoch 687/1000\n",
      "3505/3505 [==============================] - 12s 4ms/step - loss: 3.3389 - acc: 0.1361\n",
      "Epoch 688/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.3560 - acc: 0.1372\n",
      "Epoch 689/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.3383 - acc: 0.1412\n",
      "Epoch 690/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.3680 - acc: 0.1409\n",
      "Epoch 691/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.3322 - acc: 0.1364\n",
      "Epoch 692/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.3385 - acc: 0.1444\n",
      "Epoch 693/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.3616 - acc: 0.1347\n",
      "Epoch 694/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.3252 - acc: 0.1452\n",
      "Epoch 695/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.3115 - acc: 0.1404\n",
      "Epoch 696/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.3797 - acc: 0.1341\n",
      "Epoch 697/1000\n",
      "3505/3505 [==============================] - 12s 4ms/step - loss: 3.3157 - acc: 0.1435\n",
      "Epoch 698/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.3425 - acc: 0.1387\n",
      "Epoch 699/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.3228 - acc: 0.1481\n",
      "Epoch 700/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.3194 - acc: 0.1449\n",
      "Epoch 701/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.2934 - acc: 0.1427\n",
      "Epoch 702/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.3515 - acc: 0.1427\n",
      "Epoch 703/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.3107 - acc: 0.1501\n",
      "Epoch 704/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.3568 - acc: 0.1358\n",
      "Epoch 705/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.3511 - acc: 0.1338\n",
      "Epoch 706/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.3429 - acc: 0.1461\n",
      "Epoch 707/1000\n",
      "3505/3505 [==============================] - 12s 4ms/step - loss: 3.3313 - acc: 0.1438\n",
      "Epoch 708/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.3033 - acc: 0.1429\n",
      "Epoch 709/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.3391 - acc: 0.1404\n",
      "Epoch 710/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.2880 - acc: 0.1435\n",
      "Epoch 711/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.3032 - acc: 0.1458\n",
      "Epoch 712/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.3060 - acc: 0.1438\n",
      "Epoch 713/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.3169 - acc: 0.1381\n",
      "Epoch 714/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.2887 - acc: 0.1515\n",
      "Epoch 715/1000\n",
      "3505/3505 [==============================] - 12s 4ms/step - loss: 3.3129 - acc: 0.1367\n",
      "Epoch 716/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.3178 - acc: 0.1438\n",
      "Epoch 717/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.2853 - acc: 0.1492\n",
      "Epoch 718/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.3503 - acc: 0.1409\n",
      "Epoch 719/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.2645 - acc: 0.1498\n",
      "Epoch 720/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.2914 - acc: 0.1492\n",
      "Epoch 721/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.3621 - acc: 0.1347\n",
      "Epoch 722/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.3172 - acc: 0.1486\n",
      "Epoch 723/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.3596 - acc: 0.1387\n",
      "Epoch 724/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.3120 - acc: 0.1486\n",
      "Epoch 725/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.3153 - acc: 0.1429\n",
      "Epoch 726/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.4224 - acc: 0.1372\n",
      "Epoch 727/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.3315 - acc: 0.1478\n",
      "Epoch 728/1000\n",
      "3505/3505 [==============================] - 12s 4ms/step - loss: 3.2830 - acc: 0.1549\n",
      "Epoch 729/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.2914 - acc: 0.1506\n",
      "Epoch 730/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.5094 - acc: 0.1195\n",
      "Epoch 731/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.3479 - acc: 0.1418\n",
      "Epoch 732/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.3067 - acc: 0.1486\n",
      "Epoch 733/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.3269 - acc: 0.1361\n",
      "Epoch 734/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.2613 - acc: 0.1549\n",
      "Epoch 735/1000\n",
      "3505/3505 [==============================] - 12s 4ms/step - loss: 3.3004 - acc: 0.1458\n",
      "Epoch 736/1000\n",
      "3505/3505 [==============================] - 12s 4ms/step - loss: 3.3587 - acc: 0.1438\n",
      "Epoch 737/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.2957 - acc: 0.1501\n",
      "Epoch 738/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.2782 - acc: 0.1461\n",
      "Epoch 739/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.2666 - acc: 0.1541\n",
      "Epoch 740/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.3280 - acc: 0.1429\n",
      "Epoch 741/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.2686 - acc: 0.1535\n",
      "Epoch 742/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.2847 - acc: 0.1529\n",
      "Epoch 743/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.2469 - acc: 0.1504\n",
      "Epoch 744/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.2292 - acc: 0.1589\n",
      "Epoch 745/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.2293 - acc: 0.1621\n",
      "Epoch 746/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.2512 - acc: 0.1618\n",
      "Epoch 747/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.2694 - acc: 0.1512\n",
      "Epoch 748/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.2318 - acc: 0.1558\n",
      "Epoch 749/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.3079 - acc: 0.1541\n",
      "Epoch 750/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.2153 - acc: 0.1601\n",
      "Epoch 751/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.2464 - acc: 0.1589\n",
      "Epoch 752/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.2807 - acc: 0.1535\n",
      "Epoch 753/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.2217 - acc: 0.1578\n",
      "Epoch 754/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.2866 - acc: 0.1535\n",
      "Epoch 755/1000\n",
      "3505/3505 [==============================] - 12s 4ms/step - loss: 3.2787 - acc: 0.1504\n",
      "Epoch 756/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.2431 - acc: 0.1586\n",
      "Epoch 757/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.2439 - acc: 0.1549\n",
      "Epoch 758/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.2348 - acc: 0.1549\n",
      "Epoch 759/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.2554 - acc: 0.1538\n",
      "Epoch 760/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.2167 - acc: 0.1609\n",
      "Epoch 761/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.2963 - acc: 0.1478\n",
      "Epoch 762/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.2422 - acc: 0.1558\n",
      "Epoch 763/1000\n",
      "3505/3505 [==============================] - 12s 4ms/step - loss: 3.3024 - acc: 0.1421\n",
      "Epoch 764/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.1933 - acc: 0.1700\n",
      "Epoch 765/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.2319 - acc: 0.1581\n",
      "Epoch 766/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.1815 - acc: 0.1629\n",
      "Epoch 767/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.2397 - acc: 0.1544\n",
      "Epoch 768/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.2196 - acc: 0.1563\n",
      "Epoch 769/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.2426 - acc: 0.1581\n",
      "Epoch 770/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.1923 - acc: 0.1649\n",
      "Epoch 771/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.2267 - acc: 0.1532\n",
      "Epoch 772/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.1732 - acc: 0.1686\n",
      "Epoch 773/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.2163 - acc: 0.1589\n",
      "Epoch 774/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.2145 - acc: 0.1595\n",
      "Epoch 775/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.2468 - acc: 0.1598\n",
      "Epoch 776/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.2260 - acc: 0.1555\n",
      "Epoch 777/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.2488 - acc: 0.1549\n",
      "Epoch 778/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.2142 - acc: 0.1615\n",
      "Epoch 779/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.2041 - acc: 0.1589\n",
      "Epoch 780/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.2505 - acc: 0.1583\n",
      "Epoch 781/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.2352 - acc: 0.1535\n",
      "Epoch 782/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.2608 - acc: 0.1526\n",
      "Epoch 783/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.2113 - acc: 0.1592\n",
      "Epoch 784/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.2233 - acc: 0.1586\n",
      "Epoch 785/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.2276 - acc: 0.1635\n",
      "Epoch 786/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.1922 - acc: 0.1652\n",
      "Epoch 787/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.2102 - acc: 0.1615\n",
      "Epoch 788/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.1851 - acc: 0.1623\n",
      "Epoch 789/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.1818 - acc: 0.1646\n",
      "Epoch 790/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.1902 - acc: 0.1638\n",
      "Epoch 791/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.1851 - acc: 0.1621\n",
      "Epoch 792/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.1479 - acc: 0.1735\n",
      "Epoch 793/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.2412 - acc: 0.1609\n",
      "Epoch 794/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.1811 - acc: 0.1623\n",
      "Epoch 795/1000\n",
      "3505/3505 [==============================] - 12s 4ms/step - loss: 3.1689 - acc: 0.1666\n",
      "Epoch 796/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.1796 - acc: 0.1603\n",
      "Epoch 797/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.1762 - acc: 0.1660\n",
      "Epoch 798/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.1602 - acc: 0.1726\n",
      "Epoch 799/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.1619 - acc: 0.1726\n",
      "Epoch 800/1000\n",
      "3505/3505 [==============================] - 12s 4ms/step - loss: 3.1551 - acc: 0.1660\n",
      "Epoch 801/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.1778 - acc: 0.1658\n",
      "Epoch 802/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.1632 - acc: 0.1675\n",
      "Epoch 803/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.1610 - acc: 0.1698\n",
      "Epoch 804/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.1377 - acc: 0.1746\n",
      "Epoch 805/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.1698 - acc: 0.1660\n",
      "Epoch 806/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.1577 - acc: 0.1643\n",
      "Epoch 807/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.1653 - acc: 0.1720\n",
      "Epoch 808/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.1500 - acc: 0.1703\n",
      "Epoch 809/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.1923 - acc: 0.1666\n",
      "Epoch 810/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.1616 - acc: 0.1649\n",
      "Epoch 811/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.1088 - acc: 0.1738\n",
      "Epoch 812/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.1497 - acc: 0.1683\n",
      "Epoch 813/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.1711 - acc: 0.1678\n",
      "Epoch 814/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.1155 - acc: 0.1718\n",
      "Epoch 815/1000\n",
      "3505/3505 [==============================] - 12s 4ms/step - loss: 3.1545 - acc: 0.1703\n",
      "Epoch 816/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.1054 - acc: 0.1755\n",
      "Epoch 817/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.1460 - acc: 0.1760\n",
      "Epoch 818/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.1209 - acc: 0.1849\n",
      "Epoch 819/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.1716 - acc: 0.1689\n",
      "Epoch 820/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.2942 - acc: 0.1575\n",
      "Epoch 821/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.1157 - acc: 0.1743\n",
      "Epoch 822/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.1332 - acc: 0.1766\n",
      "Epoch 823/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.1143 - acc: 0.1763\n",
      "Epoch 824/1000\n",
      "3505/3505 [==============================] - 12s 4ms/step - loss: 3.1240 - acc: 0.1763\n",
      "Epoch 825/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.1310 - acc: 0.1777\n",
      "Epoch 826/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.0928 - acc: 0.1872\n",
      "Epoch 827/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.0983 - acc: 0.1863\n",
      "Epoch 828/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.1251 - acc: 0.1646\n",
      "Epoch 829/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.1251 - acc: 0.1812\n",
      "Epoch 830/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.1768 - acc: 0.1680\n",
      "Epoch 831/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.0952 - acc: 0.1826\n",
      "Epoch 832/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.1840 - acc: 0.1720\n",
      "Epoch 833/1000\n",
      "3505/3505 [==============================] - 12s 4ms/step - loss: 3.0765 - acc: 0.1869\n",
      "Epoch 834/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.1568 - acc: 0.1749\n",
      "Epoch 835/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.1060 - acc: 0.1732\n",
      "Epoch 836/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.2380 - acc: 0.1666\n",
      "Epoch 837/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.1014 - acc: 0.1792\n",
      "Epoch 838/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.0793 - acc: 0.1826\n",
      "Epoch 839/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.0933 - acc: 0.1800\n",
      "Epoch 840/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.2719 - acc: 0.1629\n",
      "Epoch 841/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.7035 - acc: 0.1175\n",
      "Epoch 842/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.1718 - acc: 0.1749\n",
      "Epoch 843/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.0791 - acc: 0.1920\n",
      "Epoch 844/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.0732 - acc: 0.1826\n",
      "Epoch 845/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.0769 - acc: 0.1903\n",
      "Epoch 846/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.0646 - acc: 0.1929\n",
      "Epoch 847/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.0693 - acc: 0.1863\n",
      "Epoch 848/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.0946 - acc: 0.1795\n",
      "Epoch 849/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.1009 - acc: 0.1749\n",
      "Epoch 850/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.0856 - acc: 0.1777\n",
      "Epoch 851/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.0997 - acc: 0.1880\n",
      "Epoch 852/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.0831 - acc: 0.1792\n",
      "Epoch 853/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.0895 - acc: 0.1840\n",
      "Epoch 854/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.0850 - acc: 0.1860\n",
      "Epoch 855/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.0271 - acc: 0.1883\n",
      "Epoch 856/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.0914 - acc: 0.1829\n",
      "Epoch 857/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.0722 - acc: 0.1829\n",
      "Epoch 858/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.1409 - acc: 0.1809\n",
      "Epoch 859/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.0285 - acc: 0.1892\n",
      "Epoch 860/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.0617 - acc: 0.1826\n",
      "Epoch 861/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.0657 - acc: 0.1849\n",
      "Epoch 862/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.0685 - acc: 0.1857\n",
      "Epoch 863/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.1122 - acc: 0.1775\n",
      "Epoch 864/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.0865 - acc: 0.1769\n",
      "Epoch 865/1000\n",
      "3505/3505 [==============================] - 12s 4ms/step - loss: 3.0584 - acc: 0.1843\n",
      "Epoch 866/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.0597 - acc: 0.1880\n",
      "Epoch 867/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.0740 - acc: 0.1852\n",
      "Epoch 868/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.0441 - acc: 0.1854\n",
      "Epoch 869/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.0383 - acc: 0.1843\n",
      "Epoch 870/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.0251 - acc: 0.1883\n",
      "Epoch 871/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.0468 - acc: 0.1940\n",
      "Epoch 872/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.0475 - acc: 0.1860\n",
      "Epoch 873/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.0494 - acc: 0.1803\n",
      "Epoch 874/1000\n",
      "3505/3505 [==============================] - 12s 4ms/step - loss: 3.0531 - acc: 0.1900\n",
      "Epoch 875/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.0230 - acc: 0.1874\n",
      "Epoch 876/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.0404 - acc: 0.1906\n",
      "Epoch 877/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.0497 - acc: 0.1883\n",
      "Epoch 878/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.0596 - acc: 0.1872\n",
      "Epoch 879/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.0508 - acc: 0.1826\n",
      "Epoch 880/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.0127 - acc: 0.1929\n",
      "Epoch 881/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.0554 - acc: 0.1837\n",
      "Epoch 882/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 2.9980 - acc: 0.1997\n",
      "Epoch 883/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.0331 - acc: 0.1932\n",
      "Epoch 884/1000\n",
      "3505/3505 [==============================] - 12s 4ms/step - loss: 3.0731 - acc: 0.1823\n",
      "Epoch 885/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.0303 - acc: 0.1903\n",
      "Epoch 886/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.0413 - acc: 0.1909\n",
      "Epoch 887/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.0681 - acc: 0.1843\n",
      "Epoch 888/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.0352 - acc: 0.1894\n",
      "Epoch 889/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.0152 - acc: 0.1934\n",
      "Epoch 890/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.1012 - acc: 0.1795\n",
      "Epoch 891/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.0189 - acc: 0.1923\n",
      "Epoch 892/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 2.9918 - acc: 0.1983\n",
      "Epoch 893/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.0108 - acc: 0.1949\n",
      "Epoch 894/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.0338 - acc: 0.1877\n",
      "Epoch 895/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.0871 - acc: 0.1826\n",
      "Epoch 896/1000\n",
      "3505/3505 [==============================] - 12s 4ms/step - loss: 3.0013 - acc: 0.1966\n",
      "Epoch 897/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 2.9979 - acc: 0.1974\n",
      "Epoch 898/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.0149 - acc: 0.1929\n",
      "Epoch 899/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.0273 - acc: 0.1923\n",
      "Epoch 900/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.0063 - acc: 0.1932\n",
      "Epoch 901/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.0577 - acc: 0.1872\n",
      "Epoch 902/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.0175 - acc: 0.1909\n",
      "Epoch 903/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 2.9788 - acc: 0.2006\n",
      "Epoch 904/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.0200 - acc: 0.2009\n",
      "Epoch 905/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.0397 - acc: 0.1837\n",
      "Epoch 906/1000\n",
      "3505/3505 [==============================] - 12s 4ms/step - loss: 3.0258 - acc: 0.1917\n",
      "Epoch 907/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.0436 - acc: 0.1860\n",
      "Epoch 908/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.0083 - acc: 0.1914\n",
      "Epoch 909/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 2.9859 - acc: 0.1997\n",
      "Epoch 910/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 2.9965 - acc: 0.2011\n",
      "Epoch 911/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.0010 - acc: 0.1926\n",
      "Epoch 912/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 2.9936 - acc: 0.1994\n",
      "Epoch 913/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 2.9818 - acc: 0.1980\n",
      "Epoch 914/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 2.9919 - acc: 0.1966\n",
      "Epoch 915/1000\n",
      "3505/3505 [==============================] - 12s 4ms/step - loss: 3.0003 - acc: 0.2009\n",
      "Epoch 916/1000\n",
      "3505/3505 [==============================] - 12s 4ms/step - loss: 2.9760 - acc: 0.1986\n",
      "Epoch 917/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.2948 - acc: 0.1638\n",
      "Epoch 918/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 2.9517 - acc: 0.2057\n",
      "Epoch 919/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 2.9854 - acc: 0.1983\n",
      "Epoch 920/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 2.9691 - acc: 0.2066\n",
      "Epoch 921/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 2.9669 - acc: 0.2049\n",
      "Epoch 922/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 2.9570 - acc: 0.2026\n",
      "Epoch 923/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.0302 - acc: 0.1937\n",
      "Epoch 924/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 2.9802 - acc: 0.2046\n",
      "Epoch 925/1000\n",
      "3505/3505 [==============================] - 12s 4ms/step - loss: 2.9838 - acc: 0.1997\n",
      "Epoch 926/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 2.9905 - acc: 0.1971\n",
      "Epoch 927/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.0785 - acc: 0.1820\n",
      "Epoch 928/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 2.9964 - acc: 0.1997\n",
      "Epoch 929/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 2.9692 - acc: 0.2043\n",
      "Epoch 930/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 2.9619 - acc: 0.2088\n",
      "Epoch 931/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 2.9719 - acc: 0.2040\n",
      "Epoch 932/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 2.9359 - acc: 0.2168\n",
      "Epoch 933/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 2.9272 - acc: 0.2117\n",
      "Epoch 934/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 2.9505 - acc: 0.2114\n",
      "Epoch 935/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 2.9584 - acc: 0.2051\n",
      "Epoch 936/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.0804 - acc: 0.1869\n",
      "Epoch 937/1000\n",
      "3505/3505 [==============================] - 12s 4ms/step - loss: 3.0437 - acc: 0.1846\n",
      "Epoch 938/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.0128 - acc: 0.2020\n",
      "Epoch 939/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 2.9460 - acc: 0.2017\n",
      "Epoch 940/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.0430 - acc: 0.1866\n",
      "Epoch 941/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 2.9673 - acc: 0.2049\n",
      "Epoch 942/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 2.9385 - acc: 0.2060\n",
      "Epoch 943/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 2.9769 - acc: 0.2017\n",
      "Epoch 944/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 2.9087 - acc: 0.2168\n",
      "Epoch 945/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 2.9351 - acc: 0.2066\n",
      "Epoch 946/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 2.9454 - acc: 0.2160\n",
      "Epoch 947/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 2.9436 - acc: 0.2111\n",
      "Epoch 948/1000\n",
      "3505/3505 [==============================] - 12s 4ms/step - loss: 2.9286 - acc: 0.2049\n",
      "Epoch 949/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 2.9196 - acc: 0.2140\n",
      "Epoch 950/1000\n",
      "3505/3505 [==============================] - 12s 4ms/step - loss: 2.9266 - acc: 0.2106\n",
      "Epoch 951/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3505/3505 [==============================] - 12s 3ms/step - loss: 2.9472 - acc: 0.2091\n",
      "Epoch 952/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 2.9293 - acc: 0.2106\n",
      "Epoch 953/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 2.9625 - acc: 0.2031\n",
      "Epoch 954/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 2.9640 - acc: 0.2029\n",
      "Epoch 955/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 2.9011 - acc: 0.2163\n",
      "Epoch 956/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 2.9814 - acc: 0.2091\n",
      "Epoch 957/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 2.9031 - acc: 0.2203\n",
      "Epoch 958/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 2.9130 - acc: 0.2160\n",
      "Epoch 959/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 2.9339 - acc: 0.2040\n",
      "Epoch 960/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 2.8835 - acc: 0.2217\n",
      "Epoch 961/1000\n",
      "3505/3505 [==============================] - 12s 4ms/step - loss: 2.9399 - acc: 0.2049\n",
      "Epoch 962/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 2.9567 - acc: 0.2049\n",
      "Epoch 963/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.0624 - acc: 0.1923\n",
      "Epoch 964/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.4654 - acc: 0.1472\n",
      "Epoch 965/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 3.0145 - acc: 0.2054\n",
      "Epoch 966/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 2.9502 - acc: 0.2117\n",
      "Epoch 967/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 2.9002 - acc: 0.2188\n",
      "Epoch 968/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 3.0717 - acc: 0.2049\n",
      "Epoch 969/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 2.8628 - acc: 0.2211\n",
      "Epoch 970/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 2.8886 - acc: 0.2211\n",
      "Epoch 971/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 2.9198 - acc: 0.2114\n",
      "Epoch 972/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 2.8846 - acc: 0.2231\n",
      "Epoch 973/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 2.9555 - acc: 0.2094\n",
      "Epoch 974/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 2.8885 - acc: 0.2188\n",
      "Epoch 975/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 2.8916 - acc: 0.2171\n",
      "Epoch 976/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 2.8818 - acc: 0.2177\n",
      "Epoch 977/1000\n",
      "3505/3505 [==============================] - 12s 4ms/step - loss: 2.8994 - acc: 0.2171\n",
      "Epoch 978/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 2.8884 - acc: 0.2165\n",
      "Epoch 979/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 2.8535 - acc: 0.2203\n",
      "Epoch 980/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 2.8818 - acc: 0.2245\n",
      "Epoch 981/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 2.9209 - acc: 0.2051\n",
      "Epoch 982/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 2.8947 - acc: 0.2203\n",
      "Epoch 983/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 2.9353 - acc: 0.2049\n",
      "Epoch 984/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 2.8619 - acc: 0.2225\n",
      "Epoch 985/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 2.8835 - acc: 0.2265\n",
      "Epoch 986/1000\n",
      "3505/3505 [==============================] - 12s 4ms/step - loss: 2.8725 - acc: 0.2231\n",
      "Epoch 987/1000\n",
      "3505/3505 [==============================] - 12s 4ms/step - loss: 2.9472 - acc: 0.2108\n",
      "Epoch 988/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 2.8598 - acc: 0.2163\n",
      "Epoch 989/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 2.9088 - acc: 0.2146\n",
      "Epoch 990/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 2.8912 - acc: 0.2188\n",
      "Epoch 991/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 2.8512 - acc: 0.2243\n",
      "Epoch 992/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 2.9501 - acc: 0.2137\n",
      "Epoch 993/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 2.9041 - acc: 0.2094\n",
      "Epoch 994/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 2.8835 - acc: 0.2154\n",
      "Epoch 995/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 2.8565 - acc: 0.2185\n",
      "Epoch 996/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 2.8338 - acc: 0.2322\n",
      "Epoch 997/1000\n",
      "3505/3505 [==============================] - 12s 3ms/step - loss: 2.8750 - acc: 0.2220\n",
      "Epoch 998/1000\n",
      "3505/3505 [==============================] - 13s 4ms/step - loss: 2.8513 - acc: 0.2191\n",
      "Epoch 999/1000\n",
      "3505/3505 [==============================] - 12s 4ms/step - loss: 2.8789 - acc: 0.2214\n",
      "Epoch 1000/1000\n",
      "3505/3505 [==============================] - 12s 4ms/step - loss: 2.8779 - acc: 0.2243\n",
      "Accuracy: 7.867731\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1(X_train_new, y_train, X_test_new, y_test,200,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_19 (Dense)             (None, 16)                16016     \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 15999)             143991    \n",
      "=================================================================\n",
      "Total params: 160,143\n",
      "Trainable params: 160,143\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/1000\n",
      "9324/9324 [==============================] - 5s 585us/step - loss: 9.0726 - acc: 5.3625e-04\n",
      "Epoch 2/1000\n",
      "9324/9324 [==============================] - 5s 499us/step - loss: 7.8403 - acc: 7.5075e-04\n",
      "Epoch 3/1000\n",
      "9324/9324 [==============================] - 5s 499us/step - loss: 7.4961 - acc: 0.0012\n",
      "Epoch 4/1000\n",
      "9324/9324 [==============================] - 5s 500us/step - loss: 7.2326 - acc: 0.0015 4s - - ETA: 1s - loss: 7.24\n",
      "Epoch 5/1000\n",
      "9324/9324 [==============================] - 5s 499us/step - loss: 6.9478 - acc: 0.0017\n",
      "Epoch 6/1000\n",
      "9324/9324 [==============================] - 5s 504us/step - loss: 6.7183 - acc: 0.0015ETA: \n",
      "Epoch 7/1000\n",
      "9324/9324 [==============================] - 5s 500us/step - loss: 6.5679 - acc: 0.0020\n",
      "Epoch 8/1000\n",
      "9324/9324 [==============================] - 5s 501us/step - loss: 6.4648 - acc: 0.0025\n",
      "Epoch 9/1000\n",
      "9324/9324 [==============================] - 5s 498us/step - loss: 6.3951 - acc: 0.0029\n",
      "Epoch 10/1000\n",
      "9324/9324 [==============================] - 5s 503us/step - loss: 6.3397 - acc: 0.0033\n",
      "Epoch 11/1000\n",
      "9324/9324 [==============================] - 5s 500us/step - loss: 6.2922 - acc: 0.0033\n",
      "Epoch 12/1000\n",
      "9324/9324 [==============================] - 5s 500us/step - loss: 6.2519 - acc: 0.0024\n",
      "Epoch 13/1000\n",
      "9324/9324 [==============================] - 5s 501us/step - loss: 6.2199 - acc: 0.0034\n",
      "Epoch 14/1000\n",
      "9324/9324 [==============================] - 5s 502us/step - loss: 6.1870 - acc: 0.0036 0s - loss: 6.1840 - acc: 0\n",
      "Epoch 15/1000\n",
      "9324/9324 [==============================] - 5s 505us/step - loss: 6.1616 - acc: 0.0033\n",
      "Epoch 16/1000\n",
      "9324/9324 [==============================] - 5s 500us/step - loss: 6.1291 - acc: 0.0038\n",
      "Epoch 17/1000\n",
      "9324/9324 [==============================] - 5s 501us/step - loss: 6.1028 - acc: 0.0049\n",
      "Epoch 18/1000\n",
      "9324/9324 [==============================] - 5s 512us/step - loss: 6.0746 - acc: 0.0054\n",
      "Epoch 19/1000\n",
      "9324/9324 [==============================] - 5s 506us/step - loss: 6.0513 - acc: 0.0047\n",
      "Epoch 20/1000\n",
      "9324/9324 [==============================] - 5s 500us/step - loss: 6.0192 - acc: 0.0050\n",
      "Epoch 21/1000\n",
      "9324/9324 [==============================] - 5s 502us/step - loss: 5.9913 - acc: 0.0064\n",
      "Epoch 22/1000\n",
      "9324/9324 [==============================] - 5s 503us/step - loss: 5.9594 - acc: 0.0075\n",
      "Epoch 23/1000\n",
      "9324/9324 [==============================] - 5s 500us/step - loss: 5.9289 - acc: 0.0092\n",
      "Epoch 24/1000\n",
      "9324/9324 [==============================] - 5s 506us/step - loss: 5.8964 - acc: 0.0077\n",
      "Epoch 25/1000\n",
      "9324/9324 [==============================] - 5s 506us/step - loss: 5.8585 - acc: 0.0094\n",
      "Epoch 26/1000\n",
      "9324/9324 [==============================] - 5s 512us/step - loss: 5.8221 - acc: 0.0103\n",
      "Epoch 27/1000\n",
      "9324/9324 [==============================] - 5s 511us/step - loss: 5.7864 - acc: 0.0125 4s - loss: 5.6 - ETA: 2s -\n",
      "Epoch 28/1000\n",
      "9324/9324 [==============================] - 5s 504us/step - loss: 5.7523 - acc: 0.0120 2s - lo\n",
      "Epoch 29/1000\n",
      "9324/9324 [==============================] - 5s 503us/step - loss: 5.7176 - acc: 0.0154\n",
      "Epoch 30/1000\n",
      "9324/9324 [==============================] - 5s 503us/step - loss: 5.6811 - acc: 0.0168\n",
      "Epoch 31/1000\n",
      "9324/9324 [==============================] - 5s 504us/step - loss: 5.6465 - acc: 0.0199\n",
      "Epoch 32/1000\n",
      "9324/9324 [==============================] - 5s 503us/step - loss: 5.6177 - acc: 0.0160\n",
      "Epoch 33/1000\n",
      "9324/9324 [==============================] - 5s 499us/step - loss: 5.5862 - acc: 0.0190\n",
      "Epoch 34/1000\n",
      "9324/9324 [==============================] - 5s 504us/step - loss: 5.5603 - acc: 0.0240\n",
      "Epoch 35/1000\n",
      "9324/9324 [==============================] - 5s 504us/step - loss: 5.5341 - acc: 0.0212\n",
      "Epoch 36/1000\n",
      "9324/9324 [==============================] - 5s 506us/step - loss: 5.5086 - acc: 0.0221\n",
      "Epoch 37/1000\n",
      "9324/9324 [==============================] - 5s 504us/step - loss: 5.4898 - acc: 0.0236\n",
      "Epoch 38/1000\n",
      "9324/9324 [==============================] - 5s 511us/step - loss: 5.4623 - acc: 0.0208\n",
      "Epoch 39/1000\n",
      "9324/9324 [==============================] - 5s 506us/step - loss: 5.4428 - acc: 0.0238\n",
      "Epoch 40/1000\n",
      "9324/9324 [==============================] - 5s 510us/step - loss: 5.4247 - acc: 0.0255 1s - loss: 5.3890 \n",
      "Epoch 41/1000\n",
      "9324/9324 [==============================] - 5s 537us/step - loss: 5.4052 - acc: 0.0266\n",
      "Epoch 42/1000\n",
      "9324/9324 [==============================] - 5s 498us/step - loss: 5.3872 - acc: 0.0278\n",
      "Epoch 43/1000\n",
      "9324/9324 [==============================] - 5s 574us/step - loss: 5.3780 - acc: 0.0279\n",
      "Epoch 44/1000\n",
      "9324/9324 [==============================] - 5s 580us/step - loss: 5.3522 - acc: 0.0300\n",
      "Epoch 45/1000\n",
      "9324/9324 [==============================] - 5s 566us/step - loss: 5.3384 - acc: 0.0265\n",
      "Epoch 46/1000\n",
      "9324/9324 [==============================] - 5s 566us/step - loss: 5.3286 - acc: 0.0299\n",
      "Epoch 47/1000\n",
      "9324/9324 [==============================] - 5s 579us/step - loss: 5.3078 - acc: 0.0323\n",
      "Epoch 48/1000\n",
      "9324/9324 [==============================] - 5s 531us/step - loss: 5.2884 - acc: 0.0347 1s - loss: 5.2775 - a\n",
      "Epoch 49/1000\n",
      "9324/9324 [==============================] - 5s 522us/step - loss: 5.2708 - acc: 0.0337\n",
      "Epoch 50/1000\n",
      "9324/9324 [==============================] - 5s 518us/step - loss: 5.2451 - acc: 0.0374\n",
      "Epoch 51/1000\n",
      "9324/9324 [==============================] - 6s 615us/step - loss: 5.2383 - acc: 0.0343\n",
      "Epoch 52/1000\n",
      "9324/9324 [==============================] - 5s 530us/step - loss: 5.2155 - acc: 0.0369\n",
      "Epoch 53/1000\n",
      "9324/9324 [==============================] - 6s 591us/step - loss: 5.1984 - acc: 0.0389\n",
      "Epoch 54/1000\n",
      "9324/9324 [==============================] - 5s 570us/step - loss: 5.1805 - acc: 0.0363\n",
      "Epoch 55/1000\n",
      "9324/9324 [==============================] - 5s 571us/step - loss: 5.1605 - acc: 0.0408\n",
      "Epoch 56/1000\n",
      "9324/9324 [==============================] - 5s 569us/step - loss: 5.1461 - acc: 0.0400\n",
      "Epoch 57/1000\n",
      "9324/9324 [==============================] - 5s 569us/step - loss: 5.1388 - acc: 0.0424\n",
      "Epoch 58/1000\n",
      "9324/9324 [==============================] - 5s 568us/step - loss: 5.1191 - acc: 0.0433\n",
      "Epoch 59/1000\n",
      "9324/9324 [==============================] - 5s 568us/step - loss: 5.1120 - acc: 0.0438\n",
      "Epoch 60/1000\n",
      "9324/9324 [==============================] - 5s 573us/step - loss: 5.0899 - acc: 0.0470\n",
      "Epoch 61/1000\n",
      "9324/9324 [==============================] - 5s 564us/step - loss: 5.0867 - acc: 0.0475\n",
      "Epoch 62/1000\n",
      "9324/9324 [==============================] - 5s 566us/step - loss: 5.0678 - acc: 0.0459\n",
      "Epoch 63/1000\n",
      "9324/9324 [==============================] - 5s 569us/step - loss: 5.0520 - acc: 0.0483\n",
      "Epoch 64/1000\n",
      "9324/9324 [==============================] - 5s 571us/step - loss: 5.0441 - acc: 0.0501\n",
      "Epoch 65/1000\n",
      "9324/9324 [==============================] - 5s 571us/step - loss: 5.0284 - acc: 0.0494\n",
      "Epoch 66/1000\n",
      "9324/9324 [==============================] - 5s 565us/step - loss: 5.0212 - acc: 0.0487\n",
      "Epoch 67/1000\n",
      "9324/9324 [==============================] - 5s 569us/step - loss: 5.0094 - acc: 0.0513\n",
      "Epoch 68/1000\n",
      "9324/9324 [==============================] - 5s 571us/step - loss: 5.0053 - acc: 0.0511\n",
      "Epoch 69/1000\n",
      "9324/9324 [==============================] - 5s 568us/step - loss: 4.9954 - acc: 0.0512\n",
      "Epoch 70/1000\n",
      "9324/9324 [==============================] - 5s 566us/step - loss: 4.9789 - acc: 0.0533\n",
      "Epoch 71/1000\n",
      "9324/9324 [==============================] - 5s 570us/step - loss: 4.9715 - acc: 0.0552\n",
      "Epoch 72/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9324/9324 [==============================] - 5s 574us/step - loss: 4.9577 - acc: 0.0526\n",
      "Epoch 73/1000\n",
      "9324/9324 [==============================] - 5s 568us/step - loss: 4.9500 - acc: 0.0542\n",
      "Epoch 74/1000\n",
      "9324/9324 [==============================] - 5s 574us/step - loss: 4.9446 - acc: 0.0575\n",
      "Epoch 75/1000\n",
      "9324/9324 [==============================] - 5s 567us/step - loss: 4.9287 - acc: 0.0557\n",
      "Epoch 76/1000\n",
      "9324/9324 [==============================] - 5s 569us/step - loss: 4.9217 - acc: 0.0580\n",
      "Epoch 77/1000\n",
      "9324/9324 [==============================] - 5s 570us/step - loss: 4.9126 - acc: 0.0596\n",
      "Epoch 78/1000\n",
      "9324/9324 [==============================] - 5s 571us/step - loss: 4.9009 - acc: 0.0587\n",
      "Epoch 79/1000\n",
      "9324/9324 [==============================] - 5s 569us/step - loss: 4.8997 - acc: 0.0593\n",
      "Epoch 80/1000\n",
      "9324/9324 [==============================] - 5s 564us/step - loss: 4.8853 - acc: 0.0591\n",
      "Epoch 81/1000\n",
      "9324/9324 [==============================] - 5s 570us/step - loss: 4.8845 - acc: 0.0616\n",
      "Epoch 82/1000\n",
      "9324/9324 [==============================] - 5s 571us/step - loss: 4.8718 - acc: 0.0620\n",
      "Epoch 83/1000\n",
      "9324/9324 [==============================] - 5s 572us/step - loss: 4.8654 - acc: 0.0612\n",
      "Epoch 84/1000\n",
      "9324/9324 [==============================] - 5s 569us/step - loss: 4.8628 - acc: 0.0650\n",
      "Epoch 85/1000\n",
      "9324/9324 [==============================] - 5s 567us/step - loss: 4.8462 - acc: 0.0633\n",
      "Epoch 86/1000\n",
      "9324/9324 [==============================] - 5s 571us/step - loss: 4.8520 - acc: 0.0633\n",
      "Epoch 87/1000\n",
      "9324/9324 [==============================] - 5s 570us/step - loss: 4.8279 - acc: 0.0637\n",
      "Epoch 88/1000\n",
      "9324/9324 [==============================] - 5s 581us/step - loss: 4.8357 - acc: 0.0636\n",
      "Epoch 89/1000\n",
      "9324/9324 [==============================] - 5s 576us/step - loss: 4.8192 - acc: 0.0664\n",
      "Epoch 90/1000\n",
      "9324/9324 [==============================] - 5s 571us/step - loss: 4.8127 - acc: 0.0676\n",
      "Epoch 91/1000\n",
      "9324/9324 [==============================] - 5s 571us/step - loss: 4.8059 - acc: 0.0668\n",
      "Epoch 92/1000\n",
      "9324/9324 [==============================] - 5s 570us/step - loss: 4.8035 - acc: 0.0631\n",
      "Epoch 93/1000\n",
      "9324/9324 [==============================] - 5s 580us/step - loss: 4.7914 - acc: 0.0714\n",
      "Epoch 94/1000\n",
      "9324/9324 [==============================] - 5s 572us/step - loss: 4.7926 - acc: 0.0715\n",
      "Epoch 95/1000\n",
      "9324/9324 [==============================] - 5s 570us/step - loss: 4.7801 - acc: 0.0692\n",
      "Epoch 96/1000\n",
      "9324/9324 [==============================] - 5s 570us/step - loss: 4.7851 - acc: 0.0666\n",
      "Epoch 97/1000\n",
      "9324/9324 [==============================] - 5s 568us/step - loss: 4.7670 - acc: 0.0720\n",
      "Epoch 98/1000\n",
      "9324/9324 [==============================] - 5s 568us/step - loss: 4.7588 - acc: 0.0721\n",
      "Epoch 99/1000\n",
      "9324/9324 [==============================] - 5s 567us/step - loss: 4.7530 - acc: 0.0715\n",
      "Epoch 100/1000\n",
      "9324/9324 [==============================] - 5s 569us/step - loss: 4.7633 - acc: 0.0708\n",
      "Epoch 101/1000\n",
      "9324/9324 [==============================] - 5s 568us/step - loss: 4.7395 - acc: 0.0713\n",
      "Epoch 102/1000\n",
      "9324/9324 [==============================] - 5s 568us/step - loss: 4.7406 - acc: 0.0756\n",
      "Epoch 103/1000\n",
      "9324/9324 [==============================] - 5s 571us/step - loss: 4.7369 - acc: 0.0737\n",
      "Epoch 104/1000\n",
      "9324/9324 [==============================] - 5s 571us/step - loss: 4.7251 - acc: 0.0741\n",
      "Epoch 105/1000\n",
      "9324/9324 [==============================] - 5s 568us/step - loss: 4.7239 - acc: 0.0756\n",
      "Epoch 106/1000\n",
      "9324/9324 [==============================] - 5s 570us/step - loss: 4.7141 - acc: 0.0748\n",
      "Epoch 107/1000\n",
      "9324/9324 [==============================] - 5s 576us/step - loss: 4.7065 - acc: 0.0774\n",
      "Epoch 108/1000\n",
      "9324/9324 [==============================] - 5s 527us/step - loss: 4.7103 - acc: 0.0769\n",
      "Epoch 109/1000\n",
      "9324/9324 [==============================] - 5s 518us/step - loss: 4.6978 - acc: 0.0769\n",
      "Epoch 110/1000\n",
      "9324/9324 [==============================] - 5s 549us/step - loss: 4.6966 - acc: 0.0771\n",
      "Epoch 111/1000\n",
      "9324/9324 [==============================] - 6s 592us/step - loss: 4.6864 - acc: 0.0761\n",
      "Epoch 112/1000\n",
      "9324/9324 [==============================] - 5s 568us/step - loss: 4.6813 - acc: 0.0784\n",
      "Epoch 113/1000\n",
      "9324/9324 [==============================] - 5s 568us/step - loss: 4.6713 - acc: 0.0822\n",
      "Epoch 114/1000\n",
      "9324/9324 [==============================] - 5s 572us/step - loss: 4.6662 - acc: 0.0826\n",
      "Epoch 115/1000\n",
      "9324/9324 [==============================] - 5s 572us/step - loss: 4.6666 - acc: 0.0814\n",
      "Epoch 116/1000\n",
      "9324/9324 [==============================] - 5s 567us/step - loss: 4.6628 - acc: 0.0802\n",
      "Epoch 117/1000\n",
      "9324/9324 [==============================] - 5s 576us/step - loss: 4.6503 - acc: 0.0798\n",
      "Epoch 118/1000\n",
      "9324/9324 [==============================] - 5s 571us/step - loss: 4.6365 - acc: 0.0816\n",
      "Epoch 119/1000\n",
      "9324/9324 [==============================] - 5s 571us/step - loss: 4.6385 - acc: 0.0825\n",
      "Epoch 120/1000\n",
      "9324/9324 [==============================] - 5s 572us/step - loss: 4.6425 - acc: 0.0830\n",
      "Epoch 121/1000\n",
      "9324/9324 [==============================] - 5s 572us/step - loss: 4.6245 - acc: 0.0850\n",
      "Epoch 122/1000\n",
      "9324/9324 [==============================] - 5s 569us/step - loss: 4.6275 - acc: 0.0801\n",
      "Epoch 123/1000\n",
      "9324/9324 [==============================] - 5s 566us/step - loss: 4.6268 - acc: 0.0863\n",
      "Epoch 124/1000\n",
      "9324/9324 [==============================] - 5s 570us/step - loss: 4.6104 - acc: 0.0868\n",
      "Epoch 125/1000\n",
      "9324/9324 [==============================] - 5s 573us/step - loss: 4.6158 - acc: 0.0858\n",
      "Epoch 126/1000\n",
      "9324/9324 [==============================] - 5s 565us/step - loss: 4.6145 - acc: 0.0826\n",
      "Epoch 127/1000\n",
      "9324/9324 [==============================] - 5s 568us/step - loss: 4.6047 - acc: 0.0891\n",
      "Epoch 128/1000\n",
      "9324/9324 [==============================] - 5s 570us/step - loss: 4.5940 - acc: 0.0856\n",
      "Epoch 129/1000\n",
      "9324/9324 [==============================] - 5s 566us/step - loss: 4.5925 - acc: 0.0897\n",
      "Epoch 130/1000\n",
      "9324/9324 [==============================] - 5s 570us/step - loss: 4.5845 - acc: 0.0870\n",
      "Epoch 131/1000\n",
      "9324/9324 [==============================] - 5s 569us/step - loss: 4.5853 - acc: 0.0914\n",
      "Epoch 132/1000\n",
      "9324/9324 [==============================] - 5s 571us/step - loss: 4.5619 - acc: 0.0928\n",
      "Epoch 133/1000\n",
      "9324/9324 [==============================] - 5s 569us/step - loss: 4.5569 - acc: 0.0920\n",
      "Epoch 134/1000\n",
      "9324/9324 [==============================] - 5s 570us/step - loss: 4.5503 - acc: 0.0918\n",
      "Epoch 135/1000\n",
      "9324/9324 [==============================] - 5s 583us/step - loss: 4.5429 - acc: 0.0926\n",
      "Epoch 136/1000\n",
      "9324/9324 [==============================] - 5s 583us/step - loss: 4.5347 - acc: 0.0966\n",
      "Epoch 137/1000\n",
      "9324/9324 [==============================] - 5s 574us/step - loss: 4.5201 - acc: 0.0950\n",
      "Epoch 138/1000\n",
      "9324/9324 [==============================] - 5s 576us/step - loss: 4.5006 - acc: 0.1002\n",
      "Epoch 139/1000\n",
      "9324/9324 [==============================] - 5s 573us/step - loss: 4.4987 - acc: 0.0948\n",
      "Epoch 140/1000\n",
      "9324/9324 [==============================] - 5s 573us/step - loss: 4.4930 - acc: 0.1041\n",
      "Epoch 141/1000\n",
      "9324/9324 [==============================] - 5s 569us/step - loss: 4.4721 - acc: 0.1057\n",
      "Epoch 142/1000\n",
      "9324/9324 [==============================] - 5s 567us/step - loss: 4.4733 - acc: 0.1025\n",
      "Epoch 143/1000\n",
      "9324/9324 [==============================] - 5s 570us/step - loss: 4.4519 - acc: 0.1027\n",
      "Epoch 144/1000\n",
      "9324/9324 [==============================] - 5s 571us/step - loss: 4.4434 - acc: 0.1109\n",
      "Epoch 145/1000\n",
      "9324/9324 [==============================] - 5s 568us/step - loss: 4.4395 - acc: 0.1047\n",
      "Epoch 146/1000\n",
      "9324/9324 [==============================] - 5s 568us/step - loss: 4.4293 - acc: 0.1037\n",
      "Epoch 147/1000\n",
      "9324/9324 [==============================] - 5s 572us/step - loss: 4.4248 - acc: 0.1108\n",
      "Epoch 148/1000\n",
      "9324/9324 [==============================] - 5s 567us/step - loss: 4.4031 - acc: 0.1106\n",
      "Epoch 149/1000\n",
      "9324/9324 [==============================] - 5s 573us/step - loss: 4.4098 - acc: 0.1092\n",
      "Epoch 150/1000\n",
      "9324/9324 [==============================] - 5s 566us/step - loss: 4.3976 - acc: 0.1113\n",
      "Epoch 151/1000\n",
      "9324/9324 [==============================] - 5s 569us/step - loss: 4.3814 - acc: 0.1149\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 152/1000\n",
      "9324/9324 [==============================] - 5s 569us/step - loss: 4.3622 - acc: 0.1170\n",
      "Epoch 153/1000\n",
      "9324/9324 [==============================] - 5s 569us/step - loss: 4.3722 - acc: 0.1143\n",
      "Epoch 154/1000\n",
      "9324/9324 [==============================] - 5s 569us/step - loss: 4.3704 - acc: 0.1164\n",
      "Epoch 155/1000\n",
      "9324/9324 [==============================] - 5s 568us/step - loss: 4.3570 - acc: 0.1201\n",
      "Epoch 156/1000\n",
      "9324/9324 [==============================] - 5s 566us/step - loss: 4.3431 - acc: 0.1224\n",
      "Epoch 157/1000\n",
      "9324/9324 [==============================] - 5s 568us/step - loss: 4.3312 - acc: 0.1213\n",
      "Epoch 158/1000\n",
      "9324/9324 [==============================] - 5s 571us/step - loss: 4.3331 - acc: 0.1207\n",
      "Epoch 159/1000\n",
      "9324/9324 [==============================] - 5s 571us/step - loss: 4.3333 - acc: 0.1210\n",
      "Epoch 160/1000\n",
      "9324/9324 [==============================] - 5s 571us/step - loss: 4.3188 - acc: 0.1195\n",
      "Epoch 161/1000\n",
      "9324/9324 [==============================] - 5s 571us/step - loss: 4.3097 - acc: 0.1226\n",
      "Epoch 162/1000\n",
      "9324/9324 [==============================] - 5s 569us/step - loss: 4.3019 - acc: 0.1223\n",
      "Epoch 163/1000\n",
      "9324/9324 [==============================] - 5s 569us/step - loss: 4.3022 - acc: 0.1239\n",
      "Epoch 164/1000\n",
      "9324/9324 [==============================] - 5s 571us/step - loss: 4.2920 - acc: 0.1273\n",
      "Epoch 165/1000\n",
      "9324/9324 [==============================] - 5s 573us/step - loss: 4.2844 - acc: 0.1337\n",
      "Epoch 166/1000\n",
      "9324/9324 [==============================] - 5s 583us/step - loss: 4.2706 - acc: 0.1247\n",
      "Epoch 167/1000\n",
      "9324/9324 [==============================] - 5s 573us/step - loss: 4.2827 - acc: 0.1242\n",
      "Epoch 168/1000\n",
      "9324/9324 [==============================] - 5s 567us/step - loss: 4.2752 - acc: 0.1290\n",
      "Epoch 169/1000\n",
      "9324/9324 [==============================] - 5s 570us/step - loss: 4.2518 - acc: 0.1284\n",
      "Epoch 170/1000\n",
      "9324/9324 [==============================] - 6s 615us/step - loss: 4.2493 - acc: 0.1270\n",
      "Epoch 171/1000\n",
      "9324/9324 [==============================] - 5s 566us/step - loss: 4.2481 - acc: 0.1279\n",
      "Epoch 172/1000\n",
      "9324/9324 [==============================] - 5s 573us/step - loss: 4.2361 - acc: 0.1340\n",
      "Epoch 173/1000\n",
      "9324/9324 [==============================] - 5s 568us/step - loss: 4.2412 - acc: 0.1321\n",
      "Epoch 174/1000\n",
      "9324/9324 [==============================] - 5s 573us/step - loss: 4.2349 - acc: 0.1347\n",
      "Epoch 175/1000\n",
      "9324/9324 [==============================] - 5s 570us/step - loss: 4.2327 - acc: 0.1311\n",
      "Epoch 176/1000\n",
      "9324/9324 [==============================] - 5s 569us/step - loss: 4.2335 - acc: 0.1371\n",
      "Epoch 177/1000\n",
      "9324/9324 [==============================] - 5s 569us/step - loss: 4.2146 - acc: 0.1340\n",
      "Epoch 178/1000\n",
      "9324/9324 [==============================] - 5s 574us/step - loss: 4.2187 - acc: 0.1357\n",
      "Epoch 179/1000\n",
      "9324/9324 [==============================] - 5s 569us/step - loss: 4.2071 - acc: 0.1327\n",
      "Epoch 180/1000\n",
      "9324/9324 [==============================] - 5s 567us/step - loss: 4.2013 - acc: 0.1373\n",
      "Epoch 181/1000\n",
      "9324/9324 [==============================] - 5s 567us/step - loss: 4.1979 - acc: 0.1362\n",
      "Epoch 182/1000\n",
      "9324/9324 [==============================] - 5s 556us/step - loss: 4.1957 - acc: 0.1399\n",
      "Epoch 183/1000\n",
      "9324/9324 [==============================] - 5s 519us/step - loss: 4.1733 - acc: 0.1419\n",
      "Epoch 184/1000\n",
      "9324/9324 [==============================] - 5s 527us/step - loss: 4.1785 - acc: 0.1430\n",
      "Epoch 185/1000\n",
      "9324/9324 [==============================] - 5s 539us/step - loss: 4.1689 - acc: 0.1409\n",
      "Epoch 186/1000\n",
      "9324/9324 [==============================] - 5s 543us/step - loss: 4.1805 - acc: 0.1361 0s - loss: 4.1701 - acc: 0\n",
      "Epoch 187/1000\n",
      "9324/9324 [==============================] - 5s 541us/step - loss: 4.1600 - acc: 0.1452\n",
      "Epoch 188/1000\n",
      "9324/9324 [==============================] - 5s 563us/step - loss: 4.1434 - acc: 0.1455\n",
      "Epoch 189/1000\n",
      "9324/9324 [==============================] - 5s 580us/step - loss: 4.1593 - acc: 0.1462\n",
      "Epoch 190/1000\n",
      "9324/9324 [==============================] - 5s 575us/step - loss: 4.1496 - acc: 0.1433\n",
      "Epoch 191/1000\n",
      "9324/9324 [==============================] - 5s 571us/step - loss: 4.1368 - acc: 0.1424\n",
      "Epoch 192/1000\n",
      "9324/9324 [==============================] - 5s 570us/step - loss: 4.1507 - acc: 0.1450\n",
      "Epoch 193/1000\n",
      "9324/9324 [==============================] - 5s 572us/step - loss: 4.1446 - acc: 0.1466\n",
      "Epoch 194/1000\n",
      "9324/9324 [==============================] - 5s 574us/step - loss: 4.1500 - acc: 0.1458\n",
      "Epoch 195/1000\n",
      "9324/9324 [==============================] - 5s 576us/step - loss: 4.1229 - acc: 0.1522\n",
      "Epoch 196/1000\n",
      "9324/9324 [==============================] - 5s 573us/step - loss: 4.1282 - acc: 0.1456\n",
      "Epoch 197/1000\n",
      "9324/9324 [==============================] - 5s 574us/step - loss: 4.1197 - acc: 0.1454\n",
      "Epoch 198/1000\n",
      "9324/9324 [==============================] - 5s 577us/step - loss: 4.1295 - acc: 0.1490\n",
      "Epoch 199/1000\n",
      "9324/9324 [==============================] - 5s 577us/step - loss: 4.1133 - acc: 0.1508\n",
      "Epoch 200/1000\n",
      "9324/9324 [==============================] - 5s 575us/step - loss: 4.1220 - acc: 0.1522\n",
      "Epoch 201/1000\n",
      "9324/9324 [==============================] - 5s 575us/step - loss: 4.1028 - acc: 0.1522\n",
      "Epoch 202/1000\n",
      "9324/9324 [==============================] - 5s 572us/step - loss: 4.0976 - acc: 0.1565\n",
      "Epoch 203/1000\n",
      "9324/9324 [==============================] - 5s 574us/step - loss: 4.0974 - acc: 0.1586\n",
      "Epoch 204/1000\n",
      "9324/9324 [==============================] - 5s 577us/step - loss: 4.0898 - acc: 0.1527\n",
      "Epoch 205/1000\n",
      "9324/9324 [==============================] - 5s 571us/step - loss: 4.0870 - acc: 0.1568\n",
      "Epoch 206/1000\n",
      "9324/9324 [==============================] - 5s 576us/step - loss: 4.0827 - acc: 0.1539\n",
      "Epoch 207/1000\n",
      "9324/9324 [==============================] - 5s 575us/step - loss: 4.0792 - acc: 0.1541\n",
      "Epoch 208/1000\n",
      "9324/9324 [==============================] - 5s 573us/step - loss: 4.0704 - acc: 0.1589\n",
      "Epoch 209/1000\n",
      "9324/9324 [==============================] - 5s 568us/step - loss: 4.0786 - acc: 0.1530\n",
      "Epoch 210/1000\n",
      "9324/9324 [==============================] - 5s 574us/step - loss: 4.0647 - acc: 0.1560\n",
      "Epoch 211/1000\n",
      "9324/9324 [==============================] - 5s 576us/step - loss: 4.0710 - acc: 0.1554\n",
      "Epoch 212/1000\n",
      "9324/9324 [==============================] - 5s 571us/step - loss: 4.0621 - acc: 0.1596\n",
      "Epoch 213/1000\n",
      "9324/9324 [==============================] - 5s 577us/step - loss: 4.0752 - acc: 0.1541\n",
      "Epoch 214/1000\n",
      "9324/9324 [==============================] - 5s 572us/step - loss: 4.0462 - acc: 0.1625\n",
      "Epoch 215/1000\n",
      "9324/9324 [==============================] - 5s 577us/step - loss: 4.0528 - acc: 0.1593\n",
      "Epoch 216/1000\n",
      "9324/9324 [==============================] - 5s 573us/step - loss: 4.0392 - acc: 0.1591\n",
      "Epoch 217/1000\n",
      "9324/9324 [==============================] - 5s 576us/step - loss: 4.0299 - acc: 0.1592\n",
      "Epoch 218/1000\n",
      "9324/9324 [==============================] - 5s 586us/step - loss: 4.0533 - acc: 0.1549\n",
      "Epoch 219/1000\n",
      "9324/9324 [==============================] - 5s 583us/step - loss: 4.0257 - acc: 0.1653\n",
      "Epoch 220/1000\n",
      "9324/9324 [==============================] - 5s 569us/step - loss: 4.0365 - acc: 0.1592\n",
      "Epoch 221/1000\n",
      "9324/9324 [==============================] - 5s 573us/step - loss: 4.0155 - acc: 0.1656\n",
      "Epoch 222/1000\n",
      "9324/9324 [==============================] - 5s 573us/step - loss: 4.0265 - acc: 0.1630\n",
      "Epoch 223/1000\n",
      "9324/9324 [==============================] - 5s 575us/step - loss: 4.0338 - acc: 0.1621\n",
      "Epoch 224/1000\n",
      "9324/9324 [==============================] - 5s 572us/step - loss: 4.0039 - acc: 0.1641\n",
      "Epoch 225/1000\n",
      "9324/9324 [==============================] - 5s 570us/step - loss: 4.0075 - acc: 0.1612\n",
      "Epoch 226/1000\n",
      "9324/9324 [==============================] - 5s 574us/step - loss: 4.0163 - acc: 0.1601\n",
      "Epoch 227/1000\n",
      "9324/9324 [==============================] - 5s 573us/step - loss: 3.9997 - acc: 0.1667\n",
      "Epoch 228/1000\n",
      "9324/9324 [==============================] - 5s 575us/step - loss: 4.0027 - acc: 0.1690\n",
      "Epoch 229/1000\n",
      "9324/9324 [==============================] - 5s 560us/step - loss: 3.9998 - acc: 0.1659\n",
      "Epoch 230/1000\n",
      "9324/9324 [==============================] - 5s 587us/step - loss: 3.9915 - acc: 0.1637\n",
      "Epoch 231/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9324/9324 [==============================] - 5s 548us/step - loss: 3.9834 - acc: 0.1717\n",
      "Epoch 232/1000\n",
      "9324/9324 [==============================] - 5s 531us/step - loss: 3.9996 - acc: 0.1610\n",
      "Epoch 233/1000\n",
      "9324/9324 [==============================] - 5s 530us/step - loss: 3.9875 - acc: 0.1654\n",
      "Epoch 234/1000\n",
      "9324/9324 [==============================] - 5s 550us/step - loss: 3.9777 - acc: 0.1658\n",
      "Epoch 235/1000\n",
      "9324/9324 [==============================] - 5s 578us/step - loss: 3.9766 - acc: 0.1697\n",
      "Epoch 236/1000\n",
      "9324/9324 [==============================] - 5s 585us/step - loss: 3.9753 - acc: 0.1691\n",
      "Epoch 237/1000\n",
      "9324/9324 [==============================] - 5s 547us/step - loss: 3.9730 - acc: 0.1637\n",
      "Epoch 238/1000\n",
      "9324/9324 [==============================] - 5s 539us/step - loss: 3.9672 - acc: 0.1671\n",
      "Epoch 239/1000\n",
      "9324/9324 [==============================] - 5s 571us/step - loss: 3.9769 - acc: 0.1668\n",
      "Epoch 240/1000\n",
      "9324/9324 [==============================] - 4s 474us/step - loss: 3.9689 - acc: 0.1684\n",
      "Epoch 241/1000\n",
      "9324/9324 [==============================] - 4s 472us/step - loss: 3.9639 - acc: 0.1683\n",
      "Epoch 242/1000\n",
      "9324/9324 [==============================] - 4s 472us/step - loss: 3.9666 - acc: 0.1674\n",
      "Epoch 243/1000\n",
      "9324/9324 [==============================] - 4s 471us/step - loss: 3.9651 - acc: 0.1734\n",
      "Epoch 244/1000\n",
      "9324/9324 [==============================] - 5s 488us/step - loss: 3.9463 - acc: 0.1719\n",
      "Epoch 245/1000\n",
      "9324/9324 [==============================] - 5s 529us/step - loss: 3.9423 - acc: 0.1712\n",
      "Epoch 246/1000\n",
      "9324/9324 [==============================] - 5s 520us/step - loss: 3.9440 - acc: 0.1761\n",
      "Epoch 247/1000\n",
      "9324/9324 [==============================] - 4s 471us/step - loss: 3.9340 - acc: 0.1737\n",
      "Epoch 248/1000\n",
      "9324/9324 [==============================] - 4s 470us/step - loss: 3.9355 - acc: 0.1734\n",
      "Epoch 249/1000\n",
      "9324/9324 [==============================] - 5s 484us/step - loss: 3.9508 - acc: 0.1746\n",
      "Epoch 250/1000\n",
      "9324/9324 [==============================] - 5s 520us/step - loss: 3.9393 - acc: 0.1747\n",
      "Epoch 251/1000\n",
      "9324/9324 [==============================] - 5s 508us/step - loss: 3.9353 - acc: 0.1767\n",
      "Epoch 252/1000\n",
      "9324/9324 [==============================] - 5s 530us/step - loss: 3.9282 - acc: 0.1729\n",
      "Epoch 253/1000\n",
      "9324/9324 [==============================] - 5s 535us/step - loss: 3.9124 - acc: 0.1792\n",
      "Epoch 254/1000\n",
      "9324/9324 [==============================] - 5s 501us/step - loss: 3.9315 - acc: 0.1755\n",
      "Epoch 255/1000\n",
      "9324/9324 [==============================] - 5s 531us/step - loss: 3.9132 - acc: 0.1769\n",
      "Epoch 256/1000\n",
      "9324/9324 [==============================] - 5s 521us/step - loss: 3.9128 - acc: 0.1771\n",
      "Epoch 257/1000\n",
      "6200/9324 [==================>...........] - ETA: 1s - loss: 3.8904 - acc: 0.1789"
     ]
    }
   ],
   "source": [
    "model2(X_train_new, y_train, X_test_new, y_test,200,1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model3(X_train_new, y_train, X_test_new, y_test,200,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_5 (LSTM)                (None, 1000, 75)          30600     \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 75)                45300     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10219)             776644    \n",
      "=================================================================\n",
      "Total params: 852,544\n",
      "Trainable params: 852,544\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Accuracy: 0.000000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model4(X_train_new, y_train, X_test_new, y_test,200,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 1000)        26000     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, None, 75)          322800    \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 75)                45300     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10219)             776644    \n",
      "=================================================================\n",
      "Total params: 1,170,744\n",
      "Trainable params: 1,170,744\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "3505/3505 [==============================] - 155s 44ms/step - loss: 8.3424 - acc: 5.7061e-04\n",
      "Epoch 2/50\n",
      "3505/3505 [==============================] - 153s 44ms/step - loss: 6.9927 - acc: 0.0011\n",
      "Epoch 3/50\n",
      "3505/3505 [==============================] - 175s 50ms/step - loss: 6.6652 - acc: 2.8531e-04\n",
      "Epoch 4/50\n",
      "3505/3505 [==============================] - 187s 53ms/step - loss: 6.5434 - acc: 2.8531e-04\n",
      "Epoch 5/50\n",
      "3505/3505 [==============================] - 142s 41ms/step - loss: 6.4900 - acc: 2.8531e-04\n",
      "Epoch 6/50\n",
      "3505/3505 [==============================] - 144s 41ms/step - loss: 6.4672 - acc: 8.5592e-04\n",
      "Epoch 7/50\n",
      "3505/3505 [==============================] - 143s 41ms/step - loss: 6.4558 - acc: 0.0011\n",
      "Epoch 8/50\n",
      "3505/3505 [==============================] - 141s 40ms/step - loss: 6.4492 - acc: 0.0014\n",
      "Epoch 9/50\n",
      "3505/3505 [==============================] - 142s 40ms/step - loss: 6.4447 - acc: 2.8531e-04\n",
      "Epoch 10/50\n",
      "3505/3505 [==============================] - 147s 42ms/step - loss: 6.4389 - acc: 0.0011\n",
      "Epoch 11/50\n",
      "3505/3505 [==============================] - 154s 44ms/step - loss: 6.4286 - acc: 0.0011\n",
      "Epoch 12/50\n",
      "3505/3505 [==============================] - 143s 41ms/step - loss: 6.4098 - acc: 0.0026\n",
      "Epoch 13/50\n",
      "3505/3505 [==============================] - 142s 40ms/step - loss: 6.3966 - acc: 0.0026\n",
      "Epoch 14/50\n",
      "3505/3505 [==============================] - 165s 47ms/step - loss: 6.3657 - acc: 0.0017\n",
      "Epoch 15/50\n",
      "3505/3505 [==============================] - 143s 41ms/step - loss: 6.3249 - acc: 0.0026\n",
      "Epoch 16/50\n",
      "3505/3505 [==============================] - 143s 41ms/step - loss: 6.2723 - acc: 0.0054\n",
      "Epoch 17/50\n",
      "3505/3505 [==============================] - 147s 42ms/step - loss: 6.1993 - acc: 0.0026\n",
      "Epoch 18/50\n",
      "3505/3505 [==============================] - 144s 41ms/step - loss: 6.1603 - acc: 0.0049\n",
      "Epoch 19/50\n",
      "3505/3505 [==============================] - 143s 41ms/step - loss: 6.0801 - acc: 0.0054\n",
      "Epoch 20/50\n",
      "3505/3505 [==============================] - 142s 40ms/step - loss: 5.9936 - acc: 0.0071\n",
      "Epoch 21/50\n",
      "3505/3505 [==============================] - 142s 41ms/step - loss: 5.9067 - acc: 0.0043\n",
      "Epoch 22/50\n",
      "3505/3505 [==============================] - 143s 41ms/step - loss: 5.8337 - acc: 0.0077\n",
      "Epoch 23/50\n",
      "3505/3505 [==============================] - 142s 40ms/step - loss: 5.7624 - acc: 0.0057\n",
      "Epoch 24/50\n",
      "3505/3505 [==============================] - 143s 41ms/step - loss: 5.7009 - acc: 0.0083\n",
      "Epoch 25/50\n",
      "3505/3505 [==============================] - 142s 40ms/step - loss: 5.6604 - acc: 0.0103\n",
      "Epoch 26/50\n",
      "3505/3505 [==============================] - 142s 40ms/step - loss: 5.5971 - acc: 0.0091\n",
      "Epoch 27/50\n",
      "3505/3505 [==============================] - 145s 41ms/step - loss: 5.5743 - acc: 0.0091\n",
      "Epoch 28/50\n",
      "3505/3505 [==============================] - 143s 41ms/step - loss: 5.5072 - acc: 0.0086\n",
      "Epoch 29/50\n",
      "3505/3505 [==============================] - 142s 41ms/step - loss: 5.4703 - acc: 0.0131\n",
      "Epoch 30/50\n",
      "3505/3505 [==============================] - 142s 41ms/step - loss: 5.4204 - acc: 0.0097\n",
      "Epoch 31/50\n",
      "3505/3505 [==============================] - 142s 41ms/step - loss: 5.4049 - acc: 0.0106\n",
      "Epoch 32/50\n",
      "3505/3505 [==============================] - 142s 40ms/step - loss: 5.3772 - acc: 0.0106\n",
      "Epoch 33/50\n",
      "3505/3505 [==============================] - 142s 40ms/step - loss: 5.3483 - acc: 0.0106\n",
      "Epoch 34/50\n",
      "3505/3505 [==============================] - 142s 41ms/step - loss: 5.3156 - acc: 0.0114\n",
      "Epoch 35/50\n",
      "3505/3505 [==============================] - 142s 40ms/step - loss: 5.2756 - acc: 0.0103\n",
      "Epoch 36/50\n",
      "3505/3505 [==============================] - 142s 40ms/step - loss: 5.2514 - acc: 0.0120\n",
      "Epoch 37/50\n",
      "3505/3505 [==============================] - 142s 40ms/step - loss: 5.2022 - acc: 0.0120\n",
      "Epoch 38/50\n",
      "3505/3505 [==============================] - 141s 40ms/step - loss: 5.1906 - acc: 0.0120\n",
      "Epoch 39/50\n",
      "3505/3505 [==============================] - 143s 41ms/step - loss: 5.1724 - acc: 0.0140\n",
      "Epoch 40/50\n",
      "3505/3505 [==============================] - 142s 41ms/step - loss: 5.1539 - acc: 0.0146\n",
      "Epoch 41/50\n",
      "3505/3505 [==============================] - 142s 41ms/step - loss: 5.1218 - acc: 0.0191\n",
      "Epoch 42/50\n",
      "3505/3505 [==============================] - 143s 41ms/step - loss: 5.1079 - acc: 0.0171\n",
      "Epoch 43/50\n",
      "3505/3505 [==============================] - 142s 41ms/step - loss: 5.0821 - acc: 0.0165\n",
      "Epoch 44/50\n",
      "3505/3505 [==============================] - 143s 41ms/step - loss: 5.0445 - acc: 0.0151\n",
      "Epoch 45/50\n",
      "3505/3505 [==============================] - 142s 41ms/step - loss: 5.0245 - acc: 0.0157\n",
      "Epoch 46/50\n",
      "3505/3505 [==============================] - 142s 41ms/step - loss: 4.9978 - acc: 0.0177\n",
      "Epoch 47/50\n",
      "3505/3505 [==============================] - 143s 41ms/step - loss: 4.9739 - acc: 0.0200\n",
      "Epoch 48/50\n",
      "3505/3505 [==============================] - 142s 41ms/step - loss: 4.9641 - acc: 0.0177\n",
      "Epoch 49/50\n",
      "3505/3505 [==============================] - 142s 40ms/step - loss: 4.9351 - acc: 0.0180\n",
      "Epoch 50/50\n",
      "3505/3505 [==============================] - 143s 41ms/step - loss: 4.9267 - acc: 0.0211\n",
      "Accuracy: 0.456100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model5(X_train_new, y_train, X_test_new, y_test,200,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model6(X_train_new, y_train, X_test_new, y_test,200,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
