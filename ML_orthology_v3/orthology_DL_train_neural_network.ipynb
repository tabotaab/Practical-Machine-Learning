{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import GRU\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers import GlobalAveragePooling1D\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import to_categorical\n",
    "from keras import optimizers\n",
    "\n",
    "from Bio import SeqIO\n",
    "from Bio.Data import IUPACData \n",
    "import csv\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import dask.dataframe as dd\n",
    "import os.path\n",
    "from matplotlib import pyplot\n",
    "\n",
    "#data_path = 'features_CENH3_DMR6_LUCA-CHLRE00002_orthologues.csv'\n",
    "data_path = 'features_oma-seqs-viridiplantae_test-3-4-5-6-7-8-9-10-11.csv'\n",
    "data_single_cluster_path = 'features_oma-seqs-viridiplantae_test-1.csv'\n",
    "#data_path = 'features_oma-seqs-viridiplantae_test-9-10-11.csv'\n",
    "#data_path = 'features_oma-seqs-viridiplantae_test-11.csv'\n",
    "#data_single_cluster_path = 'no_single_cluster.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def protein2integer(in_seq):\n",
    "    \n",
    "    ## define universe of possible input values\n",
    "    all_protein_letters = list(IUPACData.extended_protein_letters)\n",
    "    #print(all_protein_letters)\n",
    "    ## define a mapping of chars to integers \n",
    "    ## i+1 beacuse we want to start from integer 1 instead of 0. 0 will be used for padding\n",
    "    char_to_int = dict((c, i+1) for i, c in enumerate(all_protein_letters))\n",
    "    int_to_char = dict((i+1, c) for i, c in enumerate(all_protein_letters))\n",
    "    ## integer encode input data\n",
    "    integer_encoded = [char_to_int[char] for char in in_seq.upper()]\n",
    "    \n",
    "    \n",
    "    #return(integer_encoded,len(all_protein_letters))\n",
    "    return(integer_encoded)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(in_file):\n",
    "    with open(in_file, 'r') as f:\n",
    "        reader = csv.reader(f, delimiter=\"\\t\")\n",
    "        # get all the rows as a list\n",
    "        d_set = list(reader)\n",
    "        # transform data into numpy array\n",
    "        d_set = np.array(d_set).astype(str)\n",
    "        \n",
    "    integer_encoded_proteins = np.array([protein2integer(seq) for seq in d_set[:,1]])\n",
    "    \n",
    "    G = d_set[:, 0]\n",
    "    X = integer_encoded_proteins\n",
    "    Y = d_set[:, 2].astype(int)\n",
    "                         \n",
    "    return(d_set,G,X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset_single(in_file,y_value=100000):\n",
    "    with open(in_file, 'r') as f:\n",
    "        reader = csv.reader(f, delimiter=\"\\t\")\n",
    "        # get all the rows as a list\n",
    "        d_set = list(reader)\n",
    "        # transform data into numpy array\n",
    "        d_set = np.array(d_set).astype(str)\n",
    "        \n",
    "    integer_encoded_proteins = np.array([protein2integer(seq) for seq in d_set[:,1]])\n",
    "    \n",
    "    G = d_set[:, 0]\n",
    "    X = integer_encoded_proteins\n",
    "    Y = np.full((len(d_set[:, 2]),1), int(y_value))[:,0]      \n",
    "                         \n",
    "    return(d_set,G,X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset_dask(in_file):\n",
    "    data = dd.read_csv(in_file,sep='\\t', header=None)\n",
    "    df = data.compute().reset_index(drop=True)\n",
    "    integer_encoded_proteins = da.from_array([protein2integer(seq) for seq in df.values[:,1]],chunks=1000)\n",
    "    G = df.values[:,0]\n",
    "    X = integer_encoded_proteins.compute()\n",
    "    Y = df.values[:,2].astype(int)\n",
    "                     \n",
    "    return(df,G,X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_test_set_idea1(G,X,Y):\n",
    "    \n",
    "    # here we keep 80% of random indexes in train set and the rest in test set\n",
    "    \n",
    "    indices = np.random.permutation(X.shape[0])\n",
    "    train_size = int(indices.size*0.95)\n",
    "    train_idx, test_idx = indices[:train_size], indices[train_size:]\n",
    "    #print(len(train_idx),len(test_idx))\n",
    "    \n",
    "    X_train, X_test = X[train_idx,], X[test_idx,]\n",
    "    #print(X_train.shape,X_test.shape)\n",
    "    \n",
    "    y_train, y_test = Y[train_idx,], Y[test_idx,]\n",
    "    \n",
    "    #print(X[train_idx[0],])\n",
    "    #print(Y[train_idx[0],])\n",
    "\n",
    "    #print(X_train[0,])\n",
    "    #print(y_train[0,])\n",
    "    \n",
    "    return(X_train,y_train,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_test_set_idea2(G,X,Y):\n",
    "    \n",
    "    # here we try to keep one item from each cluster in test set\n",
    "    # the rest goes to train set\n",
    "    \n",
    "    test_idx = []\n",
    "    train_idx = []\n",
    "    seen_cluster_id = []\n",
    "    \n",
    "    for i in range(0,Y.shape[0]):\n",
    "            if Y[i] in seen_cluster_id :\n",
    "                train_idx.append(i)\n",
    "            else:\n",
    "                test_idx.append(i)\n",
    "                seen_cluster_id.append(Y[i])\n",
    "                \n",
    "    #print(len(train_idx),len(test_idx))\n",
    "    \n",
    "    X_train, X_test = X[train_idx,], X[test_idx,]\n",
    "    #print(X_train.shape,X_test.shape)\n",
    "    \n",
    "    y_train, y_test = Y[train_idx,], Y[test_idx,]\n",
    "    \n",
    "    #print(X[train_idx[0],])\n",
    "    #print(Y[train_idx[0],])\n",
    "\n",
    "    #print(X_train[0,])\n",
    "    #print(y_train[0,])\n",
    "    \n",
    "    return(X_train,y_train,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model1(X_train_new, y_train,X_test_new, y_test,in_batch_size=100,in_epochs=10,model_json_file=\"model.json\",model_h5_file=\"model.h5\"): # RNN: Recurrent Neural Networks\n",
    "    # https://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/\n",
    "\n",
    "    # Convert labels to categorical one-hot encoding  \n",
    "    y_train_one_hot_labels = to_categorical(y_train, num_classes=n_classes)\n",
    "    y_test_one_hot_labels = to_categorical(y_test, num_classes=n_classes)\n",
    "\n",
    "    # create the model\n",
    "    embedding_vecor_length = 4\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(num_letters, embedding_vecor_length, input_length=fixed_seq_length))\n",
    "    model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(LSTM(75))\n",
    "    model.add(Dense(n_classes, activation='softmax'))\n",
    "\n",
    "    #opt = 'rmsprop'\n",
    "    #opt = optimizers.SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False)\n",
    "    #opt = optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=0.001, decay=0.0)\n",
    "    #opt = optimizers.Adagrad(lr=0.01, epsilon=None, decay=0.0)\n",
    "    #opt = optimizers.Adadelta(lr=1.0, rho=0.95, epsilon=None, decay=0.0)\n",
    "    opt = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "    #opt = optimizers.Adamax(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0)\n",
    "    #opt = optimizers.Nadam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, schedule_decay=0.004)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',optimizer=opt, metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "\n",
    "    # fit the model\n",
    "    history = model.fit(X_train_new, y_train_one_hot_labels, epochs=in_epochs, batch_size=in_batch_size,validation_data=(X_test_new, y_test_one_hot_labels), verbose=1)\n",
    "\n",
    "    # evaluate the model\n",
    "    train_loss, train_acc = model.evaluate(X_train_new, y_train_one_hot_labels, verbose=0)\n",
    "    test_loss, test_acc = model.evaluate(X_test_new, y_test_one_hot_labels, verbose=0)\n",
    "    print('Loss     ======= Train: %.3f, Test: %.3f' % (train_loss, test_loss))\n",
    "    print('Accuracy ======= Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n",
    "\n",
    "    # list all data in history\n",
    "    \n",
    "    print(history.history.keys())\n",
    "    # summarize history for accuracy\n",
    "    pyplot.plot(history.history['acc'])\n",
    "    pyplot.plot(history.history['val_acc'])\n",
    "    pyplot.title('model accuracy')\n",
    "    pyplot.ylabel('accuracy')\n",
    "    pyplot.xlabel('epoch')\n",
    "    pyplot.legend(['train', 'test'], loc='upper left')\n",
    "    pyplot.show()\n",
    "    # summarize history for loss\n",
    "    pyplot.plot(history.history['loss'])\n",
    "    pyplot.plot(history.history['val_loss'])\n",
    "    pyplot.title('model loss')\n",
    "    pyplot.ylabel('loss')\n",
    "    pyplot.xlabel('epoch')\n",
    "    pyplot.legend(['train', 'test'], loc='upper left')\n",
    "    pyplot.show()\n",
    "\n",
    "    # serialize model to JSON\n",
    "    model_json = model.to_json()\n",
    "    with open(model_json_file, \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    # serialize weights to HDF5\n",
    "    model.save_weights(model_h5_file)\n",
    "    print(\"Saved model to disk\")\n",
    "    \n",
    "    return()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model2(X_train_new, y_train,X_test_new, y_test,in_batch_size=100,in_epochs=10): # RNN: Recurrent Neural Networks\n",
    "    # Initializing the Sequential model from KERAS.\n",
    "    model = Sequential()\n",
    "\n",
    "    # Convert labels to categorical one-hot encoding  \n",
    "    y_train_one_hot_labels = to_categorical(y_train, num_classes=n_classes)\n",
    "    y_test_one_hot_labels = to_categorical(y_test, num_classes=n_classes)\n",
    "\n",
    "    # Creating a 16 neuron hidden layer with Linear Rectified activation function.\n",
    "    #model.add(Dense(16, input_dim=1, init='uniform', activation='relu'))\n",
    "    model.add(Dense(16, input_dim=fixed_seq_length, kernel_initializer='uniform', activation='relu'))\n",
    "\n",
    "    # Creating a 8 neuron hidden layer.\n",
    "    model.add(Dense(8, kernel_initializer='uniform', activation='relu'))\n",
    "\n",
    "    # Adding a output layer.\n",
    "    model.add(Dense(n_classes, kernel_initializer='uniform', activation='softmax'))\n",
    "    \n",
    "    #opt = 'rmsprop'\n",
    "    #opt = optimizers.SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False)\n",
    "    #opt = optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=0.001, decay=0.0)\n",
    "    #opt = optimizers.Adagrad(lr=0.01, epsilon=None, decay=0.0)\n",
    "    #opt = optimizers.Adadelta(lr=1.0, rho=0.95, epsilon=None, decay=0.0)\n",
    "    opt = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "    #opt = optimizers.Adamax(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0)\n",
    "    #opt = optimizers.Nadam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, schedule_decay=0.004)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',optimizer=opt, metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "\n",
    "    # fit the model\n",
    "    history = model.fit(X_train_new, y_train_one_hot_labels, epochs=in_epochs, batch_size=in_batch_size,validation_data=(X_test_new, y_test_one_hot_labels), verbose=1)\n",
    "\n",
    "    # evaluate the model\n",
    "    train_loss, train_acc = model.evaluate(X_train_new, y_train_one_hot_labels, verbose=0)\n",
    "    test_loss, test_acc = model.evaluate(X_test_new, y_test_one_hot_labels, verbose=0)\n",
    "    print('Loss     ======= Train: %.3f, Test: %.3f' % (train_loss, test_loss))\n",
    "    print('Accuracy ======= Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n",
    "\n",
    "    # list all data in history\n",
    "    \n",
    "    print(history.history.keys())\n",
    "    # summarize history for accuracy\n",
    "    pyplot.plot(history.history['acc'])\n",
    "    pyplot.plot(history.history['val_acc'])\n",
    "    pyplot.title('model accuracy')\n",
    "    pyplot.ylabel('accuracy')\n",
    "    pyplot.xlabel('epoch')\n",
    "    pyplot.legend(['train', 'test'], loc='upper left')\n",
    "    pyplot.show()\n",
    "    # summarize history for loss\n",
    "    pyplot.plot(history.history['loss'])\n",
    "    pyplot.plot(history.history['val_loss'])\n",
    "    pyplot.title('model loss')\n",
    "    pyplot.ylabel('loss')\n",
    "    pyplot.xlabel('epoch')\n",
    "    pyplot.legend(['train', 'test'], loc='upper left')\n",
    "    pyplot.show()\n",
    "    \n",
    "    return()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_matrix(labels,C):\n",
    "    \n",
    "    C = tf.constant(C,name=\"C\")\n",
    "    one_hot_matrix = tf.one_hot(labels,C,axis=1)\n",
    "    sess = tf.Session()\n",
    "    one_hot = sess.run(one_hot_matrix)\n",
    "    sess.close()\n",
    "    \n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model3(X_train_new, y_train,X_test_new, y_test, batch_size =100, hm_epochs =100): # CNN: Convolutional Neural Networks\n",
    "    # Number of nodes in each NN hidden layer\n",
    "    n_nodes_hl1 = 1500\n",
    "    n_nodes_hl2 = 1500\n",
    "    n_nodes_hl3 = 1500\n",
    "\n",
    "    # Number of orthology clusters\n",
    "    #n_classes = len(np.unique(np.concatenate((y_train,y_test),axis=0)))     #2 or 3 or ...\n",
    "    \n",
    "    #y_all = np.concatenate((y_train,y_test),axis=0)\n",
    "    #y_min = np.amin(y_all)\n",
    "    #n_classes = np.amax(y_all-y_min)+1\n",
    "    \n",
    "    \n",
    "    train_y = one_hot_matrix(y_train,n_classes)\n",
    "    test_y = one_hot_matrix(y_test,n_classes)\n",
    "\n",
    "    # Batch size and Epoch size for training the NN\n",
    "    #batch_size = 100   #100\n",
    "    #hm_epochs = 100    #1000\n",
    "\n",
    "    # Initializing X and Y\n",
    "    x = tf.placeholder('float')\n",
    "    y = tf.placeholder('float')\n",
    "\n",
    "    # Initializing NN layers\n",
    "    hidden_1_layer = {'f_fum':n_nodes_hl1,\n",
    "                  'weight':tf.Variable(tf.random_normal([len(X_train_new[0]), n_nodes_hl1])),\n",
    "                  'bias':tf.Variable(tf.random_normal([n_nodes_hl1]))}\n",
    "\n",
    "    hidden_2_layer = {'f_fum':n_nodes_hl2,\n",
    "                  'weight':tf.Variable(tf.random_normal([n_nodes_hl1, n_nodes_hl2])),\n",
    "                  'bias':tf.Variable(tf.random_normal([n_nodes_hl2]))}\n",
    "\n",
    "    hidden_3_layer = {'f_fum':n_nodes_hl3,\n",
    "                  'weight':tf.Variable(tf.random_normal([n_nodes_hl2, n_nodes_hl3])),\n",
    "                  'bias':tf.Variable(tf.random_normal([n_nodes_hl3]))}\n",
    "\n",
    "    output_layer = {'f_fum':None,\n",
    "                'weight':tf.Variable(tf.random_normal([n_nodes_hl3, n_classes])),\n",
    "                'bias':tf.Variable(tf.random_normal([n_classes])),}\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    l1 = tf.add(tf.matmul(x,hidden_1_layer['weight']), hidden_1_layer['bias'])\n",
    "    l1 = tf.nn.relu(l1)\n",
    "\n",
    "    l2 = tf.add(tf.matmul(l1,hidden_2_layer['weight']), hidden_2_layer['bias'])\n",
    "    l2 = tf.nn.relu(l2)\n",
    "\n",
    "    l3 = tf.add(tf.matmul(l2,hidden_3_layer['weight']), hidden_3_layer['bias'])\n",
    "    l3 = tf.nn.relu(l3)\n",
    "\n",
    "    prediction = tf.matmul(l3,output_layer['weight']) + output_layer['bias']\n",
    "\n",
    "        \n",
    "    cost = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits_v2(logits=prediction,labels=y) )\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=0.001).minimize(cost)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        #try:\n",
    "        #    epoch = int(open(tf_log,'r').read().split('\\n')[-2])+1\n",
    "        #    print('STARTING:',epoch)\n",
    "        #except:\n",
    "        #    epoch = 1\n",
    "        epoch = 1\n",
    "\n",
    "        while epoch <= hm_epochs:\n",
    "            epoch_loss = 1\n",
    "            \n",
    "            i=0\n",
    "            while i < len(X_train_new):\n",
    "                start = i\n",
    "                end = i+batch_size\n",
    "                batch_x = np.array(X_train_new[start:end])\n",
    "                batch_y = np.array(train_y[start:end])\n",
    "\n",
    "                _, c = sess.run([optimizer, cost], feed_dict={x: batch_x,y: batch_y})\n",
    "                epoch_loss += c\n",
    "                i+=batch_size\n",
    "                \n",
    "            \n",
    "            print('Epoch ',epoch,' out of ',hm_epochs,'- loss:',epoch_loss)\n",
    " \n",
    "            \n",
    "            #with open(tf_log,'a') as f:\n",
    "            #    f.write(str(epoch)+'\\n') \n",
    "            epoch +=1\n",
    "            \n",
    "        correct = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct, 'float'))\n",
    "\n",
    "        #print(\"\\nModel saved in path: %s \" % my_model_save_path)\n",
    "        print('\\nAccuracy:',accuracy.eval({x:X_test_new, y:test_y}) * 100)\n",
    "    return()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model4(X_train_new, y_train,X_test_new, y_test,in_batch_size=100,in_epochs=10): \n",
    "    # RNN: Recurrent Neural Networks\n",
    "    \n",
    "    # Convert labels to categorical one-hot encoding  \n",
    "    y_train_one_hot_labels = to_categorical(y_train, num_classes=n_classes)\n",
    "    y_test_one_hot_labels = to_categorical(y_test, num_classes=n_classes)\n",
    "\n",
    "    # create the model    \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(num_letters, output_dim=fixed_seq_length))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv1D(filters=64, kernel_size=5, padding='same', activation='relu'))\n",
    "    model.add(MaxPooling1D(4))\n",
    "    model.add(LSTM(70))\n",
    "    model.add(Dense(n_classes, activation='softmax'))\n",
    "    \n",
    "    #opt = 'rmsprop'\n",
    "    #opt = optimizers.SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False)\n",
    "    #opt = optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=0.001, decay=0.0)\n",
    "    #opt = optimizers.Adagrad(lr=0.01, epsilon=None, decay=0.0)\n",
    "    #opt = optimizers.Adadelta(lr=1.0, rho=0.95, epsilon=None, decay=0.0)\n",
    "    opt = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "    #opt = optimizers.Adamax(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0)\n",
    "    #opt = optimizers.Nadam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, schedule_decay=0.004)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',optimizer=opt, metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "\n",
    "    # fit the model\n",
    "    history = model.fit(X_train_new, y_train_one_hot_labels, epochs=in_epochs, batch_size=in_batch_size,validation_data=(X_test_new, y_test_one_hot_labels), verbose=1)\n",
    "\n",
    "    # evaluate the model\n",
    "    train_loss, train_acc = model.evaluate(X_train_new, y_train_one_hot_labels, verbose=0)\n",
    "    test_loss, test_acc = model.evaluate(X_test_new, y_test_one_hot_labels, verbose=0)\n",
    "    print('Loss     ======= Train: %.3f, Test: %.3f' % (train_loss, test_loss))\n",
    "    print('Accuracy ======= Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n",
    "\n",
    "    # list all data in history\n",
    "    \n",
    "    print(history.history.keys())\n",
    "    # summarize history for accuracy\n",
    "    pyplot.plot(history.history['acc'])\n",
    "    pyplot.plot(history.history['val_acc'])\n",
    "    pyplot.title('model accuracy')\n",
    "    pyplot.ylabel('accuracy')\n",
    "    pyplot.xlabel('epoch')\n",
    "    pyplot.legend(['train', 'test'], loc='upper left')\n",
    "    pyplot.show()\n",
    "    # summarize history for loss\n",
    "    pyplot.plot(history.history['loss'])\n",
    "    pyplot.plot(history.history['val_loss'])\n",
    "    pyplot.title('model loss')\n",
    "    pyplot.ylabel('loss')\n",
    "    pyplot.xlabel('epoch')\n",
    "    pyplot.legend(['train', 'test'], loc='upper left')\n",
    "    pyplot.show()\n",
    "    \n",
    "  \n",
    "    return()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model5(X_train_new, y_train,X_test_new, y_test,in_batch_size=100,in_epochs=10,model_json_file=\"model.json\",model_h5_file=\"model.h5\"): # RNN: Recurrent Neural Networks\n",
    "    # LSTM\n",
    "    \n",
    "    # Convert labels to categorical one-hot encoding  \n",
    "    y_train_one_hot_labels = to_categorical(y_train, num_classes=n_classes)\n",
    "    y_test_one_hot_labels = to_categorical(y_test, num_classes=n_classes)\n",
    "\n",
    "    # create the model    \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(num_letters, output_dim=fixed_seq_length))\n",
    "    model.add(LSTM(500,return_sequences=True))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(LSTM(250,return_sequences=True))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(LSTM(125,return_sequences=True))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(LSTM(75,return_sequences=True))\n",
    "    model.add(LSTM(25))\n",
    "    #model.add(Dropout(0.25))\n",
    "    model.add(Dense(n_classes, activation='softmax'))\n",
    "    \n",
    "    #opt = 'rmsprop'\n",
    "    #opt = optimizers.SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False)\n",
    "    #opt = optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=0.001, decay=0.0)\n",
    "    #opt = optimizers.Adagrad(lr=0.01, epsilon=None, decay=0.0)\n",
    "    #opt = optimizers.Adadelta(lr=1.0, rho=0.95, epsilon=None, decay=0.0)\n",
    "    opt = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "    #opt = optimizers.Adamax(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0)\n",
    "    #opt = optimizers.Nadam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, schedule_decay=0.004)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',optimizer=opt, metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "\n",
    "    # fit the model\n",
    "    history = model.fit(X_train_new, y_train_one_hot_labels, epochs=in_epochs, batch_size=in_batch_size,validation_data=(X_test_new, y_test_one_hot_labels), verbose=1)\n",
    "\n",
    "    # evaluate the model\n",
    "    train_loss, train_acc = model.evaluate(X_train_new, y_train_one_hot_labels, verbose=0)\n",
    "    test_loss, test_acc = model.evaluate(X_test_new, y_test_one_hot_labels, verbose=0)\n",
    "    print('Loss     ======= Train: %.3f, Test: %.3f' % (train_loss, test_loss))\n",
    "    print('Accuracy ======= Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n",
    "\n",
    "    # list all data in history\n",
    "    \n",
    "    print(history.history.keys())\n",
    "    # summarize history for accuracy\n",
    "    pyplot.plot(history.history['acc'])\n",
    "    pyplot.plot(history.history['val_acc'])\n",
    "    pyplot.title('model accuracy')\n",
    "    pyplot.ylabel('accuracy')\n",
    "    pyplot.xlabel('epoch')\n",
    "    pyplot.legend(['train', 'test'], loc='upper left')\n",
    "    pyplot.show()\n",
    "    # summarize history for loss\n",
    "    pyplot.plot(history.history['loss'])\n",
    "    pyplot.plot(history.history['val_loss'])\n",
    "    pyplot.title('model loss')\n",
    "    pyplot.ylabel('loss')\n",
    "    pyplot.xlabel('epoch')\n",
    "    pyplot.legend(['train', 'test'], loc='upper left')\n",
    "    pyplot.show()\n",
    "    \n",
    "    # serialize model to JSON\n",
    "    model_json = model.to_json()\n",
    "    with open(model_json_file, \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    # serialize weights to HDF5\n",
    "    model.save_weights(model_h5_file)\n",
    "    print(\"Saved model to disk\")\n",
    "\n",
    "    return()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model6(X_train_new, y_train,X_test_new, y_test,in_batch_size=100,in_epochs=10): # RNN: Recurrent Neural Networks\n",
    "    # GRU\n",
    "    \n",
    "    # Convert labels to categorical one-hot encoding  \n",
    "    y_train_one_hot_labels = to_categorical(y_train, num_classes=n_classes)\n",
    "    y_test_one_hot_labels = to_categorical(y_test, num_classes=n_classes)\n",
    "    \n",
    "    # create the model    \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(num_letters, output_dim=fixed_seq_length))\n",
    "    model.add(GRU(128, return_sequences=False))\n",
    "    model.add(Dense(n_classes, activation='softmax'))\n",
    "\n",
    "    #opt = 'rmsprop'\n",
    "    #opt = optimizers.SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False)\n",
    "    #opt = optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=0.001, decay=0.0)\n",
    "    #opt = optimizers.Adagrad(lr=0.01, epsilon=None, decay=0.0)\n",
    "    #opt = optimizers.Adadelta(lr=1.0, rho=0.95, epsilon=None, decay=0.0)\n",
    "    opt = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "    #opt = optimizers.Adamax(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0)\n",
    "    #opt = optimizers.Nadam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, schedule_decay=0.004)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',optimizer=opt, metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "\n",
    "    # fit the model\n",
    "    history = model.fit(X_train_new, y_train_one_hot_labels, epochs=in_epochs, batch_size=in_batch_size,validation_data=(X_test_new, y_test_one_hot_labels), verbose=1)\n",
    "\n",
    "    # evaluate the model\n",
    "    train_loss, train_acc = model.evaluate(X_train_new, y_train_one_hot_labels, verbose=0)\n",
    "    test_loss, test_acc = model.evaluate(X_test_new, y_test_one_hot_labels, verbose=0)\n",
    "    print('Loss     ======= Train: %.3f, Test: %.3f' % (train_loss, test_loss))\n",
    "    print('Accuracy ======= Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n",
    "\n",
    "    # list all data in history\n",
    "    \n",
    "    print(history.history.keys())\n",
    "    # summarize history for accuracy\n",
    "    pyplot.plot(history.history['acc'])\n",
    "    pyplot.plot(history.history['val_acc'])\n",
    "    pyplot.title('model accuracy')\n",
    "    pyplot.ylabel('accuracy')\n",
    "    pyplot.xlabel('epoch')\n",
    "    pyplot.legend(['train', 'test'], loc='upper left')\n",
    "    pyplot.show()\n",
    "    # summarize history for loss\n",
    "    pyplot.plot(history.history['loss'])\n",
    "    pyplot.plot(history.history['val_loss'])\n",
    "    pyplot.title('model loss')\n",
    "    pyplot.ylabel('loss')\n",
    "    pyplot.xlabel('epoch')\n",
    "    pyplot.legend(['train', 'test'], loc='upper left')\n",
    "    pyplot.show()\n",
    "    \n",
    "    return()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def use_model(model_json_file=\"model.json\",model_h5_file=\"model.h5\"):\n",
    "    # load json and create model\n",
    "    json_file = open(model_json_file, 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    loaded_model = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    loaded_model.load_weights(model_h5_file)\n",
    "    print(\"Loaded model from disk\")\n",
    " \n",
    "    # evaluate loaded model on test data\n",
    "    loaded_model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "    score = loaded_model.evaluate(X, Y, verbose=0)\n",
    "    print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))\n",
    "    \n",
    "    return()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model7(X_train_new, y_train,X_test_new, y_test,in_batch_size=100,in_epochs=10,model_json_file=\"model.json\",model_h5_file=\"model.h5\"): # RNN: Recurrent Neural Networks\n",
    "    # GRU\n",
    "    \n",
    "    # Convert labels to categorical one-hot encoding  \n",
    "    y_train_one_hot_labels = to_categorical(y_train, num_classes=n_classes)\n",
    "    y_test_one_hot_labels = to_categorical(y_test, num_classes=n_classes)\n",
    "    \n",
    "    # create the model    \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(num_letters, output_dim=fixed_seq_length))\n",
    "    model.add(GRU(128, return_sequences=False))\n",
    "    model.add(Dense(n_classes, activation='softmax'))\n",
    "\n",
    "    #opt = 'rmsprop'\n",
    "    #opt = optimizers.SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False)\n",
    "    #opt = optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=0.001, decay=0.0)\n",
    "    #opt = optimizers.Adagrad(lr=0.01, epsilon=None, decay=0.0)\n",
    "    #opt = optimizers.Adadelta(lr=1.0, rho=0.95, epsilon=None, decay=0.0)\n",
    "    opt = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "    #opt = optimizers.Adamax(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0)\n",
    "    #opt = optimizers.Nadam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, schedule_decay=0.004)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',optimizer=opt, metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "\n",
    "    # fit the model\n",
    "    history = model.fit(X_train_new, y_train_one_hot_labels, epochs=in_epochs, batch_size=in_batch_size,validation_data=(X_test_new, y_test_one_hot_labels), verbose=1)\n",
    "\n",
    "    # evaluate the model\n",
    "    train_loss, train_acc = model.evaluate(X_train_new, y_train_one_hot_labels, verbose=0)\n",
    "    test_loss, test_acc = model.evaluate(X_test_new, y_test_one_hot_labels, verbose=0)\n",
    "    print('Loss     ======= Train: %.3f, Test: %.3f' % (train_loss, test_loss))\n",
    "    print('Accuracy ======= Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n",
    "\n",
    "    # list all data in history\n",
    "    \n",
    "    print(history.history.keys())\n",
    "    # summarize history for accuracy\n",
    "    pyplot.plot(history.history['acc'])\n",
    "    pyplot.plot(history.history['val_acc'])\n",
    "    pyplot.title('model accuracy')\n",
    "    pyplot.ylabel('accuracy')\n",
    "    pyplot.xlabel('epoch')\n",
    "    pyplot.legend(['train', 'test'], loc='upper left')\n",
    "    pyplot.show()\n",
    "    # summarize history for loss\n",
    "    pyplot.plot(history.history['loss'])\n",
    "    pyplot.plot(history.history['val_loss'])\n",
    "    pyplot.title('model loss')\n",
    "    pyplot.ylabel('loss')\n",
    "    pyplot.xlabel('epoch')\n",
    "    pyplot.legend(['train', 'test'], loc='upper left')\n",
    "    pyplot.show()\n",
    " \n",
    "    # serialize model to JSON\n",
    "    model_json = model.to_json()\n",
    "    with open(model_json_file, \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    # serialize weights to HDF5\n",
    "    model.save_weights(model_h5_file)\n",
    "    print(\"Saved model to disk\")\n",
    "    \n",
    "    return()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model8(X_train_new, y_train,X_test_new, y_test,in_batch_size=100,in_epochs=10,model_json_file=\"model.json\",model_h5_file=\"model.h5\"): # RNN: Recurrent Neural Networks\n",
    "    # https://keras.io/getting-started/sequential-model-guide/\n",
    "\n",
    "    # Convert labels to categorical one-hot encoding  \n",
    "    y_train_one_hot_labels = to_categorical(y_train, num_classes=n_classes)\n",
    "    y_test_one_hot_labels = to_categorical(y_test, num_classes=n_classes)\n",
    "\n",
    "    # create the model\n",
    "    embedding_vecor_length = 4\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(num_letters, embedding_vecor_length, input_length=fixed_seq_length))\n",
    "    model.add(Conv1D(filters=64, kernel_size=3, padding='same', activation='relu'))\n",
    "    model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
    "    model.add(MaxPooling1D(3))\n",
    "    model.add(Conv1D(filters=128, kernel_size=3, activation='relu'))\n",
    "    model.add(Conv1D(filters=128, kernel_size=3, activation='relu'))\n",
    "    model.add(MaxPooling1D(3))\n",
    "    model.add(Conv1D(filters=256, kernel_size=3, activation='relu'))\n",
    "    model.add(Conv1D(filters=256, kernel_size=3, activation='relu'))\n",
    "    model.add(MaxPooling1D(3))\n",
    "    model.add(Conv1D(filters=512, kernel_size=3, activation='relu'))\n",
    "    model.add(Conv1D(filters=512, kernel_size=3, activation='relu'))\n",
    "    #model.add(GlobalAveragePooling1D())\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(4096, activation='relu'))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(4096, activation='relu'))\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    #model.add(GRU(128, return_sequences=True))\n",
    "    model.add(LSTM(128,return_sequences=True))\n",
    "    model.add(LSTM(256,return_sequences=False))\n",
    "        \n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(n_classes, activation='softmax'))\n",
    "    \n",
    "    #opt = 'rmsprop'\n",
    "    #opt = optimizers.SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False)\n",
    "    #opt = optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=0.001, decay=0.0)\n",
    "    #opt = optimizers.Adagrad(lr=0.01, epsilon=None, decay=0.0)\n",
    "    #opt = optimizers.Adadelta(lr=1.0, rho=0.95, epsilon=None, decay=0.0)\n",
    "    opt = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "    #opt = optimizers.Adamax(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0)\n",
    "    #opt = optimizers.Nadam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, schedule_decay=0.004)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',optimizer=opt, metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "\n",
    "    # fit the model\n",
    "    history = model.fit(X_train_new, y_train_one_hot_labels, epochs=in_epochs, batch_size=in_batch_size,validation_data=(X_test_new, y_test_one_hot_labels), verbose=1)\n",
    "\n",
    "    # evaluate the model\n",
    "    train_loss, train_acc = model.evaluate(X_train_new, y_train_one_hot_labels, verbose=0)\n",
    "    test_loss, test_acc = model.evaluate(X_test_new, y_test_one_hot_labels, verbose=0)\n",
    "    print('Loss     ======= Train: %.3f, Test: %.3f' % (train_loss, test_loss))\n",
    "    print('Accuracy ======= Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n",
    "\n",
    "    # list all data in history\n",
    "    \n",
    "    print(history.history.keys())\n",
    "    # summarize history for accuracy\n",
    "    pyplot.plot(history.history['acc'])\n",
    "    pyplot.plot(history.history['val_acc'])\n",
    "    pyplot.title('model accuracy')\n",
    "    pyplot.ylabel('accuracy')\n",
    "    pyplot.xlabel('epoch')\n",
    "    pyplot.legend(['train', 'test'], loc='upper left')\n",
    "    pyplot.show()\n",
    "    # summarize history for loss\n",
    "    pyplot.plot(history.history['loss'])\n",
    "    pyplot.plot(history.history['val_loss'])\n",
    "    pyplot.title('model loss')\n",
    "    pyplot.ylabel('loss')\n",
    "    pyplot.xlabel('epoch')\n",
    "    pyplot.legend(['train', 'test'], loc='upper left')\n",
    "    pyplot.show()\n",
    "\n",
    "    # serialize model to JSON\n",
    "    model_json = model.to_json()\n",
    "    with open(model_json_file, \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    # serialize weights to HDF5\n",
    "    model.save_weights(model_h5_file)\n",
    "    print(\"Saved model to disk\")\n",
    "    \n",
    "    return()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model9(X_train_new, y_train,X_test_new, y_test,in_batch_size=100,in_epochs=10,model_json_file=\"model.json\",model_h5_file=\"model.h5\"): # RNN: Recurrent Neural Networks\n",
    "    # https://keras.io/getting-started/sequential-model-guide/\n",
    "\n",
    "    # Convert labels to categorical one-hot encoding  \n",
    "    y_train_one_hot_labels = to_categorical(y_train, num_classes=n_classes)\n",
    "    y_test_one_hot_labels = to_categorical(y_test, num_classes=n_classes)\n",
    "\n",
    "    # create the model\n",
    "    embedding_vecor_length = 4\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(num_letters, embedding_vecor_length, input_length=fixed_seq_length))\n",
    "    model.add(TimeDistributed(Conv1D(filters=64, kernel_size=3, padding='same', activation='relu')))\n",
    "    model.add(TimeDistributed(Conv1D(filters=64, kernel_size=3, activation='relu')))\n",
    "    model.add(TimeDistributed(MaxPooling1D(3)))\n",
    "    model.add(TimeDistributed(Conv1D(filters=128, kernel_size=3, activation='relu')))\n",
    "    model.add(TimeDistributed(Conv1D(filters=128, kernel_size=3, activation='relu')))\n",
    "    model.add(TimeDistributed(MaxPooling1D(3)))\n",
    "    model.add(TimeDistributed(Conv1D(filters=256, kernel_size=3, activation='relu')))\n",
    "    model.add(TimeDistributed(Conv1D(filters=256, kernel_size=3, activation='relu')))\n",
    "    model.add(TimeDistributed(MaxPooling1D(3)))\n",
    "    model.add(TimeDistributed(Conv1D(filters=512, kernel_size=3, activation='relu')))\n",
    "    model.add(TimeDistributed(Conv1D(filters=512, kernel_size=3, activation='relu')))\n",
    "    #model.add(GlobalAveragePooling1D())\n",
    "    model.add(TimeDistributed(Dropout(0.25)))\n",
    "    model.add(TimeDistributed(Dense(4096, activation='relu')))\n",
    "    model.add(TimeDistributed(Dropout(0.25)))\n",
    "    model.add(TimeDistributed(Dense(4096, activation='relu')))\n",
    "    model.add(TimeDistributed(Dropout(0.25)))\n",
    "    \n",
    "    model.add(TimeDistributed(MaxPooling1D(pool_size=2)))\n",
    "    #model.add(GRU(128, return_sequences=True))\n",
    "    model.add(LSTM(128,return_sequences=True))\n",
    "    model.add(LSTM(256,return_sequences=False))\n",
    "        \n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(n_classes, activation='softmax'))\n",
    "    \n",
    "    #opt = 'rmsprop'\n",
    "    #opt = optimizers.SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False)\n",
    "    #opt = optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=0.001, decay=0.0)\n",
    "    #opt = optimizers.Adagrad(lr=0.01, epsilon=None, decay=0.0)\n",
    "    #opt = optimizers.Adadelta(lr=1.0, rho=0.95, epsilon=None, decay=0.0)\n",
    "    opt = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "    #opt = optimizers.Adamax(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0)\n",
    "    #opt = optimizers.Nadam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, schedule_decay=0.004)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',optimizer=opt, metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "\n",
    "    # fit the model\n",
    "    history = model.fit(X_train_new, y_train_one_hot_labels, epochs=in_epochs, batch_size=in_batch_size,validation_data=(X_test_new, y_test_one_hot_labels), verbose=1)\n",
    "\n",
    "    # evaluate the model\n",
    "    train_loss, train_acc = model.evaluate(X_train_new, y_train_one_hot_labels, verbose=0)\n",
    "    test_loss, test_acc = model.evaluate(X_test_new, y_test_one_hot_labels, verbose=0)\n",
    "    print('Loss     ======= Train: %.3f, Test: %.3f' % (train_loss, test_loss))\n",
    "    print('Accuracy ======= Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n",
    "\n",
    "    # list all data in history\n",
    "    \n",
    "    print(history.history.keys())\n",
    "    # summarize history for accuracy\n",
    "    pyplot.plot(history.history['acc'])\n",
    "    pyplot.plot(history.history['val_acc'])\n",
    "    pyplot.title('model accuracy')\n",
    "    pyplot.ylabel('accuracy')\n",
    "    pyplot.xlabel('epoch')\n",
    "    pyplot.legend(['train', 'test'], loc='upper left')\n",
    "    pyplot.show()\n",
    "    # summarize history for loss\n",
    "    pyplot.plot(history.history['loss'])\n",
    "    pyplot.plot(history.history['val_loss'])\n",
    "    pyplot.title('model loss')\n",
    "    pyplot.ylabel('loss')\n",
    "    pyplot.xlabel('epoch')\n",
    "    pyplot.legend(['train', 'test'], loc='upper left')\n",
    "    pyplot.show()\n",
    "\n",
    "    # serialize model to JSON\n",
    "    model_json = model.to_json()\n",
    "    with open(model_json_file, \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    # serialize weights to HDF5\n",
    "    model.save_weights(model_h5_file)\n",
    "    print(\"Saved model to disk\")\n",
    "    \n",
    "    return()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model10(X_train_new, y_train,X_test_new, y_test,in_batch_size=100,in_epochs=10,model_json_file=\"model.json\",model_h5_file=\"model.h5\"): # RNN: Recurrent Neural Networks\n",
    "    # LSTM\n",
    "    \n",
    "    # Convert labels to categorical one-hot encoding  \n",
    "    y_train_one_hot_labels = to_categorical(y_train, num_classes=n_classes)\n",
    "    y_test_one_hot_labels = to_categorical(y_test, num_classes=n_classes)\n",
    "\n",
    "    # create the model    \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(num_letters, output_dim=fixed_seq_length))\n",
    "    model.add(LSTM(75,return_sequences=True))\n",
    "    model.add(LSTM(25))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(16, input_dim=fixed_seq_length, kernel_initializer='uniform', activation='relu'))\n",
    "    model.add(Dense(8, kernel_initializer='uniform', activation='relu'))\n",
    "    model.add(Dense(n_classes, kernel_initializer='uniform', activation='softmax'))\n",
    "    \n",
    "    #opt = 'rmsprop'\n",
    "    #opt = optimizers.SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False)\n",
    "    #opt = optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=0.001, decay=0.0)\n",
    "    #opt = optimizers.Adagrad(lr=0.01, epsilon=None, decay=0.0)\n",
    "    #opt = optimizers.Adadelta(lr=1.0, rho=0.95, epsilon=None, decay=0.0)\n",
    "    opt = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "    #opt = optimizers.Adamax(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0)\n",
    "    #opt = optimizers.Nadam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, schedule_decay=0.004)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',optimizer=opt, metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "\n",
    "    # fit the model\n",
    "    history = model.fit(X_train_new, y_train_one_hot_labels, epochs=in_epochs, batch_size=in_batch_size,validation_data=(X_test_new, y_test_one_hot_labels), verbose=1)\n",
    "\n",
    "    # evaluate the model\n",
    "    train_loss, train_acc = model.evaluate(X_train_new, y_train_one_hot_labels, verbose=0)\n",
    "    test_loss, test_acc = model.evaluate(X_test_new, y_test_one_hot_labels, verbose=0)\n",
    "    print('Loss     ======= Train: %.3f, Test: %.3f' % (train_loss, test_loss))\n",
    "    print('Accuracy ======= Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n",
    "\n",
    "    # list all data in history\n",
    "    \n",
    "    print(history.history.keys())\n",
    "    # summarize history for accuracy\n",
    "    pyplot.plot(history.history['acc'])\n",
    "    pyplot.plot(history.history['val_acc'])\n",
    "    pyplot.title('model accuracy')\n",
    "    pyplot.ylabel('accuracy')\n",
    "    pyplot.xlabel('epoch')\n",
    "    pyplot.legend(['train', 'test'], loc='upper left')\n",
    "    pyplot.show()\n",
    "    # summarize history for loss\n",
    "    pyplot.plot(history.history['loss'])\n",
    "    pyplot.plot(history.history['val_loss'])\n",
    "    pyplot.title('model loss')\n",
    "    pyplot.ylabel('loss')\n",
    "    pyplot.xlabel('epoch')\n",
    "    pyplot.legend(['train', 'test'], loc='upper left')\n",
    "    pyplot.show()\n",
    "    \n",
    "    # serialize model to JSON\n",
    "    model_json = model.to_json()\n",
    "    with open(model_json_file, \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    # serialize weights to HDF5\n",
    "    model.save_weights(model_h5_file)\n",
    "    print(\"Saved model to disk\")\n",
    "\n",
    "    return()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_1(d_set):\n",
    "    print(d_set[:,0])\n",
    "    print(d_set[0:2,1])\n",
    "    print(d_set[:,2].astype(int))\n",
    "    print(d_set.shape)\n",
    "    print(d_set[:,1].shape)\n",
    "    return()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_2(d_set):\n",
    "    integer_encoded_proteins = np.array([protein2integer(seq) for seq in d_set[:,1]])\n",
    "    print(len(integer_encoded_proteins))\n",
    "    print(integer_encoded_proteins[0])\n",
    "    #np.array(integer_encoded_proteins).shape\n",
    "    print(integer_encoded_proteins.shape)\n",
    "    #protein2integer(dataset[:,1])\n",
    "    return()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_3(G,X,Y):\n",
    "    print(G.shape)\n",
    "    print(X.shape)\n",
    "    print(Y.shape)\n",
    "\n",
    "    print(G[0:3,])\n",
    "    print(X[0:3,])\n",
    "    print(Y[0:3,])\n",
    "    \n",
    "    print('Maximum sequence length: {}'.format(len(max(X, key=len))))\n",
    "    print('Minimum sequence length: {}'.format(len(min(X, key=len))))\n",
    "    \n",
    "    return()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_4(X_train_new,X_train):\n",
    "    print(X_train_new.shape)\n",
    "    print(X_train_new[0,:])\n",
    "    print(X_train.shape)\n",
    "    print(X_train[0,])\n",
    "    return()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, G, X, Y = make_dataset(data_path)\n",
    "X_train,y_train,X_test,y_test = make_train_test_set_idea2(G,X,Y)\n",
    "\n",
    "if (os.path.exists(data_single_cluster_path)):\n",
    "    n_classes = int(np.amax(np.concatenate((y_train,y_test),axis=0))+1)\n",
    "    make_dataset_single(data_single_cluster_path,n_classes+1)\n",
    "    dataset, G, X, Y = make_dataset_single(data_single_cluster_path,n_classes+1)\n",
    "    X_train_s,y_train_s,X_test_s,y_test_s = make_train_test_set_idea1(G,X,Y)\n",
    "\n",
    "    X_train = np.concatenate((X_train,X_train_s),axis=0)\n",
    "    X_test = np.concatenate((X_test,X_test_s),axis=0)\n",
    "    y_train = np.concatenate((y_train,y_train_s),axis=0)\n",
    "    y_test = np.concatenate((y_test,y_test_s),axis=0)\n",
    "\n",
    "#print(\"============ Test 1 =======================\")\n",
    "#test_1(dataset)\n",
    "#print(\"============ Test 2 =======================\")\n",
    "#test_2(dataset)\n",
    "#print(\"============ Test 3 =======================\")\n",
    "#test_3(G,X,Y)\n",
    "#print(\"============ Test 4 =======================\")\n",
    "#test_4(X_train,X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "418\n"
     ]
    }
   ],
   "source": [
    "num_letters = len(list(IUPACData.extended_protein_letters)) # = 26\n",
    "#fixed_seq_length = len(max(X, key=len)) # maximum\n",
    "fixed_seq_length = (sum(len(X[i,]) for i in range(X.shape[0]))/X.shape[0])  # average ~490\n",
    "#fixed_seq_length = 1000\n",
    "n_classes = int(np.amax(np.concatenate((y_train,y_test),axis=0))+1)\n",
    "# truncate and pad input sequences\n",
    "X_train_new = sequence.pad_sequences(X_train, maxlen=fixed_seq_length, padding='post', truncating='post')\n",
    "X_test_new = sequence.pad_sequences(X_test, maxlen=fixed_seq_length, padding='post', truncating='post')\n",
    "  \n",
    "#print(\"============ Test 4 =======================\")\n",
    "#test_4(X_train_new,X_train)\n",
    "\n",
    "#all_length = []\n",
    "#for i in range(X.shape[0]):\n",
    "#    all_length.append(len(X[i,]))\n",
    "#fixed_seq_length = np.median(all_length)  # median ~394\n",
    "\n",
    "print(fixed_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 418, 4)            104       \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 418, 32)           416       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 209, 32)           0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 75)                32400     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 31273)             2376748   \n",
      "=================================================================\n",
      "Total params: 2,409,668\n",
      "Trainable params: 2,409,668\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 35746 samples, validate on 9044 samples\n",
      "Epoch 1/10\n",
      "35746/35746 [==============================] - 106s 3ms/step - loss: 9.6730 - acc: 0.0631 - val_loss: 9.5230 - val_acc: 0.0134\n",
      "Epoch 2/10\n",
      "35746/35746 [==============================] - 103s 3ms/step - loss: 8.8352 - acc: 0.0639 - val_loss: 9.3086 - val_acc: 0.0134\n",
      "Epoch 3/10\n",
      "35746/35746 [==============================] - 102s 3ms/step - loss: 8.7740 - acc: 0.0639 - val_loss: 9.2541 - val_acc: 0.0134\n",
      "Epoch 4/10\n",
      "35746/35746 [==============================] - 105s 3ms/step - loss: 8.7614 - acc: 0.0639 - val_loss: 9.2425 - val_acc: 0.0134\n",
      "Epoch 5/10\n",
      "35746/35746 [==============================] - 105s 3ms/step - loss: 8.7574 - acc: 0.0639 - val_loss: 9.2271 - val_acc: 0.0134\n",
      "Epoch 6/10\n",
      "35746/35746 [==============================] - 106s 3ms/step - loss: 8.7565 - acc: 0.0639 - val_loss: 9.2265 - val_acc: 0.0134\n",
      "Epoch 7/10\n",
      "35746/35746 [==============================] - 106s 3ms/step - loss: 8.7554 - acc: 0.0639 - val_loss: 9.2362 - val_acc: 0.0134\n",
      "Epoch 8/10\n",
      "35746/35746 [==============================] - 105s 3ms/step - loss: 8.7550 - acc: 0.0639 - val_loss: 9.2305 - val_acc: 0.0134\n",
      "Epoch 9/10\n",
      "35746/35746 [==============================] - 106s 3ms/step - loss: 8.7511 - acc: 0.0639 - val_loss: 9.2265 - val_acc: 0.0134\n",
      "Epoch 10/10\n",
      "35746/35746 [==============================] - 105s 3ms/step - loss: 8.7507 - acc: 0.0639 - val_loss: 9.2319 - val_acc: 0.0134\n",
      "Loss     ======= Train: 8.628, Test: 9.232\n",
      "Accuracy ======= Train: 0.064, Test: 0.013\n",
      "*****\n",
      "['acc', 'loss', 'val_acc', 'val_loss']\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1(X_train_new, y_train, X_test_new, y_test,256,10,\"results/CNV_LSTM_model_test.json\",model_h5_file=\"results/CNV_LSTM_model_test.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_6 (Dense)              (None, 16)                6704      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 31273)             281457    \n",
      "=================================================================\n",
      "Total params: 288,297\n",
      "Trainable params: 288,297\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 35746 samples, validate on 9044 samples\n",
      "Epoch 1/100\n",
      "35746/35746 [==============================] - 42s 1ms/step - loss: 9.6841 - acc: 0.0493 - val_loss: 9.4511 - val_acc: 0.0134\n",
      "Epoch 2/100\n",
      "35746/35746 [==============================] - 40s 1ms/step - loss: 8.7911 - acc: 0.0639 - val_loss: 9.2225 - val_acc: 0.0134\n",
      "Epoch 3/100\n",
      "35746/35746 [==============================] - 39s 1ms/step - loss: 8.5481 - acc: 0.0639 - val_loss: 8.9906 - val_acc: 0.0134\n",
      "Epoch 4/100\n",
      "35746/35746 [==============================] - 40s 1ms/step - loss: 8.3284 - acc: 0.0639 - val_loss: 8.7970 - val_acc: 0.0134\n",
      "Epoch 5/100\n",
      "35746/35746 [==============================] - 41s 1ms/step - loss: 8.1921 - acc: 0.0639 - val_loss: 8.6916 - val_acc: 0.0134\n",
      "Epoch 6/100\n",
      "35746/35746 [==============================] - 41s 1ms/step - loss: 8.0804 - acc: 0.0639 - val_loss: 8.5444 - val_acc: 0.0134\n",
      "Epoch 7/100\n",
      "35746/35746 [==============================] - 40s 1ms/step - loss: 7.9652 - acc: 0.0639 - val_loss: 8.4036 - val_acc: 0.0134\n",
      "Epoch 8/100\n",
      "35746/35746 [==============================] - 41s 1ms/step - loss: 7.8472 - acc: 0.0639 - val_loss: 8.3244 - val_acc: 0.0134\n",
      "Epoch 9/100\n",
      "35746/35746 [==============================] - 43s 1ms/step - loss: 7.7640 - acc: 0.0639 - val_loss: 8.2599 - val_acc: 0.0134\n",
      "Epoch 10/100\n",
      "35746/35746 [==============================] - 43s 1ms/step - loss: 7.7032 - acc: 0.0639 - val_loss: 8.2256 - val_acc: 0.0134\n",
      "Epoch 11/100\n",
      "35746/35746 [==============================] - 42s 1ms/step - loss: 7.6480 - acc: 0.0639 - val_loss: 8.2078 - val_acc: 0.0134\n",
      "Epoch 12/100\n",
      "35746/35746 [==============================] - 42s 1ms/step - loss: 7.5983 - acc: 0.0639 - val_loss: 8.1855 - val_acc: 0.0134\n",
      "Epoch 13/100\n",
      "35746/35746 [==============================] - 42s 1ms/step - loss: 7.5485 - acc: 0.0639 - val_loss: 8.1886 - val_acc: 0.0133\n",
      "Epoch 14/100\n",
      "35746/35746 [==============================] - 42s 1ms/step - loss: 7.5017 - acc: 0.0638 - val_loss: 8.1958 - val_acc: 0.0133\n",
      "Epoch 15/100\n",
      "35746/35746 [==============================] - 42s 1ms/step - loss: 7.4597 - acc: 0.0637 - val_loss: 8.1851 - val_acc: 0.0133\n",
      "Epoch 16/100\n",
      "35746/35746 [==============================] - 42s 1ms/step - loss: 7.4184 - acc: 0.0637 - val_loss: 8.1860 - val_acc: 0.0132\n",
      "Epoch 17/100\n",
      "35746/35746 [==============================] - 43s 1ms/step - loss: 7.3865 - acc: 0.0636 - val_loss: 8.2200 - val_acc: 0.0133\n",
      "Epoch 18/100\n",
      "35746/35746 [==============================] - 42s 1ms/step - loss: 7.3575 - acc: 0.0636 - val_loss: 8.2042 - val_acc: 0.0130\n",
      "Epoch 19/100\n",
      "35746/35746 [==============================] - 43s 1ms/step - loss: 7.3304 - acc: 0.0633 - val_loss: 8.2295 - val_acc: 0.0132\n",
      "Epoch 20/100\n",
      "35746/35746 [==============================] - 43s 1ms/step - loss: 7.3045 - acc: 0.0633 - val_loss: 8.2162 - val_acc: 0.0130\n",
      "Epoch 21/100\n",
      "35746/35746 [==============================] - 42s 1ms/step - loss: 7.2801 - acc: 0.0633 - val_loss: 8.2096 - val_acc: 0.0129\n",
      "Epoch 22/100\n",
      "35746/35746 [==============================] - 45s 1ms/step - loss: 7.2632 - acc: 0.0630 - val_loss: 8.2866 - val_acc: 0.0130\n",
      "Epoch 23/100\n",
      "35746/35746 [==============================] - 44s 1ms/step - loss: 7.2434 - acc: 0.0630 - val_loss: 8.2582 - val_acc: 0.0129\n",
      "Epoch 24/100\n",
      "35746/35746 [==============================] - 42s 1ms/step - loss: 7.2268 - acc: 0.0627 - val_loss: 8.2635 - val_acc: 0.0129\n",
      "Epoch 25/100\n",
      "35746/35746 [==============================] - 44s 1ms/step - loss: 7.2079 - acc: 0.0628 - val_loss: 8.2739 - val_acc: 0.0129\n",
      "Epoch 26/100\n",
      "35746/35746 [==============================] - 42s 1ms/step - loss: 7.1945 - acc: 0.0627 - val_loss: 8.3471 - val_acc: 0.0132\n",
      "Epoch 27/100\n",
      "35746/35746 [==============================] - 42s 1ms/step - loss: 7.1786 - acc: 0.0623 - val_loss: 8.3000 - val_acc: 0.0130\n",
      "Epoch 28/100\n",
      "35746/35746 [==============================] - 41s 1ms/step - loss: 7.1637 - acc: 0.0624 - val_loss: 8.3188 - val_acc: 0.0129\n",
      "Epoch 29/100\n",
      "35746/35746 [==============================] - 45s 1ms/step - loss: 7.1514 - acc: 0.0622 - val_loss: 8.3451 - val_acc: 0.0127\n",
      "Epoch 30/100\n",
      "35746/35746 [==============================] - 46s 1ms/step - loss: 7.1385 - acc: 0.0627 - val_loss: 8.3116 - val_acc: 0.0128\n",
      "Epoch 31/100\n",
      "35746/35746 [==============================] - 43s 1ms/step - loss: 7.1239 - acc: 0.0623 - val_loss: 8.3644 - val_acc: 0.0130\n",
      "Epoch 32/100\n",
      "35746/35746 [==============================] - 44s 1ms/step - loss: 7.1125 - acc: 0.0626 - val_loss: 8.3556 - val_acc: 0.0129\n",
      "Epoch 33/100\n",
      "35746/35746 [==============================] - 40s 1ms/step - loss: 7.0989 - acc: 0.0624 - val_loss: 8.4002 - val_acc: 0.0135\n",
      "Epoch 34/100\n",
      "35746/35746 [==============================] - 44s 1ms/step - loss: 7.0868 - acc: 0.0623 - val_loss: 8.3587 - val_acc: 0.0130\n",
      "Epoch 35/100\n",
      "35746/35746 [==============================] - 42s 1ms/step - loss: 7.0773 - acc: 0.0621 - val_loss: 8.3952 - val_acc: 0.0132\n",
      "Epoch 36/100\n",
      "35746/35746 [==============================] - 42s 1ms/step - loss: 7.0656 - acc: 0.0624 - val_loss: 8.3818 - val_acc: 0.0132\n",
      "Epoch 37/100\n",
      "35746/35746 [==============================] - 45s 1ms/step - loss: 7.0519 - acc: 0.0624 - val_loss: 8.3924 - val_acc: 0.0127\n",
      "Epoch 38/100\n",
      "35746/35746 [==============================] - 44s 1ms/step - loss: 7.0391 - acc: 0.0621 - val_loss: 8.4008 - val_acc: 0.0130\n",
      "Epoch 39/100\n",
      "35746/35746 [==============================] - 43s 1ms/step - loss: 7.0288 - acc: 0.0625 - val_loss: 8.4350 - val_acc: 0.0133\n",
      "Epoch 40/100\n",
      "35746/35746 [==============================] - 42s 1ms/step - loss: 7.0168 - acc: 0.0622 - val_loss: 8.4684 - val_acc: 0.0129\n",
      "Epoch 41/100\n",
      "35746/35746 [==============================] - 44s 1ms/step - loss: 7.0033 - acc: 0.0623 - val_loss: 8.4690 - val_acc: 0.0132\n",
      "Epoch 42/100\n",
      "35746/35746 [==============================] - 43s 1ms/step - loss: 6.9913 - acc: 0.0621 - val_loss: 8.4663 - val_acc: 0.0132\n",
      "Epoch 43/100\n",
      "35746/35746 [==============================] - 41s 1ms/step - loss: 6.9798 - acc: 0.0624 - val_loss: 8.4667 - val_acc: 0.0130\n",
      "Epoch 44/100\n",
      "35746/35746 [==============================] - 43s 1ms/step - loss: 6.9683 - acc: 0.0622 - val_loss: 8.4648 - val_acc: 0.0132\n",
      "Epoch 45/100\n",
      "35746/35746 [==============================] - 40s 1ms/step - loss: 6.9558 - acc: 0.0624 - val_loss: 8.4736 - val_acc: 0.0132\n",
      "Epoch 46/100\n",
      "35746/35746 [==============================] - 41s 1ms/step - loss: 6.9472 - acc: 0.0619 - val_loss: 8.5069 - val_acc: 0.0128\n",
      "Epoch 47/100\n",
      "35746/35746 [==============================] - 41s 1ms/step - loss: 6.9356 - acc: 0.0624 - val_loss: 8.5147 - val_acc: 0.0130\n",
      "Epoch 48/100\n",
      "35746/35746 [==============================] - 45s 1ms/step - loss: 6.9257 - acc: 0.0618 - val_loss: 8.5477 - val_acc: 0.0134\n",
      "Epoch 49/100\n",
      "35746/35746 [==============================] - 43s 1ms/step - loss: 6.9102 - acc: 0.0616 - val_loss: 8.5674 - val_acc: 0.0132\n",
      "Epoch 50/100\n",
      "35746/35746 [==============================] - 43s 1ms/step - loss: 6.9015 - acc: 0.0620 - val_loss: 8.6050 - val_acc: 0.0127\n",
      "Epoch 51/100\n",
      "35746/35746 [==============================] - 41s 1ms/step - loss: 6.8923 - acc: 0.0622 - val_loss: 8.5841 - val_acc: 0.0133\n",
      "Epoch 52/100\n",
      "35746/35746 [==============================] - 40s 1ms/step - loss: 6.8764 - acc: 0.0617 - val_loss: 8.5649 - val_acc: 0.0133\n",
      "Epoch 53/100\n",
      "35746/35746 [==============================] - 38s 1ms/step - loss: 6.8661 - acc: 0.0617 - val_loss: 8.6113 - val_acc: 0.0130\n",
      "Epoch 54/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35746/35746 [==============================] - 38s 1ms/step - loss: 6.8537 - acc: 0.0620 - val_loss: 8.6073 - val_acc: 0.0127\n",
      "Epoch 55/100\n",
      "35746/35746 [==============================] - 38s 1ms/step - loss: 6.8447 - acc: 0.0618 - val_loss: 8.6016 - val_acc: 0.0130\n",
      "Epoch 56/100\n",
      "35746/35746 [==============================] - 39s 1ms/step - loss: 6.8344 - acc: 0.0622 - val_loss: 8.6461 - val_acc: 0.0130\n",
      "Epoch 57/100\n",
      "35746/35746 [==============================] - 39s 1ms/step - loss: 6.8240 - acc: 0.0613 - val_loss: 8.7037 - val_acc: 0.0129\n",
      "Epoch 58/100\n",
      "35746/35746 [==============================] - 42s 1ms/step - loss: 6.8182 - acc: 0.0616 - val_loss: 8.6915 - val_acc: 0.0138\n",
      "Epoch 59/100\n",
      "35746/35746 [==============================] - 39s 1ms/step - loss: 6.8099 - acc: 0.0623 - val_loss: 8.7143 - val_acc: 0.0127\n",
      "Epoch 60/100\n",
      "35746/35746 [==============================] - 41s 1ms/step - loss: 6.7961 - acc: 0.0619 - val_loss: 8.7100 - val_acc: 0.0129\n",
      "Epoch 61/100\n",
      "35746/35746 [==============================] - 38s 1ms/step - loss: 6.7878 - acc: 0.0621 - val_loss: 8.7205 - val_acc: 0.0132\n",
      "Epoch 62/100\n",
      "35746/35746 [==============================] - 38s 1ms/step - loss: 6.7810 - acc: 0.0617 - val_loss: 8.7330 - val_acc: 0.0129\n",
      "Epoch 63/100\n",
      "35746/35746 [==============================] - 37s 1ms/step - loss: 6.7719 - acc: 0.0619 - val_loss: 8.7450 - val_acc: 0.0128\n",
      "Epoch 64/100\n",
      "35746/35746 [==============================] - 37s 1ms/step - loss: 6.7625 - acc: 0.0622 - val_loss: 8.7517 - val_acc: 0.0137\n",
      "Epoch 65/100\n",
      "35746/35746 [==============================] - 38s 1ms/step - loss: 6.7585 - acc: 0.0613 - val_loss: 8.7178 - val_acc: 0.0130\n",
      "Epoch 66/100\n",
      "35746/35746 [==============================] - 39s 1ms/step - loss: 6.7531 - acc: 0.0621 - val_loss: 8.7660 - val_acc: 0.0134\n",
      "Epoch 67/100\n",
      "35746/35746 [==============================] - 39s 1ms/step - loss: 6.7424 - acc: 0.0615 - val_loss: 8.8556 - val_acc: 0.0132\n",
      "Epoch 68/100\n",
      "35746/35746 [==============================] - 40s 1ms/step - loss: 6.7370 - acc: 0.0614 - val_loss: 8.8297 - val_acc: 0.0133\n",
      "Epoch 69/100\n",
      "35746/35746 [==============================] - 38s 1ms/step - loss: 6.7312 - acc: 0.0620 - val_loss: 8.8532 - val_acc: 0.0132\n",
      "Epoch 70/100\n",
      "35746/35746 [==============================] - 39s 1ms/step - loss: 6.7230 - acc: 0.0621 - val_loss: 8.8311 - val_acc: 0.0137\n",
      "Epoch 71/100\n",
      "35746/35746 [==============================] - 36s 1ms/step - loss: 6.7182 - acc: 0.0623 - val_loss: 8.8313 - val_acc: 0.0132\n",
      "Epoch 72/100\n",
      "35746/35746 [==============================] - 36s 1ms/step - loss: 6.7165 - acc: 0.0616 - val_loss: 8.8615 - val_acc: 0.0136\n",
      "Epoch 73/100\n",
      "35746/35746 [==============================] - 38s 1ms/step - loss: 6.7036 - acc: 0.0621 - val_loss: 8.8536 - val_acc: 0.0130\n",
      "Epoch 74/100\n",
      "35746/35746 [==============================] - 38s 1ms/step - loss: 6.6989 - acc: 0.0621 - val_loss: 8.8841 - val_acc: 0.0135\n",
      "Epoch 75/100\n",
      "35746/35746 [==============================] - 36s 1ms/step - loss: 6.6902 - acc: 0.0629 - val_loss: 8.8355 - val_acc: 0.0138\n",
      "Epoch 76/100\n",
      "35746/35746 [==============================] - 37s 1ms/step - loss: 6.6930 - acc: 0.0623 - val_loss: 8.9182 - val_acc: 0.0135\n",
      "Epoch 77/100\n",
      "35746/35746 [==============================] - 37s 1ms/step - loss: 6.6846 - acc: 0.0627 - val_loss: 8.8987 - val_acc: 0.0137\n",
      "Epoch 78/100\n",
      "35746/35746 [==============================] - 37s 1ms/step - loss: 6.6783 - acc: 0.0624 - val_loss: 8.8677 - val_acc: 0.0136\n",
      "Epoch 79/100\n",
      "35746/35746 [==============================] - 36s 996us/step - loss: 6.6775 - acc: 0.0622 - val_loss: 8.9052 - val_acc: 0.0137\n",
      "Epoch 80/100\n",
      "35746/35746 [==============================] - 36s 1ms/step - loss: 6.6647 - acc: 0.0624 - val_loss: 8.9406 - val_acc: 0.0143\n",
      "Epoch 81/100\n",
      "35746/35746 [==============================] - 36s 1ms/step - loss: 6.6618 - acc: 0.0626 - val_loss: 9.0064 - val_acc: 0.0132\n",
      "Epoch 82/100\n",
      "35746/35746 [==============================] - 36s 1ms/step - loss: 6.6599 - acc: 0.0625 - val_loss: 8.9725 - val_acc: 0.0134\n",
      "Epoch 83/100\n",
      "35746/35746 [==============================] - 37s 1ms/step - loss: 6.6561 - acc: 0.0630 - val_loss: 8.9735 - val_acc: 0.0135\n",
      "Epoch 84/100\n",
      "35746/35746 [==============================] - 36s 1ms/step - loss: 6.6529 - acc: 0.0627 - val_loss: 9.0605 - val_acc: 0.0143\n",
      "Epoch 85/100\n",
      "35746/35746 [==============================] - 37s 1ms/step - loss: 6.6440 - acc: 0.0628 - val_loss: 8.9979 - val_acc: 0.0135\n",
      "Epoch 86/100\n",
      "35746/35746 [==============================] - 37s 1ms/step - loss: 6.6396 - acc: 0.0629 - val_loss: 9.0240 - val_acc: 0.0132\n",
      "Epoch 87/100\n",
      "35746/35746 [==============================] - 37s 1ms/step - loss: 6.6471 - acc: 0.0628 - val_loss: 9.0488 - val_acc: 0.0137\n",
      "Epoch 88/100\n",
      "35746/35746 [==============================] - 37s 1ms/step - loss: 6.6362 - acc: 0.0629 - val_loss: 9.0048 - val_acc: 0.0130\n",
      "Epoch 89/100\n",
      "35746/35746 [==============================] - 37s 1ms/step - loss: 6.6335 - acc: 0.0629 - val_loss: 9.0143 - val_acc: 0.0137\n",
      "Epoch 90/100\n",
      "35746/35746 [==============================] - 38s 1ms/step - loss: 6.6261 - acc: 0.0629 - val_loss: 9.0429 - val_acc: 0.0136\n",
      "Epoch 91/100\n",
      "35746/35746 [==============================] - 37s 1ms/step - loss: 6.6227 - acc: 0.0629 - val_loss: 9.1022 - val_acc: 0.0136\n",
      "Epoch 92/100\n",
      "35746/35746 [==============================] - 36s 1ms/step - loss: 6.6192 - acc: 0.0631 - val_loss: 9.0390 - val_acc: 0.0129\n",
      "Epoch 93/100\n",
      "35746/35746 [==============================] - 35s 992us/step - loss: 6.6164 - acc: 0.0626 - val_loss: 9.0594 - val_acc: 0.0137\n",
      "Epoch 94/100\n",
      "35746/35746 [==============================] - 37s 1ms/step - loss: 6.6107 - acc: 0.0634 - val_loss: 9.1051 - val_acc: 0.0134\n",
      "Epoch 95/100\n",
      "35746/35746 [==============================] - 37s 1ms/step - loss: 6.6097 - acc: 0.0626 - val_loss: 9.0907 - val_acc: 0.0134\n",
      "Epoch 96/100\n",
      "35746/35746 [==============================] - 37s 1ms/step - loss: 6.6087 - acc: 0.0624 - val_loss: 9.1142 - val_acc: 0.0142\n",
      "Epoch 97/100\n",
      "35746/35746 [==============================] - 36s 1ms/step - loss: 6.6022 - acc: 0.0633 - val_loss: 9.1317 - val_acc: 0.0136\n",
      "Epoch 98/100\n",
      "35746/35746 [==============================] - 40s 1ms/step - loss: 6.6030 - acc: 0.0626 - val_loss: 9.1016 - val_acc: 0.0132\n",
      "Epoch 99/100\n",
      "35746/35746 [==============================] - 37s 1ms/step - loss: 6.5962 - acc: 0.0628 - val_loss: 9.1096 - val_acc: 0.0134\n",
      "Epoch 100/100\n",
      "35746/35746 [==============================] - 38s 1ms/step - loss: 6.5912 - acc: 0.0626 - val_loss: 9.1060 - val_acc: 0.0133\n",
      "Loss     ======= Train: 6.445, Test: 9.106\n",
      "Accuracy ======= Train: 0.066, Test: 0.013\n",
      "*****\n",
      "['acc', 'loss', 'val_acc', 'val_loss']\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2(X_train_new, y_train, X_test_new, y_test,256,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model3(X_train_new, y_train, X_test_new, y_test,256,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model4(X_train_new, y_train, X_test_new, y_test,256,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model5(X_train_new, y_train, X_test_new, y_test,128,100,\"results/LSTM_model_test.json\",model_h5_file=\"results/LSTM_model_test.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "##model6(X_train_new, y_train, X_test_new, y_test,256,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, None, 418)         10868     \n",
      "_________________________________________________________________\n",
      "gru_2 (GRU)                  (None, 128)               210048    \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 31273)             4034217   \n",
      "=================================================================\n",
      "Total params: 4,255,133\n",
      "Trainable params: 4,255,133\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 35746 samples, validate on 9044 samples\n",
      "Epoch 1/100\n",
      "22784/35746 [==================>...........] - ETA: 2:17 - loss: 9.8336 - acc: 0.0630"
     ]
    }
   ],
   "source": [
    "model7(X_train_new, y_train, X_test_new, y_test,256,100,\"results/GRU_model_test.json\",model_h5_file=\"results/GRU_model_test.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model8(X_train_new, y_train, X_test_new, y_test,8,10,\"results/CNV_LSTM_XL_model_test.json\",model_h5_file=\"results/CNV_LSTM_XL_model_test.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model9(X_train_new, y_train, X_test_new, y_test,8,10,\"results/CNV_LSTM_XL_model_test.json\",model_h5_file=\"results/CNV_LSTM_XL_model_test.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, None, 418)         10868     \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, None, 75)          148200    \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 25)                10100     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 25)                0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 16)                416       \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 31273)             281457    \n",
      "=================================================================\n",
      "Total params: 451,177\n",
      "Trainable params: 451,177\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 35746 samples, validate on 9044 samples\n",
      "Epoch 1/50\n",
      "35746/35746 [==============================] - 362s 10ms/step - loss: 9.9986 - acc: 0.0626 - val_loss: 9.5899 - val_acc: 0.0134\n",
      "Epoch 2/50\n",
      "35746/35746 [==============================] - 362s 10ms/step - loss: 8.8883 - acc: 0.0639 - val_loss: 9.3806 - val_acc: 0.0134\n",
      "Epoch 3/50\n",
      "35746/35746 [==============================] - 368s 10ms/step - loss: 8.7584 - acc: 0.0639 - val_loss: 9.2910 - val_acc: 0.0134\n",
      "Epoch 4/50\n",
      "15872/35746 [============>.................] - ETA: 3:24 - loss: 8.7007 - acc: 0.0641"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-----------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-4fde0f8928b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel10\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_new\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test_new\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"results/LSTM_model_test.json\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel_h5_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"results/LSTM_model_test.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-19-a517e28cebf8>\u001b[0m in \u001b[0;36mmodel10\u001b[0;34m(X_train_new, y_train, X_test_new, y_test, in_batch_size, in_epochs, model_json_file, model_h5_file)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;31m# fit the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_new\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_one_hot_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0min_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0min_batch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_new\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_one_hot_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;31m# evaluate the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/training_arrays.pyc\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1397\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1398\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1399\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1400\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1401\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model10(X_train_new, y_train, X_test_new, y_test,256,50,\"results/LSTM_model_test.json\",model_h5_file=\"results/LSTM_model_test.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
