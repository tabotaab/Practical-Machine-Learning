{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import GRU\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers import GlobalAveragePooling1D\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import to_categorical\n",
    "from keras import optimizers\n",
    "\n",
    "from Bio import SeqIO\n",
    "from Bio.Data import IUPACData \n",
    "import csv\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import dask.dataframe as dd\n",
    "import os.path\n",
    "\n",
    "#data_path = 'features_CENH3_DMR6_LUCA-CHLRE00002_orthologues.csv'\n",
    "data_path = 'features_oma-seqs-viridiplantae_test-3-4-5-6-7-8-9-10-11.csv'\n",
    "data_single_cluster_path = '/mnt/WORKSPACE/smo_workspace/moduls/ML_orthology_RNN/features_oma-seqs-viridiplantae_test-1.csv'\n",
    "#data_path = 'features_oma-seqs-viridiplantae_test-9-10-11.csv'\n",
    "#data_path = 'features_oma-seqs-viridiplantae_test-11.csv'\n",
    "#data_single_cluster_path = 'no_single_cluster.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def protein2integer(in_seq):\n",
    "    \n",
    "    ## define universe of possible input values\n",
    "    all_protein_letters = list(IUPACData.extended_protein_letters)\n",
    "    #print(all_protein_letters)\n",
    "    ## define a mapping of chars to integers \n",
    "    ## i+1 beacuse we want to start from integer 1 instead of 0. 0 will be used for padding\n",
    "    char_to_int = dict((c, i+1) for i, c in enumerate(all_protein_letters))\n",
    "    int_to_char = dict((i+1, c) for i, c in enumerate(all_protein_letters))\n",
    "    ## integer encode input data\n",
    "    integer_encoded = [char_to_int[char] for char in in_seq.upper()]\n",
    "    \n",
    "    \n",
    "    #return(integer_encoded,len(all_protein_letters))\n",
    "    return(integer_encoded)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(in_file):\n",
    "    with open(in_file, 'r') as f:\n",
    "        reader = csv.reader(f, delimiter=\"\\t\")\n",
    "        # get all the rows as a list\n",
    "        d_set = list(reader)\n",
    "        # transform data into numpy array\n",
    "        d_set = np.array(d_set).astype(str)\n",
    "        \n",
    "    integer_encoded_proteins = np.array([protein2integer(seq) for seq in d_set[:,1]])\n",
    "    \n",
    "    G = d_set[:, 0]\n",
    "    X = integer_encoded_proteins\n",
    "    Y = d_set[:, 2].astype(int)\n",
    "                         \n",
    "    return(d_set,G,X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset_single(in_file,y_value=100000):\n",
    "    with open(in_file, 'r') as f:\n",
    "        reader = csv.reader(f, delimiter=\"\\t\")\n",
    "        # get all the rows as a list\n",
    "        d_set = list(reader)\n",
    "        # transform data into numpy array\n",
    "        d_set = np.array(d_set).astype(str)\n",
    "        \n",
    "    integer_encoded_proteins = np.array([protein2integer(seq) for seq in d_set[:,1]])\n",
    "    \n",
    "    G = d_set[:, 0]\n",
    "    X = integer_encoded_proteins\n",
    "    Y = np.full((len(d_set[:, 2]),1), int(y_value))[:,0]      \n",
    "                         \n",
    "    return(d_set,G,X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset_dask(in_file):\n",
    "    data = dd.read_csv(in_file,sep='\\t', header=None)\n",
    "    df = data.compute().reset_index(drop=True)\n",
    "    integer_encoded_proteins = da.from_array([protein2integer(seq) for seq in df.values[:,1]],chunks=1000)\n",
    "    G = df.values[:,0]\n",
    "    X = integer_encoded_proteins.compute()\n",
    "    Y = df.values[:,2].astype(int)\n",
    "                     \n",
    "    return(df,G,X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_test_set_idea1(G,X,Y):\n",
    "    \n",
    "    # here we keep 80% of random indexes in train set and the rest in test set\n",
    "    \n",
    "    indices = np.random.permutation(X.shape[0])\n",
    "    train_size = int(indices.size*0.95)\n",
    "    train_idx, test_idx = indices[:train_size], indices[train_size:]\n",
    "    #print(len(train_idx),len(test_idx))\n",
    "    \n",
    "    X_train, X_test = X[train_idx,], X[test_idx,]\n",
    "    #print(X_train.shape,X_test.shape)\n",
    "    \n",
    "    y_train, y_test = Y[train_idx,], Y[test_idx,]\n",
    "    \n",
    "    #print(X[train_idx[0],])\n",
    "    #print(Y[train_idx[0],])\n",
    "\n",
    "    #print(X_train[0,])\n",
    "    #print(y_train[0,])\n",
    "    \n",
    "    return(X_train,y_train,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_test_set_idea2(G,X,Y):\n",
    "    \n",
    "    # here we try to keep one item from each cluster in test set\n",
    "    # the rest goes to train set\n",
    "    \n",
    "    test_idx = []\n",
    "    train_idx = []\n",
    "    seen_cluster_id = []\n",
    "    \n",
    "    for i in range(0,Y.shape[0]):\n",
    "            if Y[i] in seen_cluster_id :\n",
    "                train_idx.append(i)\n",
    "            else:\n",
    "                test_idx.append(i)\n",
    "                seen_cluster_id.append(Y[i])\n",
    "                \n",
    "    #print(len(train_idx),len(test_idx))\n",
    "    \n",
    "    X_train, X_test = X[train_idx,], X[test_idx,]\n",
    "    #print(X_train.shape,X_test.shape)\n",
    "    \n",
    "    y_train, y_test = Y[train_idx,], Y[test_idx,]\n",
    "    \n",
    "    #print(X[train_idx[0],])\n",
    "    #print(Y[train_idx[0],])\n",
    "\n",
    "    #print(X_train[0,])\n",
    "    #print(y_train[0,])\n",
    "    \n",
    "    return(X_train,y_train,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model1(X_train_new, y_train,X_test_new, y_test,in_batch_size=100,in_epochs=10,model_json_file=\"model.json\",model_h5_file=\"model.h5\"): # RNN: Recurrent Neural Networks\n",
    "    # https://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/\n",
    "    # create the model\n",
    "    embedding_vecor_length = 4\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(num_letters, embedding_vecor_length, input_length=fixed_seq_length))\n",
    "    model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(LSTM(75))\n",
    "    model.add(Dense(n_classes, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy',optimizer='rmsprop', metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "    \n",
    "    # Convert labels to categorical one-hot encoding & fit the model\n",
    "    y_train_one_hot_labels = to_categorical(y_train, num_classes=n_classes)\n",
    "    model.fit(X_train_new, y_train_one_hot_labels, epochs=in_epochs, batch_size=in_batch_size)\n",
    "\n",
    "    # evaluate the model\n",
    "    y_test_one_hot_labels = to_categorical(y_test, num_classes=n_classes)\n",
    "    loss, accuracy = model.evaluate(X_test_new, y_test_one_hot_labels, verbose=0)\n",
    "    print('Accuracy: %f' % (accuracy*100))\n",
    "\n",
    "    # serialize model to JSON\n",
    "    model_json = model.to_json()\n",
    "    with open(model_json_file, \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    # serialize weights to HDF5\n",
    "    model.save_weights(model_h5_file)\n",
    "    print(\"Saved model to disk\")\n",
    "    \n",
    "    return()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model2(X_train_new, y_train,X_test_new, y_test,in_batch_size=100,in_epochs=10): # RNN: Recurrent Neural Networks\n",
    "    # Initializing the Sequential model from KERAS.\n",
    "    model = Sequential()\n",
    "\n",
    "    # Creating a 16 neuron hidden layer with Linear Rectified activation function.\n",
    "    #model.add(Dense(16, input_dim=1, init='uniform', activation='relu'))\n",
    "    model.add(Dense(16, input_dim=fixed_seq_length, kernel_initializer='uniform', activation='relu'))\n",
    "\n",
    "    # Creating a 8 neuron hidden layer.\n",
    "    model.add(Dense(8, kernel_initializer='uniform', activation='relu'))\n",
    "\n",
    "    # Adding a output layer.\n",
    "    model.add(Dense(n_classes, kernel_initializer='uniform', activation='softmax'))\n",
    "    \n",
    "    # Compiling the model\n",
    "    model.compile(loss='categorical_crossentropy',optimizer='rmsprop', metrics=['accuracy'])\n",
    "\n",
    "    print(model.summary())\n",
    "   \n",
    "    # Convert labels to categorical one-hot encoding & fit the model\n",
    "    y_train_one_hot_labels = to_categorical(y_train, num_classes=n_classes)\n",
    "    model.fit(X_train_new, y_train_one_hot_labels, epochs=in_epochs, batch_size=in_batch_size)\n",
    "\n",
    "    # fit the model\n",
    "#    y_train_one_hot_labels = to_categorical(y_train, num_classes=n_classes)\n",
    "#    model.fit(X_train_new, y_train_one_hot_labels, epochs=in_epochs, verbose=1)\n",
    "\n",
    "    # evaluate the model\n",
    "    y_test_one_hot_labels = to_categorical(y_test, num_classes=n_classes)\n",
    "    loss, accuracy = model.evaluate(X_test_new, y_test_one_hot_labels, verbose=0)\n",
    "    print('Accuracy: %f' % (accuracy*100))\n",
    "    \n",
    "    return()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_matrix(labels,C):\n",
    "    \n",
    "    C = tf.constant(C,name=\"C\")\n",
    "    one_hot_matrix = tf.one_hot(labels,C,axis=1)\n",
    "    sess = tf.Session()\n",
    "    one_hot = sess.run(one_hot_matrix)\n",
    "    sess.close()\n",
    "    \n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model3(X_train_new, y_train,X_test_new, y_test, batch_size =100, hm_epochs =100): # CNN: Convolutional Neural Networks\n",
    "    # Number of nodes in each NN hidden layer\n",
    "    n_nodes_hl1 = 1500\n",
    "    n_nodes_hl2 = 1500\n",
    "    n_nodes_hl3 = 1500\n",
    "\n",
    "    # Number of orthology clusters\n",
    "    #n_classes = len(np.unique(np.concatenate((y_train,y_test),axis=0)))     #2 or 3 or ...\n",
    "    \n",
    "    #y_all = np.concatenate((y_train,y_test),axis=0)\n",
    "    #y_min = np.amin(y_all)\n",
    "    #n_classes = np.amax(y_all-y_min)+1\n",
    "    \n",
    "    \n",
    "    train_y = one_hot_matrix(y_train,n_classes)\n",
    "    test_y = one_hot_matrix(y_test,n_classes)\n",
    "\n",
    "    # Batch size and Epoch size for training the NN\n",
    "    #batch_size = 100   #100\n",
    "    #hm_epochs = 100    #1000\n",
    "\n",
    "    # Initializing X and Y\n",
    "    x = tf.placeholder('float')\n",
    "    y = tf.placeholder('float')\n",
    "\n",
    "    # Initializing NN layers\n",
    "    hidden_1_layer = {'f_fum':n_nodes_hl1,\n",
    "                  'weight':tf.Variable(tf.random_normal([len(X_train_new[0]), n_nodes_hl1])),\n",
    "                  'bias':tf.Variable(tf.random_normal([n_nodes_hl1]))}\n",
    "\n",
    "    hidden_2_layer = {'f_fum':n_nodes_hl2,\n",
    "                  'weight':tf.Variable(tf.random_normal([n_nodes_hl1, n_nodes_hl2])),\n",
    "                  'bias':tf.Variable(tf.random_normal([n_nodes_hl2]))}\n",
    "\n",
    "    hidden_3_layer = {'f_fum':n_nodes_hl3,\n",
    "                  'weight':tf.Variable(tf.random_normal([n_nodes_hl2, n_nodes_hl3])),\n",
    "                  'bias':tf.Variable(tf.random_normal([n_nodes_hl3]))}\n",
    "\n",
    "    output_layer = {'f_fum':None,\n",
    "                'weight':tf.Variable(tf.random_normal([n_nodes_hl3, n_classes])),\n",
    "                'bias':tf.Variable(tf.random_normal([n_classes])),}\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    l1 = tf.add(tf.matmul(x,hidden_1_layer['weight']), hidden_1_layer['bias'])\n",
    "    l1 = tf.nn.relu(l1)\n",
    "\n",
    "    l2 = tf.add(tf.matmul(l1,hidden_2_layer['weight']), hidden_2_layer['bias'])\n",
    "    l2 = tf.nn.relu(l2)\n",
    "\n",
    "    l3 = tf.add(tf.matmul(l2,hidden_3_layer['weight']), hidden_3_layer['bias'])\n",
    "    l3 = tf.nn.relu(l3)\n",
    "\n",
    "    prediction = tf.matmul(l3,output_layer['weight']) + output_layer['bias']\n",
    "\n",
    "        \n",
    "    cost = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits_v2(logits=prediction,labels=y) )\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=0.001).minimize(cost)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        #try:\n",
    "        #    epoch = int(open(tf_log,'r').read().split('\\n')[-2])+1\n",
    "        #    print('STARTING:',epoch)\n",
    "        #except:\n",
    "        #    epoch = 1\n",
    "        epoch = 1\n",
    "\n",
    "        while epoch <= hm_epochs:\n",
    "            epoch_loss = 1\n",
    "            \n",
    "            i=0\n",
    "            while i < len(X_train_new):\n",
    "                start = i\n",
    "                end = i+batch_size\n",
    "                batch_x = np.array(X_train_new[start:end])\n",
    "                batch_y = np.array(train_y[start:end])\n",
    "\n",
    "                _, c = sess.run([optimizer, cost], feed_dict={x: batch_x,y: batch_y})\n",
    "                epoch_loss += c\n",
    "                i+=batch_size\n",
    "                \n",
    "            \n",
    "            print('Epoch ',epoch,' out of ',hm_epochs,'- loss:',epoch_loss)\n",
    " \n",
    "            \n",
    "            #with open(tf_log,'a') as f:\n",
    "            #    f.write(str(epoch)+'\\n') \n",
    "            epoch +=1\n",
    "            \n",
    "        correct = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct, 'float'))\n",
    "\n",
    "        #print(\"\\nModel saved in path: %s \" % my_model_save_path)\n",
    "        print('\\nAccuracy:',accuracy.eval({x:X_test_new, y:test_y}) * 100)\n",
    "    return()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model4(X_train_new, y_train,X_test_new, y_test,in_batch_size=100,in_epochs=10): \n",
    "    # RNN: Recurrent Neural Networks\n",
    "    \n",
    "    # Convert labels to categorical one-hot encoding  \n",
    "    y_train_one_hot_labels = to_categorical(y_train, num_classes=n_classes)\n",
    "    y_test_one_hot_labels = to_categorical(y_test, num_classes=n_classes)\n",
    "\n",
    "    # create the model    \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(num_letters, output_dim=fixed_seq_length))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv1D(filters=64, kernel_size=5, padding='same', activation='relu'))\n",
    "    model.add(MaxPooling1D(4))\n",
    "    model.add(LSTM(70))\n",
    "    model.add(Dense(n_classes, activation='softmax'))\n",
    "    \n",
    "    #opt = 'rmsprop'\n",
    "    #opt = optimizers.SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False)\n",
    "    opt = optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=0.001, decay=0.0)\n",
    "    #opt = optimizers.Adagrad(lr=0.01, epsilon=None, decay=0.0)\n",
    "    #opt = optimizers.Adadelta(lr=1.0, rho=0.95, epsilon=None, decay=0.0)\n",
    "    #opt = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=0.000001, decay=0.0, amsgrad=False)\n",
    "    #opt = optimizers.Adamax(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0)\n",
    "    #opt = optimizers.Nadam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, schedule_decay=0.004)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',optimizer=opt, metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "\n",
    "    # fit the model\n",
    "    model.fit(X_train_new, y_train_one_hot_labels, epochs=in_epochs, batch_size=in_batch_size)\n",
    "\n",
    "    # evaluate the model\n",
    "    loss, accuracy = model.evaluate(X_test_new, y_test_one_hot_labels, verbose=0)\n",
    "    print('Accuracy: %f' % (accuracy*100))\n",
    "    \n",
    "  \n",
    "    return()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model5(X_train_new, y_train,X_test_new, y_test,in_batch_size=100,in_epochs=10,model_json_file=\"model.json\",model_h5_file=\"model.h5\"): # RNN: Recurrent Neural Networks\n",
    "    # LSTM\n",
    "    \n",
    "    # Convert labels to categorical one-hot encoding  \n",
    "    y_train_one_hot_labels = to_categorical(y_train, num_classes=n_classes)\n",
    "    y_test_one_hot_labels = to_categorical(y_test, num_classes=n_classes)\n",
    "\n",
    "    # create the model    \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(num_letters, output_dim=fixed_seq_length))\n",
    "    model.add(LSTM(500,return_sequences=True))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(LSTM(250,return_sequences=True))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(LSTM(125,return_sequences=True))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(LSTM(75,return_sequences=True))\n",
    "    model.add(LSTM(25))\n",
    "    #model.add(Dropout(0.25))\n",
    "    model.add(Dense(n_classes, activation='softmax'))\n",
    "    \n",
    "    #opt = 'rmsprop'\n",
    "    #opt = optimizers.SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False)\n",
    "    #opt = optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=0.001, decay=0.0)\n",
    "    #opt = optimizers.Adagrad(lr=0.01, epsilon=None, decay=0.0)\n",
    "    #opt = optimizers.Adadelta(lr=1.0, rho=0.95, epsilon=None, decay=0.0)\n",
    "    opt = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "    #opt = optimizers.Adamax(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0)\n",
    "    #opt = optimizers.Nadam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, schedule_decay=0.004)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',optimizer=opt, metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "\n",
    "    # fit the model\n",
    "    model.fit(X_train_new, y_train_one_hot_labels, epochs=in_epochs, batch_size=in_batch_size)\n",
    "\n",
    "    # evaluate the model\n",
    "    loss, accuracy = model.evaluate(X_test_new, y_test_one_hot_labels, verbose=0)\n",
    "    print('Accuracy: %f' % (accuracy*100))\n",
    "    \n",
    "    # serialize model to JSON\n",
    "    model_json = model.to_json()\n",
    "    with open(model_json_file, \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    # serialize weights to HDF5\n",
    "    model.save_weights(model_h5_file)\n",
    "    print(\"Saved model to disk\")\n",
    "\n",
    "    return()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model6(X_train_new, y_train,X_test_new, y_test,in_batch_size=100,in_epochs=10): # RNN: Recurrent Neural Networks\n",
    "    # GRU\n",
    "    \n",
    "    # Convert labels to categorical one-hot encoding  \n",
    "    y_train_one_hot_labels = to_categorical(y_train, num_classes=n_classes)\n",
    "    y_test_one_hot_labels = to_categorical(y_test, num_classes=n_classes)\n",
    "    \n",
    "    # create the model    \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(num_letters, output_dim=fixed_seq_length))\n",
    "    model.add(GRU(128, return_sequences=False))\n",
    "    model.add(Dense(n_classes, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "\n",
    "    # fit the model\n",
    "    model.fit(X_train_new, y_train_one_hot_labels, epochs=in_epochs, batch_size=in_batch_size)\n",
    "\n",
    "    # evaluate the model\n",
    "    loss, accuracy = model.evaluate(X_test_new, y_test_one_hot_labels, verbose=0)\n",
    "    print('Accuracy: %f' % (accuracy*100))\n",
    "    \n",
    "    return()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def use_model(model_json_file=\"model.json\",model_h5_file=\"model.h5\"):\n",
    "    # load json and create model\n",
    "    json_file = open(model_json_file, 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    loaded_model = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    loaded_model.load_weights(model_h5_file)\n",
    "    print(\"Loaded model from disk\")\n",
    " \n",
    "    # evaluate loaded model on test data\n",
    "    loaded_model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "    score = loaded_model.evaluate(X, Y, verbose=0)\n",
    "    print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))\n",
    "    \n",
    "    return()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model7(X_train_new, y_train,X_test_new, y_test,in_batch_size=100,in_epochs=10,model_json_file=\"model.json\",model_h5_file=\"model.h5\"): # RNN: Recurrent Neural Networks\n",
    "    # GRU\n",
    "    \n",
    "    # Convert labels to categorical one-hot encoding  \n",
    "    y_train_one_hot_labels = to_categorical(y_train, num_classes=n_classes)\n",
    "    y_test_one_hot_labels = to_categorical(y_test, num_classes=n_classes)\n",
    "    \n",
    "    # create the model    \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(num_letters, output_dim=fixed_seq_length))\n",
    "    model.add(GRU(128, return_sequences=False))\n",
    "    model.add(Dense(n_classes, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "\n",
    "    # fit the model\n",
    "    model.fit(X_train_new, y_train_one_hot_labels, epochs=in_epochs, batch_size=in_batch_size)\n",
    "\n",
    "    # evaluate the model\n",
    "    #loss, accuracy = model.evaluate(X_test_new, y_test_one_hot_labels, verbose=0)\n",
    "    #print('Accuracy: %f' % (accuracy*100))\n",
    "    scores = model.evaluate(X_test_new, y_test_one_hot_labels, verbose=0)\n",
    "    print(\"%s: %.2f%%\" % (model.metrics_names[0], scores[0])) # Loss\n",
    "    print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100)) # Accuracy\n",
    " \n",
    "    # serialize model to JSON\n",
    "    model_json = model.to_json()\n",
    "    with open(model_json_file, \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    # serialize weights to HDF5\n",
    "    model.save_weights(model_h5_file)\n",
    "    print(\"Saved model to disk\")\n",
    "    \n",
    "    return()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model8(X_train_new, y_train,X_test_new, y_test,in_batch_size=100,in_epochs=10,model_json_file=\"model.json\",model_h5_file=\"model.h5\"): # RNN: Recurrent Neural Networks\n",
    "    # https://keras.io/getting-started/sequential-model-guide/\n",
    "    # create the model\n",
    "    embedding_vecor_length = 4\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(num_letters, embedding_vecor_length, input_length=fixed_seq_length))\n",
    "    model.add(Conv1D(filters=64, kernel_size=3, padding='same', activation='relu'))\n",
    "    model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
    "    model.add(MaxPooling1D(3))\n",
    "    model.add(Conv1D(filters=128, kernel_size=3, activation='relu'))\n",
    "    model.add(Conv1D(filters=128, kernel_size=3, activation='relu'))\n",
    "    model.add(MaxPooling1D(3))\n",
    "    model.add(Conv1D(filters=256, kernel_size=3, activation='relu'))\n",
    "    model.add(Conv1D(filters=256, kernel_size=3, activation='relu'))\n",
    "    model.add(MaxPooling1D(3))\n",
    "    model.add(Conv1D(filters=512, kernel_size=3, activation='relu'))\n",
    "    model.add(Conv1D(filters=512, kernel_size=3, activation='relu'))\n",
    "    #model.add(GlobalAveragePooling1D())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(4096, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(4096, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    #model.add(GRU(128, return_sequences=True))\n",
    "    model.add(LSTM(128,return_sequences=True))\n",
    "    model.add(LSTM(256,return_sequences=False))\n",
    "        \n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(n_classes, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy',optimizer='rmsprop', metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "    \n",
    "    # Convert labels to categorical one-hot encoding & fit the model\n",
    "    y_train_one_hot_labels = to_categorical(y_train, num_classes=n_classes)\n",
    "    model.fit(X_train_new, y_train_one_hot_labels, epochs=in_epochs, batch_size=in_batch_size)\n",
    "\n",
    "    # evaluate the model\n",
    "    y_test_one_hot_labels = to_categorical(y_test, num_classes=n_classes)\n",
    "    loss, accuracy = model.evaluate(X_test_new, y_test_one_hot_labels, verbose=0)\n",
    "    print('Accuracy: %f' % (accuracy*100))\n",
    "\n",
    "    # serialize model to JSON\n",
    "    model_json = model.to_json()\n",
    "    with open(model_json_file, \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    # serialize weights to HDF5\n",
    "    model.save_weights(model_h5_file)\n",
    "    print(\"Saved model to disk\")\n",
    "    \n",
    "    return()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model9(X_train_new, y_train,X_test_new, y_test,in_batch_size=100,in_epochs=10,model_json_file=\"model.json\",model_h5_file=\"model.h5\"): # RNN: Recurrent Neural Networks\n",
    "    # https://keras.io/getting-started/sequential-model-guide/\n",
    "    # create the model\n",
    "    embedding_vecor_length = 4\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(num_letters, embedding_vecor_length, input_length=fixed_seq_length))\n",
    "    model.add(TimeDistributed(Conv1D(filters=64, kernel_size=3, padding='same', activation='relu')))\n",
    "    model.add(TimeDistributed(Conv1D(filters=64, kernel_size=3, activation='relu')))\n",
    "    model.add(TimeDistributed(MaxPooling1D(3)))\n",
    "    model.add(TimeDistributed(Conv1D(filters=128, kernel_size=3, activation='relu')))\n",
    "    model.add(TimeDistributed(Conv1D(filters=128, kernel_size=3, activation='relu')))\n",
    "    model.add(TimeDistributed(MaxPooling1D(3)))\n",
    "    model.add(TimeDistributed(Conv1D(filters=256, kernel_size=3, activation='relu')))\n",
    "    model.add(TimeDistributed(Conv1D(filters=256, kernel_size=3, activation='relu')))\n",
    "    model.add(TimeDistributed(MaxPooling1D(3)))\n",
    "    model.add(TimeDistributed(Conv1D(filters=512, kernel_size=3, activation='relu')))\n",
    "    model.add(TimeDistributed(Conv1D(filters=512, kernel_size=3, activation='relu')))\n",
    "    #model.add(GlobalAveragePooling1D())\n",
    "    model.add(TimeDistributed(Dropout(0.5)))\n",
    "    model.add(TimeDistributed(Dense(4096, activation='relu')))\n",
    "    model.add(TimeDistributed(Dropout(0.5)))\n",
    "    model.add(TimeDistributed(Dense(4096, activation='relu')))\n",
    "    model.add(TimeDistributed(Dropout(0.5)))\n",
    "    \n",
    "    model.add(TimeDistributed(MaxPooling1D(pool_size=2)))\n",
    "    #model.add(GRU(128, return_sequences=True))\n",
    "    model.add(LSTM(128,return_sequences=True))\n",
    "    model.add(LSTM(256,return_sequences=False))\n",
    "        \n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(n_classes, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy',optimizer='rmsprop', metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "    \n",
    "    # Convert labels to categorical one-hot encoding & fit the model\n",
    "    y_train_one_hot_labels = to_categorical(y_train, num_classes=n_classes)\n",
    "    model.fit(X_train_new, y_train_one_hot_labels, epochs=in_epochs, batch_size=in_batch_size)\n",
    "\n",
    "    # evaluate the model\n",
    "    y_test_one_hot_labels = to_categorical(y_test, num_classes=n_classes)\n",
    "    loss, accuracy = model.evaluate(X_test_new, y_test_one_hot_labels, verbose=0)\n",
    "    print('Accuracy: %f' % (accuracy*100))\n",
    "\n",
    "    # serialize model to JSON\n",
    "    model_json = model.to_json()\n",
    "    with open(model_json_file, \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    # serialize weights to HDF5\n",
    "    model.save_weights(model_h5_file)\n",
    "    print(\"Saved model to disk\")\n",
    "    \n",
    "    return()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_1(d_set):\n",
    "    print(d_set[:,0])\n",
    "    print(d_set[0:2,1])\n",
    "    print(d_set[:,2].astype(int))\n",
    "    print(d_set.shape)\n",
    "    print(d_set[:,1].shape)\n",
    "    return()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_2(d_set):\n",
    "    integer_encoded_proteins = np.array([protein2integer(seq) for seq in d_set[:,1]])\n",
    "    print(len(integer_encoded_proteins))\n",
    "    print(integer_encoded_proteins[0])\n",
    "    #np.array(integer_encoded_proteins).shape\n",
    "    print(integer_encoded_proteins.shape)\n",
    "    #protein2integer(dataset[:,1])\n",
    "    return()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_3(G,X,Y):\n",
    "    print(G.shape)\n",
    "    print(X.shape)\n",
    "    print(Y.shape)\n",
    "\n",
    "    print(G[0:3,])\n",
    "    print(X[0:3,])\n",
    "    print(Y[0:3,])\n",
    "    \n",
    "    print('Maximum sequence length: {}'.format(len(max(X, key=len))))\n",
    "    print('Minimum sequence length: {}'.format(len(min(X, key=len))))\n",
    "    \n",
    "    return()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_4(X_train_new,X_train):\n",
    "    print(X_train_new.shape)\n",
    "    print(X_train_new[0,:])\n",
    "    print(X_train.shape)\n",
    "    print(X_train[0,])\n",
    "    return()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, G, X, Y = make_dataset(data_path)\n",
    "X_train,y_train,X_test,y_test = make_train_test_set_idea2(G,X,Y)\n",
    "\n",
    "if (os.path.exists(data_single_cluster_path)):\n",
    "    n_classes = int(np.amax(np.concatenate((y_train,y_test),axis=0))+1)\n",
    "    make_dataset_single(data_single_cluster_path,n_classes+1)\n",
    "    dataset, G, X, Y = make_dataset_single(data_single_cluster_path,n_classes+1)\n",
    "    X_train_s,y_train_s,X_test_s,y_test_s = make_train_test_set_idea1(G,X,Y)\n",
    "\n",
    "    X_train = np.concatenate((X_train,X_train_s),axis=0)\n",
    "    X_test = np.concatenate((X_test,X_test_s),axis=0)\n",
    "    y_train = np.concatenate((y_train,y_train_s),axis=0)\n",
    "    y_test = np.concatenate((y_test,y_test_s),axis=0)\n",
    "\n",
    "#print(\"============ Test 1 =======================\")\n",
    "#test_1(dataset)\n",
    "#print(\"============ Test 2 =======================\")\n",
    "#test_2(dataset)\n",
    "#print(\"============ Test 3 =======================\")\n",
    "#test_3(G,X,Y)\n",
    "#print(\"============ Test 4 =======================\")\n",
    "#test_4(X_train,X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "418\n"
     ]
    }
   ],
   "source": [
    "num_letters = len(list(IUPACData.extended_protein_letters)) # = 26\n",
    "#fixed_seq_length = len(max(X, key=len)) # maximum\n",
    "fixed_seq_length = (sum(len(X[i,]) for i in range(X.shape[0]))/X.shape[0])  # average ~490\n",
    "#fixed_seq_length = 1000\n",
    "n_classes = int(np.amax(np.concatenate((y_train,y_test),axis=0))+1)\n",
    "# truncate and pad input sequences\n",
    "X_train_new = sequence.pad_sequences(X_train, maxlen=fixed_seq_length, padding='post', truncating='post')\n",
    "X_test_new = sequence.pad_sequences(X_test, maxlen=fixed_seq_length, padding='post', truncating='post')\n",
    "  \n",
    "#print(\"============ Test 4 =======================\")\n",
    "#test_4(X_train_new,X_train)\n",
    "\n",
    "#all_length = []\n",
    "#for i in range(X.shape[0]):\n",
    "#    all_length.append(len(X[i,]))\n",
    "#fixed_seq_length = np.median(all_length)  # median ~394\n",
    "\n",
    "print(fixed_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model1(X_train_new, y_train, X_test_new, y_test,256,10,\"results/CNV_LSTM_model_test.json\",model_h5_file=\"results/CNV_LSTM_model_test.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model2(X_train_new, y_train, X_test_new, y_test,256,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model3(X_train_new, y_train, X_test_new, y_test,256,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model4(X_train_new, y_train, X_test_new, y_test,256,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model5(X_train_new, y_train, X_test_new, y_test,128,100,\"results/LSTM_model_test.json\",model_h5_file=\"results/LSTM_model_test.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "##model6(X_train_new, y_train, X_test_new, y_test,256,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model7(X_train_new, y_train, X_test_new, y_test,256,10,\"results/GRU_model_test.json\",model_h5_file=\"results/GRU_model_test.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model8(X_train_new, y_train, X_test_new, y_test,8,10,\"results/CNV_LSTM_XL_model_test.json\",model_h5_file=\"results/CNV_LSTM_XL_model_test.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-eb0c4b5be3e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel9\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_new\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test_new\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"results/CNV_LSTM_XL_model_test.json\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel_h5_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"results/CNV_LSTM_XL_model_test.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-18-db96a0d6fdda>\u001b[0m in \u001b[0;36mmodel9\u001b[0;34m(X_train_new, y_train, X_test_new, y_test, in_batch_size, in_epochs, model_json_file, model_h5_file)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_letters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_vecor_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfixed_seq_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTimeDistributed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mConv1D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'same'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTimeDistributed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mConv1D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTimeDistributed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMaxPooling1D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/sequential.pyc\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, layer)\u001b[0m\n\u001b[1;32m    179\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_source_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m             \u001b[0moutput_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m                 raise TypeError('All layers in a Sequential model '\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/base_layer.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    455\u001b[0m             \u001b[0;31m# Actually call the layer,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m             \u001b[0;31m# collecting output(s), mask(s), and shape(s).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m             \u001b[0moutput_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprevious_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/layers/wrappers.pyc\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    246\u001b[0m                 \u001b[0minner_mask_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_shape_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mask'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minner_mask_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_uses_learning_phase'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m                 \u001b[0muses_learning_phase\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_uses_learning_phase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/layers/convolutional.pyc\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    161\u001b[0m                 \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m                 \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m                 dilation_rate=self.dilation_rate[0])\n\u001b[0m\u001b[1;32m    164\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrank\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m             outputs = K.conv2d(\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36mconv1d\u001b[0;34m(x, kernel, strides, padding, data_format, dilation_rate)\u001b[0m\n\u001b[1;32m   3609\u001b[0m         \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstrides\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3610\u001b[0m         \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3611\u001b[0;31m         data_format=tf_data_format)\n\u001b[0m\u001b[1;32m   3612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3613\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdata_format\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'channels_first'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtf_data_format\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'NWC'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/nn_ops.pyc\u001b[0m in \u001b[0;36mconvolution\u001b[0;34m(input, filter, padding, strides, dilation_rate, name, data_format)\u001b[0m\n\u001b[1;32m    778\u001b[0m         \u001b[0mdilation_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdilation_rate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         data_format=data_format)\n\u001b[0m\u001b[1;32m    781\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/nn_ops.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_shape, filter_shape, padding, strides, dilation_rate, name, data_format)\u001b[0m\n\u001b[1;32m    827\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    828\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdata_format\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdata_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"NC\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 829\u001b[0;31m       \u001b[0minput_channels_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnum_spatial_dims\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    830\u001b[0m       \u001b[0mspatial_dims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_spatial_dims\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/tensor_shape.pyc\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    614\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mTensorShape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dims\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    615\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 616\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dims\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    617\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    618\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "model9(X_train_new, y_train, X_test_new, y_test,8,10,\"results/CNV_LSTM_XL_model_test.json\",model_h5_file=\"results/CNV_LSTM_XL_model_test.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
